{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df37289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🌍 EJEMPLO: ENTRENAMIENTO BNN GLOBAL MULTI-SENSOR\n",
      "============================================================\n",
      "🌍 Entrenamiento BNN Global Multi-Sensor\n",
      "Using device: cpu\n",
      "\n",
      "📂 Cargando datos...\n",
      "\n",
      "⚙️ Configuración Global:\n",
      "  Sensores entrenamiento: ['28079004', '28079008', '28079011', '28079016', '28079036', '28079038', '28079039', '28079040', '28079047', '28079048']\n",
      "  Sensores test: ['28079050', '28079056', '28079035']\n",
      "  Fecha división: 2024-01-01\n",
      "  Outliers: none\n",
      "\n",
      "🔧 Usando 13 características\n",
      "\n",
      "🌍 Preparando datos para modelo global multi-sensor\n",
      "   Sensores entrenamiento: ['28079004', '28079008', '28079011', '28079016', '28079036', '28079038', '28079039', '28079040', '28079047', '28079048']\n",
      "   Sensores test: ['28079050', '28079056', '28079035']\n",
      "📊 Datos entrenamiento originales: 568499\n",
      "📊 Datos test originales: 167571\n",
      "📅 Datos entrenamiento finales: 490597\n",
      "📅 Datos test finales: 22569\n",
      "📊 Datos: Entrenamiento=490597, Test=22569\n",
      "🚀 Iniciando entrenamiento BNN...\n",
      "   Epoch 20/120 | beta_t=0.139 | NLL: 1.091 | KL: 2.613\n",
      "   Epoch 40/120 | beta_t=0.278 | NLL: 1.171 | KL: 1.065\n",
      "   Epoch 60/120 | beta_t=0.417 | NLL: 1.230 | KL: 0.815\n",
      "   Epoch 80/120 | beta_t=0.500 | NLL: 1.250 | KL: 0.664\n",
      "   Epoch 100/120 | beta_t=0.500 | NLL: 1.247 | KL: 0.622\n",
      "   Epoch 120/120 | beta_t=0.500 | NLL: 1.256 | KL: 0.603\n",
      "✅ Entrenamiento completado.\n",
      "\n",
      "🔍 Evaluando modelo global...\n",
      "💾 Predicciones guardadas en: ../predictions/bnn_predictions_with_epistemic_uncertainty_global.csv\n",
      "\n",
      "🔍 Evaluando modelo global por sensor individual...\n",
      "   📊 Evaluando sensor: 28079050\n",
      "💾 Predicciones guardadas en: ../predictions/bnn_predictions_with_epistemic_uncertainty_28079050.csv\n",
      "      RMSE: 14.71, R²: 0.269, MAE: 11.62\n",
      "   📊 Evaluando sensor: 28079056\n",
      "💾 Predicciones guardadas en: ../predictions/bnn_predictions_with_epistemic_uncertainty_28079056.csv\n",
      "      RMSE: 18.43, R²: 0.239, MAE: 13.82\n",
      "   📊 Evaluando sensor: 28079035\n",
      "💾 Predicciones guardadas en: ../predictions/bnn_predictions_with_epistemic_uncertainty_28079035.csv\n",
      "      RMSE: 17.15, R²: 0.047, MAE: 13.81\n",
      "\n",
      "📊 Métricas Globales\n",
      "  RMSE: 16.72 µg/m³\n",
      "  R²: 0.214\n",
      "  MAE: 12.91 µg/m³\n",
      "  Test NLL: 1.048\n",
      "\n",
      "🌍 Resumen Modelo Global - Resultados por Sensor\n",
      "============================================================\n",
      "Métricas Promedio:\n",
      "  RMSE Promedio: 16.76 µg/m³ (±1.55)\n",
      "  R² Promedio: 0.185 (±0.098)\n",
      "  MAE Promedio: 13.08 µg/m³ (±1.03)\n",
      "\n",
      "Detalle por Sensor:\n",
      "  28079035: RMSE=17.15, R²=0.047, MAE=13.81, n=7108\n",
      "  28079050: RMSE=14.71, R²=0.269, MAE=11.62, n=7813\n",
      "  28079056: RMSE=18.43, R²=0.239, MAE=13.82, n=7648\n",
      "💾 Modelo guardado en: ../models/bnn_model_global.pkl\n",
      "📉 Guardando curvas de entrenamiento en ../models/bnn_training_loss_global.png\n",
      "\n",
      "✅ Proceso global completado.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.BNNUnifiedTrainer at 0x31173bb60>,\n",
       " Sequential(\n",
       "   (MFVILinear0): MFVILinear()\n",
       "   (Nonlinarity0): ReLU()\n",
       "   (MFVILinear1): MFVILinear()\n",
       "   (Nonlinarity1): ReLU()\n",
       "   (MFVILinear2): MFVILinear()\n",
       "   (Nonlinarity2): ReLU()\n",
       "   (MFVILinear3): MFVILinear()\n",
       " ),\n",
       " {'rmse': 16.72022253374116,\n",
       "  'r2': 0.21384328602931568,\n",
       "  'mae': 12.914381145791848,\n",
       "  'test_nll': 1.0478442907333374,\n",
       "  'y_pred': array([23.884783, 29.626791, 34.10759 , ..., 65.82534 , 51.005898,\n",
       "         49.786438], dtype=float32),\n",
       "  'y_pred_std': array([22.705925, 23.278582, 23.686918, ..., 24.532648, 23.060678,\n",
       "         23.022573], dtype=float32),\n",
       "  'predictions_df':        prediction  epistemic_uncertainty\n",
       "  0       23.884783               4.955683\n",
       "  1       29.626791               7.133876\n",
       "  2       34.107590               8.370775\n",
       "  3       37.525906               8.579056\n",
       "  4       40.747250               8.789637\n",
       "  ...           ...                    ...\n",
       "  22564   77.404579              13.184403\n",
       "  22565   73.798401              12.300256\n",
       "  22566   65.825340              10.528563\n",
       "  22567   51.005898               6.387068\n",
       "  22568   49.786438               6.248098\n",
       "  \n",
       "  [22569 rows x 2 columns]},\n",
       " {'28079050': {'metrics': {'rmse': 14.705936344689949,\n",
       "    'r2': 0.26908543932778817,\n",
       "    'mae': 11.620069527952555,\n",
       "    'test_nll': 0.9848790168762207,\n",
       "    'y_pred': array([32.85808 , 37.676647, 38.115486, ..., 55.17012 , 48.253033,\n",
       "           48.06801 ], dtype=float32),\n",
       "    'y_pred_std': array([23.121857, 23.17492 , 23.30088 , ..., 23.320225, 76.62727 ,\n",
       "           83.925095], dtype=float32),\n",
       "    'predictions_df':       prediction  epistemic_uncertainty\n",
       "    0      32.858082               6.604553\n",
       "    1      37.676647               6.787984\n",
       "    2      38.115486               7.206313\n",
       "    3      42.724602               7.911815\n",
       "    4      40.745953               8.358730\n",
       "    ...          ...                    ...\n",
       "    7808   45.350788               7.276989\n",
       "    7809   50.919998               6.974384\n",
       "    7810   55.170120               7.268610\n",
       "    7811   48.253033              73.353523\n",
       "    7812   48.068008              80.947029\n",
       "    \n",
       "    [7813 rows x 2 columns]},\n",
       "   'test_df':           id_no2               fecha  no2_value  latitud_no2  longitud_no2  \\\n",
       "   670150  28079050 2024-01-01 00:00:00       20.0    40.465584     -3.688745   \n",
       "   670151  28079050 2024-01-01 01:00:00       28.0    40.465584     -3.688745   \n",
       "   670152  28079050 2024-01-01 02:00:00       42.0    40.465584     -3.688745   \n",
       "   670153  28079050 2024-01-01 03:00:00       52.0    40.465584     -3.688745   \n",
       "   670154  28079050 2024-01-01 04:00:00       53.0    40.465584     -3.688745   \n",
       "   ...          ...                 ...        ...          ...           ...   \n",
       "   677958  28079050 2024-11-30 20:00:00       65.0    40.465584     -3.688745   \n",
       "   677959  28079050 2024-11-30 21:00:00       65.0    40.465584     -3.688745   \n",
       "   677960  28079050 2024-11-30 22:00:00       68.0    40.465584     -3.688745   \n",
       "   677961  28079050 2024-11-30 23:00:00       74.0    40.465584     -3.688745   \n",
       "   677962  28079050 2024-12-01 00:00:00       69.0    40.465584     -3.688745   \n",
       "   \n",
       "          id_trafico  distance_m  latitud_trafico  longitud_trafico  intensidad  \\\n",
       "   670150       5465       148.7        40.464276          -3.68911      135.25   \n",
       "   670151       5465       148.7        40.464276          -3.68911      443.75   \n",
       "   670152       5465       148.7        40.464276          -3.68911      332.75   \n",
       "   670153       5465       148.7        40.464276          -3.68911      194.25   \n",
       "   670154       5465       148.7        40.464276          -3.68911      120.50   \n",
       "   ...           ...         ...              ...               ...         ...   \n",
       "   677958       5465       148.7        40.464276          -3.68911      549.00   \n",
       "   677959       5465       148.7        40.464276          -3.68911      506.00   \n",
       "   677960       5465       148.7        40.464276          -3.68911      457.00   \n",
       "   677961       5465       148.7        40.464276          -3.68911      335.00   \n",
       "   677962       5465       148.7        40.464276          -3.68911      406.00   \n",
       "   \n",
       "           ...  ocupacion_lag3  ocupacion_lag4  ocupacion_lag6  ocupacion_lag8  \\\n",
       "   670150  ...        2.761497        0.576819        0.764616        1.000000   \n",
       "   670151  ...        2.041164        0.000000        0.576819        0.764616   \n",
       "   670152  ...        0.469298        0.000000        0.000000        0.576819   \n",
       "   670153  ...        4.120419        0.507042        0.000000        0.000000   \n",
       "   670154  ...        3.376121        0.000000        0.507042        0.000000   \n",
       "   ...     ...             ...             ...             ...             ...   \n",
       "   677958  ...        6.979887        1.000000        0.522020        0.295177   \n",
       "   677959  ...        7.492852        0.511696        1.000000        0.522020   \n",
       "   677960  ...        9.654680        1.222678        0.511696        1.000000   \n",
       "   677961  ...        9.048430        1.254447        1.222678        0.511696   \n",
       "   677962  ...        4.464789        0.779540        1.254447        1.222678   \n",
       "   \n",
       "           carga_lag1  carga_lag2  carga_lag3  carga_lag4  carga_lag6  carga_lag8  \n",
       "   670150   14.508162    9.764835   21.852406   13.384771   15.788002   16.740561  \n",
       "   670151    8.279240    3.552680   14.508162    9.764835   13.384771   15.788002  \n",
       "   670152   26.467982   13.018028    8.279240    3.552680    9.764835   13.384771  \n",
       "   670153   25.069844   11.754320   26.467982   13.018028    3.552680    9.764835  \n",
       "   670154   13.353088    5.626770   25.069844   11.754320   13.018028    3.552680  \n",
       "   ...            ...         ...         ...         ...         ...         ...  \n",
       "   677958   31.995916   15.255848   35.489629   15.034483   14.015267   11.025723  \n",
       "   677959   40.749108   17.027778   31.995916   15.255848   15.034483   14.015267  \n",
       "   677960   39.543293   17.322134   40.749108   17.027778   15.255848   15.034483  \n",
       "   677961   28.959403   15.545952   39.543293   17.322134   17.027778   15.255848  \n",
       "   677962   18.766333   10.098507   28.959403   15.545952   17.322134   17.027778  \n",
       "   \n",
       "   [7813 rows x 83 columns],\n",
       "   'n_samples': 7813},\n",
       "  '28079056': {'metrics': {'rmse': 18.432943007895833,\n",
       "    'r2': 0.23910681589363558,\n",
       "    'mae': 13.823070937120765,\n",
       "    'test_nll': 1.1021230220794678,\n",
       "    'y_pred': array([40.251972, 56.869854, 56.652435, ..., 65.60286 , 52.556824,\n",
       "           51.123634], dtype=float32),\n",
       "    'y_pred_std': array([23.36261 , 23.837315, 23.804232, ..., 30.889502, 22.942257,\n",
       "           22.937416], dtype=float32),\n",
       "    'predictions_df':       prediction  epistemic_uncertainty\n",
       "    0      40.251972               7.403474\n",
       "    1      56.869854               8.787345\n",
       "    2      56.652435               8.697200\n",
       "    3      51.024174               8.633564\n",
       "    4      48.148182               9.082989\n",
       "    ...          ...                    ...\n",
       "    7643   79.102577              13.057400\n",
       "    7644   75.546326              11.937438\n",
       "    7645   65.602859              21.521175\n",
       "    7646   52.556824               5.945337\n",
       "    7647   51.123634               5.926625\n",
       "    \n",
       "    [7648 rows x 2 columns]},\n",
       "   'test_df':           id_no2               fecha  no2_value  latitud_no2  longitud_no2  \\\n",
       "   728422  28079056 2024-01-01 00:00:00       24.0    40.385034     -3.718768   \n",
       "   728423  28079056 2024-01-01 01:00:00       32.0    40.385034     -3.718768   \n",
       "   728424  28079056 2024-01-01 02:00:00       44.0    40.385034     -3.718768   \n",
       "   728425  28079056 2024-01-01 03:00:00       39.0    40.385034     -3.718768   \n",
       "   728426  28079056 2024-01-01 04:00:00       35.0    40.385034     -3.718768   \n",
       "   ...          ...                 ...        ...          ...           ...   \n",
       "   736065  28079056 2024-11-30 20:00:00       99.0    40.385034     -3.718768   \n",
       "   736066  28079056 2024-11-30 21:00:00      111.0    40.385034     -3.718768   \n",
       "   736067  28079056 2024-11-30 22:00:00       69.0    40.385034     -3.718768   \n",
       "   736068  28079056 2024-11-30 23:00:00       54.0    40.385034     -3.718768   \n",
       "   736069  28079056 2024-12-01 00:00:00       48.0    40.385034     -3.718768   \n",
       "   \n",
       "          id_trafico  distance_m  latitud_trafico  longitud_trafico  intensidad  \\\n",
       "   728422       5084       170.1        40.383633          -3.71796      334.00   \n",
       "   728423       5084       170.1        40.383633          -3.71796     1042.25   \n",
       "   728424       5084       170.1        40.383633          -3.71796      897.25   \n",
       "   728425       5084       170.1        40.383633          -3.71796      558.50   \n",
       "   728426       5084       170.1        40.383633          -3.71796      376.50   \n",
       "   ...           ...         ...              ...               ...         ...   \n",
       "   736065       5084       170.1        40.383633          -3.71796     1121.75   \n",
       "   736066       5084       170.1        40.383633          -3.71796     1056.25   \n",
       "   736067       5084       170.1        40.383633          -3.71796      877.25   \n",
       "   736068       5084       170.1        40.383633          -3.71796      665.25   \n",
       "   736069       5084       170.1        40.383633          -3.71796      659.75   \n",
       "   \n",
       "           ...  ocupacion_lag3  ocupacion_lag4  ocupacion_lag6  ocupacion_lag8  \\\n",
       "   728422  ...        0.930693        0.000000        2.752589        1.792062   \n",
       "   728423  ...        4.814433        0.329305        7.055441        5.582090   \n",
       "   728424  ...        2.720779        0.642202        5.887964        5.370016   \n",
       "   728425  ...        0.747253        0.305389        2.797217        3.273635   \n",
       "   728426  ...        2.811475        0.000000        1.308097        2.208920   \n",
       "   ...     ...             ...             ...             ...             ...   \n",
       "   736065  ...        6.477573        1.190751        9.517088       12.776988   \n",
       "   736066  ...        3.771704        1.238095        8.359118       12.515532   \n",
       "   736067  ...        5.816327        1.288079        6.083449       11.493037   \n",
       "   736068  ...        5.734694        0.971302        3.760606       16.455490   \n",
       "   736069  ...        1.760274        2.173134        3.768705       34.878788   \n",
       "   \n",
       "           carga_lag1  carga_lag2  carga_lag3  carga_lag4  carga_lag6  carga_lag8  \n",
       "   728422    4.601307    5.071066    2.891089    1.588785   26.157652   15.423641  \n",
       "   728423    9.960474   22.219231    4.391753    6.196375   46.602206   27.334183  \n",
       "   728424    6.694030   18.285714    3.759740    6.149847   37.125662   24.750797  \n",
       "   728425    6.631922   10.637037    1.824176    3.958084   20.727966   15.904874  \n",
       "   728426    5.025210    6.305369    2.696721    4.127907   11.393022   12.771518  \n",
       "   ...            ...         ...         ...         ...         ...         ...  \n",
       "   736065    9.625755   32.345758    6.773087    7.656069   58.788662   71.865472  \n",
       "   736066   10.098434   24.678752    5.035370    8.576190   51.594389   69.593617  \n",
       "   736067    7.209644   22.535809    5.622449    5.556291   40.243046   64.955921  \n",
       "   736068    9.669118   17.956005    5.481633    9.487859   27.514141   57.370920  \n",
       "   736069   13.436170   14.111559    3.000000   12.901493   22.769947   53.967172  \n",
       "   \n",
       "   [7648 rows x 83 columns],\n",
       "   'n_samples': 7648},\n",
       "  '28079035': {'metrics': {'rmse': 17.15097841867375,\n",
       "    'r2': 0.047235762712982576,\n",
       "    'mae': 13.805610929638332,\n",
       "    'test_nll': 1.0621075630187988,\n",
       "    'y_pred': array([24.158926, 29.971706, 34.726704, ..., 54.763496, 68.28721 ,\n",
       "           75.24772 ], dtype=float32),\n",
       "    'y_pred_std': array([22.772512, 23.462496, 23.70802 , ..., 23.219187, 23.620224,\n",
       "           23.751526], dtype=float32),\n",
       "    'predictions_df':       prediction  epistemic_uncertainty\n",
       "    0      24.158926               5.252349\n",
       "    1      29.971706               7.712881\n",
       "    2      34.726704               8.430302\n",
       "    3      38.140366               8.696154\n",
       "    4      41.419971               8.812071\n",
       "    ...          ...                    ...\n",
       "    7103   63.192768               7.480112\n",
       "    7104   57.174824               6.338251\n",
       "    7105   54.763496               6.937616\n",
       "    7106   68.287209               8.180147\n",
       "    7107   75.247719               8.551889\n",
       "    \n",
       "    [7108 rows x 2 columns]},\n",
       "   'test_df':           id_no2               fecha  no2_value  latitud_no2  longitud_no2  \\\n",
       "   262339  28079035 2024-01-01 00:00:00       14.0    40.419209     -3.703166   \n",
       "   262340  28079035 2024-01-01 01:00:00       14.0    40.419209     -3.703166   \n",
       "   262341  28079035 2024-01-01 02:00:00       23.0    40.419209     -3.703166   \n",
       "   262342  28079035 2024-01-01 03:00:00       29.0    40.419209     -3.703166   \n",
       "   262343  28079035 2024-01-01 04:00:00       34.0    40.419209     -3.703166   \n",
       "   ...          ...                 ...        ...          ...           ...   \n",
       "   269442  28079035 2024-11-30 20:00:00       47.0    40.419209     -3.703166   \n",
       "   269443  28079035 2024-11-30 21:00:00       61.0    40.419209     -3.703166   \n",
       "   269444  28079035 2024-11-30 22:00:00       82.0    40.419209     -3.703166   \n",
       "   269445  28079035 2024-11-30 23:00:00       72.0    40.419209     -3.703166   \n",
       "   269446  28079035 2024-12-01 00:00:00       49.0    40.419209     -3.703166   \n",
       "   \n",
       "          id_trafico  distance_m  latitud_trafico  longitud_trafico  intensidad  \\\n",
       "   262339       3731        26.8         40.41945          -3.70318       29.75   \n",
       "   262340       3731        26.8         40.41945          -3.70318       25.25   \n",
       "   262341       3731        26.8         40.41945          -3.70318       28.75   \n",
       "   262342       3731        26.8         40.41945          -3.70318       32.25   \n",
       "   262343       3731        26.8         40.41945          -3.70318       27.00   \n",
       "   ...           ...         ...              ...               ...         ...   \n",
       "   269442       3731        26.8         40.41945          -3.70318       44.75   \n",
       "   269443       3731        26.8         40.41945          -3.70318       59.00   \n",
       "   269444       3731        26.8         40.41945          -3.70318       52.75   \n",
       "   269445       3731        26.8         40.41945          -3.70318       48.75   \n",
       "   269446       3731        26.8         40.41945          -3.70318       63.50   \n",
       "   \n",
       "           ...  ocupacion_lag3  ocupacion_lag4  ocupacion_lag6  ocupacion_lag8  \\\n",
       "   262339  ...        4.773810        2.739394       29.622423        7.508571   \n",
       "   262340  ...        4.353293        1.848739       23.066311        2.739394   \n",
       "   262341  ...        2.401235        1.207921       25.461538        1.848739   \n",
       "   262342  ...        4.200000        2.408696       17.132890        1.207921   \n",
       "   262343  ...        3.232877        1.519380       35.030764        2.408696   \n",
       "   ...     ...             ...             ...             ...             ...   \n",
       "   269442  ...        2.819672       21.404677        5.564103       28.768705   \n",
       "   269443  ...       14.648045        6.493431        2.819672       35.522830   \n",
       "   269444  ...        5.152542        7.708738       14.648045       11.613402   \n",
       "   269445  ...        2.952607       40.603589        5.152542       12.010989   \n",
       "   269446  ...        3.984615       43.304958        2.952607       22.090214   \n",
       "   \n",
       "           carga_lag1  carga_lag2  carga_lag3  carga_lag4  carga_lag6  carga_lag8  \n",
       "   262339   38.093697   50.072309   10.742063    6.242424   56.337601   12.851429  \n",
       "   262340   26.918605   43.994903    9.473054    4.815126   50.072309    6.242424  \n",
       "   262341   32.942182   43.166113    7.216049    4.574257   43.994903    4.815126  \n",
       "   262342   33.932386   58.940091    9.200000    5.408696   43.166113    4.574257  \n",
       "   262343   43.113569   64.789730    6.945205    6.736434   58.940091    5.408696  \n",
       "   ...            ...         ...         ...         ...         ...         ...  \n",
       "   269442   20.125547   24.149485    6.459016   42.281139   14.686610   54.939818  \n",
       "   269443   19.106796   24.675824   19.681564   20.125547    6.459016   55.199918  \n",
       "   269444   57.019032   45.671254   11.288136   19.106796   19.681564   24.149485  \n",
       "   269445   61.442738   56.480797    8.758294   57.019032   11.288136   24.675824  \n",
       "   269446   49.331741   58.942572    8.148718   61.442738    8.758294   45.671254  \n",
       "   \n",
       "   [7108 rows x 83 columns],\n",
       "   'n_samples': 7108}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Módulo unificado para entrenamiento de Redes Neuronales Bayesianas (BNN) con MFVI,\n",
    "ajustado para mejorar RMSE manteniendo la cuantificación de incertidumbre.\n",
    "\n",
    "Cambios clave respecto a la versión anterior:\n",
    "1) Escalado correcto de la KL por minibatch con annealing de beta.\n",
    "2) Scheduler de tasa de aprendizaje tipo MultiStepLR.\n",
    "3) Clipping de gradiente para estabilidad.\n",
    "4) Inicialización del ruido más baja.\n",
    "5) Priors algo más amplios e init_std mayor.\n",
    "6) Aumento de épocas y ajuste de learning rate en el ejemplo.\n",
    "7) Corrección en prepare_individual_data para no usar variables indefinidas.\n",
    "8) Semillas para reproducibilidad.\n",
    "\n",
    "Nota: No se usa el ID de la estación como variable predictora.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.classes.__path__ = []  # estabilidad para entornos sin extensiones\n",
    "\n",
    "# Constante para estabilidad numérica\n",
    "EPS = 1e-6\n",
    "\n",
    "# ==================== COMPONENTES BNN (MFVI) ====================\n",
    "\n",
    "class MFVILinear(nn.Module):\n",
    "    \"\"\"Capa lineal Bayesiana usando Inferencia Variacional de Campo Medio (MFVI).\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, prior_weight_std=1.5, prior_bias_std=1.5, init_std=0.10, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.weight_mean = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self.bias_mean = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        self._weight_std_param = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self._bias_std_param = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        \n",
    "        self.reset_parameters(init_std)\n",
    "\n",
    "        prior_mean = 0.0\n",
    "        self.register_buffer('prior_weight_mean', torch.full_like(self.weight_mean, prior_mean))\n",
    "        self.register_buffer('prior_weight_std', torch.full_like(self._weight_std_param, prior_weight_std))\n",
    "        self.register_buffer('prior_bias_mean', torch.full_like(self.bias_mean, prior_mean))\n",
    "        self.register_buffer('prior_bias_std', torch.full_like(self._bias_std_param, prior_bias_std))\n",
    "\n",
    "    def reset_parameters(self, init_std=0.10):\n",
    "        nn.init.kaiming_uniform_(self.weight_mean, a=math.sqrt(5))\n",
    "        bound = self.dim_in ** -0.5 if self.dim_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias_mean, -bound, bound)\n",
    "        _init_std_param = np.log(init_std)\n",
    "        self._weight_std_param.data = torch.full_like(self.weight_mean, _init_std_param)\n",
    "        self._bias_std_param.data = torch.full_like(self.bias_mean, _init_std_param)\n",
    "\n",
    "    @property\n",
    "    def weight_std(self):\n",
    "        return torch.clamp(torch.exp(self._weight_std_param), min=EPS)\n",
    "\n",
    "    @property\n",
    "    def bias_std(self):\n",
    "        return torch.clamp(torch.exp(self._bias_std_param), min=EPS)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        q_weight = dist.Normal(self.weight_mean, self.weight_std)\n",
    "        p_weight = dist.Normal(self.prior_weight_mean, self.prior_weight_std)\n",
    "        kl = dist.kl_divergence(q_weight, p_weight).sum()\n",
    "        \n",
    "        q_bias = dist.Normal(self.bias_mean, self.bias_std)\n",
    "        p_bias = dist.Normal(self.prior_bias_mean, self.prior_bias_std)\n",
    "        kl += dist.kl_divergence(q_bias, p_bias).sum()\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self._normal_sample(self.weight_mean, self.weight_std)\n",
    "        bias = self._normal_sample(self.bias_mean, self.bias_std)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _normal_sample(self, mean, std):\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + std * epsilon\n",
    "\n",
    "\n",
    "def make_mfvi_bnn(layer_sizes, activation='GELU', **layer_kwargs):\n",
    "    nonlinearity = getattr(nn, activation)() if isinstance(activation, str) else activation\n",
    "    net = nn.Sequential()\n",
    "    for i, (dim_in, dim_out) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        net.add_module(f'MFVILinear{i}', MFVILinear(dim_in, dim_out, **layer_kwargs))\n",
    "        if i < len(layer_sizes) - 2:\n",
    "            net.add_module(f'Nonlinarity{i}', nonlinearity)\n",
    "    return net\n",
    "\n",
    "\n",
    "def kl_divergence_model(bnn):\n",
    "    kl = 0.0\n",
    "    for module in bnn.modules():\n",
    "        if hasattr(module, 'kl_divergence'):\n",
    "            kl += module.kl_divergence()\n",
    "    return kl\n",
    "\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    # y: (N,1) o broadcast a (K,N,1); y_pred: (K,N,1) o (N,1)\n",
    "    l2_dist = (y - y_pred).pow(2).sum(-1)\n",
    "    return -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "\n",
    "def test_nll(y, y_pred, log_noise_var):\n",
    "    nll_samples = -gauss_loglik(y, y_pred, log_noise_var)  # (K, N)\n",
    "    nll = -torch.logsumexp(-nll_samples, dim=0) + math.log(nll_samples.shape[0])\n",
    "    return nll.mean()\n",
    "\n",
    "# ==================== DATASET PYTORCH ====================\n",
    "\n",
    "class BayesianDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "\n",
    "# ==================== ENTRENADOR UNIFICADO ====================\n",
    "\n",
    "class BNNUnifiedTrainer:\n",
    "    \"\"\"\n",
    "    Clase unificada para entrenamiento de BNNs que maneja tanto modelos individuales \n",
    "    como globales con una interfaz consistente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_master = None\n",
    "        self.model = None\n",
    "        self.scaler_dict = {}\n",
    "        self.scaler_target = None\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        # Semillas\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        if 'cuda' in self.device:\n",
    "            torch.cuda.manual_seed_all(42)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Cargar datos desde el archivo parquet.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_parquet('../data/super_processed/7_4_no2_with_traffic_and_1meteo_and_1trafic_id.parquet')\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar los datos: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def get_available_sensors(self) -> List[str]:\n",
    "        \"\"\"Obtener lista de sensores disponibles en los datos.\"\"\"\n",
    "        if self.df_master is None:\n",
    "            self.df_master = self.load_data()\n",
    "        return sorted(self.df_master['id_no2'].unique().tolist())\n",
    "\n",
    "    def remove_outliers(self, df: pd.DataFrame, method: str, columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Remover outliers usando el método especificado.\"\"\"\n",
    "        if method == 'none': \n",
    "            return df\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = ['no2_value']\n",
    "            \n",
    "        df_filtered = df.copy()\n",
    "        for col in columns:\n",
    "            if col not in df_filtered.columns: \n",
    "                continue\n",
    "            if method == 'iqr':\n",
    "                Q1, Q3 = df_filtered[col].quantile(0.25), df_filtered[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                if IQR > 0:\n",
    "                    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]\n",
    "            elif method == 'zscore':\n",
    "                df_filtered = df_filtered[np.abs(zscore(df_filtered[col], nan_policy='omit')) < 3]\n",
    "        return df_filtered\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, split_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Dividir datos por fecha.\"\"\"\n",
    "        train = df[df['fecha'] < split_date].copy()\n",
    "        test = df[df['fecha'] >= split_date].copy()\n",
    "        return train, test\n",
    "\n",
    "    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        \"\"\"Escalar características usando StandardScaler.\"\"\"\n",
    "        scaler_dict = {}\n",
    "        X_train_s, X_test_s = X_train.copy(), X_test.copy()\n",
    "        for feature in features:\n",
    "            if feature in X_train.columns and pd.api.types.is_numeric_dtype(X_train[feature]):\n",
    "                scaler = StandardScaler()\n",
    "                X_train_s[feature] = scaler.fit_transform(X_train[[feature]]).flatten()\n",
    "                X_test_s[feature] = scaler.transform(X_test[[feature]]).flatten()\n",
    "                scaler_dict[feature] = scaler\n",
    "        return X_train_s, X_test_s, scaler_dict\n",
    "\n",
    "    def scale_target(self, y_train: pd.Series) -> Tuple[pd.Series, StandardScaler]:\n",
    "        \"\"\"Escalar variable objetivo.\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        y_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        return pd.Series(y_scaled, index=y_train.index, name=y_train.name), scaler\n",
    "\n",
    "    def prepare_individual_data(self, config: Dict, selected_features: List[str]) -> Dict:\n",
    "        \"\"\"Preparar datos para entrenamiento individual.\"\"\"\n",
    "        print(f\"\\n📊 Preparando datos para sensor individual: {config['sensor_id']}\")\n",
    "        \n",
    "        # Filtrar por sensor\n",
    "        df_sensor = self.df_master[self.df_master['id_no2'] == config['sensor_id']].copy()\n",
    "        if df_sensor.empty:\n",
    "            print(f\"❌ Error: No se encontraron datos para el sensor {config['sensor_id']}\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"📊 Datos originales del sensor: {len(df_sensor)}\")\n",
    "        \n",
    "        # División temporal\n",
    "        train_df, test_df = self.split_data(df_sensor, pd.to_datetime(config['split_date']))\n",
    "        \n",
    "        # Remover outliers solo en entrenamiento si aplica\n",
    "        outliers_removed = 0\n",
    "        if config['outlier_method'] != 'none':\n",
    "            len_before = len(train_df)\n",
    "            train_df = self.remove_outliers(train_df, config['outlier_method'], ['no2_value'])\n",
    "            outliers_removed = len_before - len(train_df)\n",
    "            print(f\"❌ Outliers eliminados: {outliers_removed}\")\n",
    "        \n",
    "        print(f\"📅 Datos entrenamiento finales: {len(train_df)}\")\n",
    "        print(f\"📅 Datos test finales: {len(test_df)}\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'test_df': test_df,\n",
    "            'outliers_removed': outliers_removed,\n",
    "            'mode': 'individual',\n",
    "            'sensor_id': config['sensor_id']\n",
    "        }\n",
    "\n",
    "    def prepare_global_data(self, config: Dict, selected_features: List[str]) -> Dict:\n",
    "        \"\"\"Preparar datos para entrenamiento global (múltiples sensores).\"\"\"\n",
    "        print(f\"\\n🌍 Preparando datos para modelo global multi-sensor\")\n",
    "        print(f\"   Sensores entrenamiento: {config['sensors_train']}\")\n",
    "        print(f\"   Sensores test: {config['sensors_test']}\")\n",
    "        \n",
    "        df_train = self.df_master[self.df_master['id_no2'].isin(config['sensors_train'])].copy()\n",
    "        df_test = self.df_master[self.df_master['id_no2'].isin(config['sensors_test'])].copy()\n",
    "        \n",
    "        print(f\"📊 Datos entrenamiento originales: {len(df_train)}\")\n",
    "        print(f\"📊 Datos test originales: {len(df_test)}\")\n",
    "        \n",
    "        # División temporal adicional\n",
    "        df_train = df_train[df_train['fecha'] < pd.to_datetime(config['split_date'])].copy()\n",
    "        df_test = df_test[df_test['fecha'] >= pd.to_datetime(config['split_date'])].copy()\n",
    "        \n",
    "        # Validar que todas las features existen\n",
    "        missing_features_train = [f for f in selected_features if f not in df_train.columns]\n",
    "        missing_features_test = [f for f in selected_features if f not in df_test.columns]\n",
    "        \n",
    "        if missing_features_train or missing_features_test:\n",
    "            print(f\"❌ Features faltantes en train: {missing_features_train}\")\n",
    "            print(f\"❌ Features faltantes en test: {missing_features_test}\")\n",
    "            return {}\n",
    "        \n",
    "        # Remover outliers solo en entrenamiento si aplica\n",
    "        outliers_removed = 0\n",
    "        if config['outlier_method'] != 'none':\n",
    "            len_before = len(df_train)\n",
    "            df_train = self.remove_outliers(df_train, config['outlier_method'], ['no2_value'])\n",
    "            outliers_removed = len_before - len(df_train)\n",
    "            print(f\"❌ Outliers eliminados: {outliers_removed}\")\n",
    "        \n",
    "        print(f\"📅 Datos entrenamiento finales: {len(df_train)}\")\n",
    "        print(f\"📅 Datos test finales: {len(df_test)}\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': df_train,\n",
    "            'test_df': df_test,\n",
    "            'outliers_removed': outliers_removed,\n",
    "            'mode': 'global',\n",
    "            'sensors_train': config['sensors_train'],\n",
    "            'sensors_test': config['sensors_test']\n",
    "        }\n",
    "\n",
    "    def train_bnn_model(self, X_train, y_train, train_config):\n",
    "        \"\"\"Entrenar el modelo BNN con annealing de KL, scheduler y clipping.\"\"\"\n",
    "        # Setup\n",
    "        dataset = BayesianDataset(X_train, y_train)\n",
    "        auto_bs = min(train_config['batch_size'], max(64, len(dataset) // 4))\n",
    "        dataloader = DataLoader(dataset, batch_size=auto_bs, shuffle=True)\n",
    "\n",
    "        \n",
    "        layer_sizes = [X_train.shape[1]] + train_config['hidden_dims'] + [1]\n",
    "        model = make_mfvi_bnn(\n",
    "            layer_sizes,\n",
    "            activation=train_config['activation'],\n",
    "            prior_weight_std=1.0,   # antes 1.5\n",
    "            prior_bias_std=1.0,     # antes 1.5\n",
    "            init_std=0.05,          # antes 0.10\n",
    "            device=self.device\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "        # Ruido inicial más bajo\n",
    "        log_noise_var = nn.Parameter(torch.ones(1, device=self.device) * -3.0)\n",
    "        \n",
    "        params = list(model.parameters()) + [log_noise_var]\n",
    "        optimizer = torch.optim.Adam(params, lr=train_config['learning_rate'])\n",
    "\n",
    "        # Scheduler simple\n",
    "        T = train_config['n_epochs']\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[int(0.6*T), int(0.85*T)], gamma=0.5  # antes 0.2\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Annealing de beta durante warmup\n",
    "        warmup = max(1, int(0.6 * T))\n",
    "\n",
    "        logs = []\n",
    "        N_data = len(dataset)\n",
    "        \n",
    "        print(\"🚀 Iniciando entrenamiento BNN...\")\n",
    "        for epoch in range(T):\n",
    "            model.train()\n",
    "            epoch_nll, epoch_kl = 0.0, 0.0\n",
    "\n",
    "            beta_t = train_config['beta'] * min(1.0, (epoch + 1) / warmup)\n",
    "\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = model(x_batch)\n",
    "                nll = -gauss_loglik(y_batch, y_pred, log_noise_var).mean()\n",
    "                kl = kl_divergence_model(model)\n",
    "\n",
    "                # Opción A corregida: repartir la KL entre batches + annealing\n",
    "                loss = nll + beta_t * kl * (len(x_batch) / N_data)\n",
    "                \n",
    "                loss.backward()\n",
    "                # Clipping de gradiente\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_nll += nll.item() * len(x_batch)\n",
    "                epoch_kl += kl.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            logs.append({'nll': epoch_nll / N_data, 'kl': epoch_kl / N_data})\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"   Epoch {epoch+1}/{T} | beta_t={beta_t:.3f} | NLL: {logs[-1]['nll']:.3f} | KL: {logs[-1]['kl']:.3f}\")\n",
    "        \n",
    "        print(\"✅ Entrenamiento completado.\")\n",
    "        return model, log_noise_var, logs\n",
    "\n",
    "    def predict(self, model, X_test, K, log_noise_var):\n",
    "        \"\"\"Realizar predicciones con el modelo BNN.\"\"\"\n",
    "        model.eval()\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_preds = torch.stack([model(X_test_tensor) for _ in range(K)], dim=0)  # (K, N, 1)\n",
    "        \n",
    "        pred_mean = y_preds.mean(0)  # (N,1)\n",
    "        epistemic_uncertainty = y_preds.var(0).sqrt()  # (N,1)\n",
    "        aleatoric_uncertainty = torch.exp(0.5 * log_noise_var).expand_as(pred_mean)  # (N,1)\n",
    "        total_uncertainty = (epistemic_uncertainty**2 + aleatoric_uncertainty**2).sqrt()\n",
    "        \n",
    "        return {\n",
    "            'y_preds_all': y_preds,\n",
    "            'mean': pred_mean,\n",
    "            'epistemic_std': epistemic_uncertainty,\n",
    "            'aleatoric_std': aleatoric_uncertainty,\n",
    "            'total_std': total_uncertainty\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, predictions, y_test, y_test_scaled, scaler_target, log_noise_var, sensor_id: str = None):\n",
    "        \"\"\"Evaluar el modelo y calcular métricas.\"\"\"\n",
    "        # Desescalar predicciones\n",
    "        pred_mean_scaled = predictions['mean'].detach().cpu().numpy()\n",
    "        pred_mean = scaler_target.inverse_transform(pred_mean_scaled).flatten()\n",
    "        \n",
    "        total_std_scaled = predictions['total_std'].detach().cpu().numpy().flatten()\n",
    "        unscaled_std = total_std_scaled * scaler_target.scale_[0]\n",
    "        \n",
    "        epistemic_std = predictions['epistemic_std'].detach().cpu().numpy().flatten()\n",
    "        epistemic_unscaled_std = epistemic_std * scaler_target.scale_[0]\n",
    "\n",
    "        # Guardar predicciones\n",
    "        df_preds = pd.DataFrame({\n",
    "            'prediction': pred_mean,\n",
    "            'epistemic_uncertainty': epistemic_unscaled_std\n",
    "        })\n",
    "        sensor_suffix = f\"_{sensor_id}\" if sensor_id else \"_global\"\n",
    "        filename = f'../predictions/bnn_predictions_with_epistemic_uncertainty{sensor_suffix}.csv'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        df_preds.to_csv(filename, index=False)\n",
    "        print(f\"💾 Predicciones guardadas en: {filename}\")\n",
    "\n",
    "        # Métricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred_mean))\n",
    "        r2 = r2_score(y_test, pred_mean)\n",
    "        mae = mean_absolute_error(y_test, pred_mean)\n",
    "        \n",
    "        # Test NLL\n",
    "        y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32).reshape(-1, 1).to(self.device)\n",
    "        nll = test_nll(y_test_tensor, predictions['y_preds_all'], log_noise_var).item()\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse, 'r2': r2, 'mae': mae, 'test_nll': nll,\n",
    "            'y_pred': pred_mean, 'y_pred_std': unscaled_std,\n",
    "            'predictions_df': df_preds\n",
    "        }\n",
    "\n",
    "    def evaluate_global_by_sensor(self, data_prep: Dict, model, log_noise_var, scaler_dict, \n",
    "                                  scaler_target, selected_features, K_predict: int) -> Dict:\n",
    "        \"\"\"Evaluar modelo global por sensor individual.\"\"\"\n",
    "        results_by_sensor = {}\n",
    "        test_df = data_prep['test_df']\n",
    "        \n",
    "        print(f\"\\n🔍 Evaluando modelo global por sensor individual...\")\n",
    "        \n",
    "        for sensor_id in data_prep['sensors_test']:\n",
    "            print(f\"   📊 Evaluando sensor: {sensor_id}\")\n",
    "            \n",
    "            sensor_test_df = test_df[test_df['id_no2'] == sensor_id].copy()\n",
    "            if len(sensor_test_df) == 0:\n",
    "                print(f\"      ⚠️ No hay datos de test para sensor {sensor_id}\")\n",
    "                continue\n",
    "            \n",
    "            X_test_sensor = sensor_test_df[selected_features].copy()\n",
    "            y_test_sensor = sensor_test_df['no2_value'].copy()\n",
    "            \n",
    "            # Escalado con scalers del entrenamiento global\n",
    "            X_test_sensor_scaled = X_test_sensor.copy()\n",
    "            for feature in selected_features:\n",
    "                if feature in scaler_dict:\n",
    "                    X_test_sensor_scaled[feature] = scaler_dict[feature].transform(X_test_sensor[[feature]]).flatten()\n",
    "            \n",
    "            y_test_sensor_scaled = pd.Series(\n",
    "                scaler_target.transform(y_test_sensor.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_test_sensor.index,\n",
    "                name=y_test_sensor.name\n",
    "            )\n",
    "            \n",
    "            predictions = self.predict(model, X_test_sensor_scaled, K_predict, log_noise_var)\n",
    "            metrics = self.evaluate_model(predictions, y_test_sensor, y_test_sensor_scaled, \n",
    "                                          scaler_target, log_noise_var, sensor_id)\n",
    "            \n",
    "            results_by_sensor[sensor_id] = {\n",
    "                'metrics': metrics,\n",
    "                'test_df': sensor_test_df,\n",
    "                'n_samples': len(sensor_test_df)\n",
    "            }\n",
    "            \n",
    "            print(f\"      RMSE: {metrics['rmse']:.2f}, R²: {metrics['r2']:.3f}, MAE: {metrics['mae']:.2f}\")\n",
    "        \n",
    "        return results_by_sensor\n",
    "    \n",
    "    def save_model(self, path: str, model, log_noise_var, scaler_dict, scaler_target, \n",
    "                   feature_names: List[str], model_config: Dict = None):\n",
    "        \"\"\"Guardar modelo entrenado.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        model_state = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'log_noise_var': log_noise_var,  # tensor con grad\n",
    "            'scaler_dict': scaler_dict,\n",
    "            'scaler_target': scaler_target,\n",
    "            'feature_names': feature_names,\n",
    "            'model_config': model_config or {}\n",
    "        }\n",
    "        joblib.dump(model_state, path)\n",
    "        print(f\"💾 Modelo guardado en: {path}\")\n",
    "\n",
    "    def load_model(self, path: str, layer_sizes: List[int], activation: str = 'GELU'):\n",
    "        \"\"\"Cargar modelo guardado.\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"❌ Modelo no encontrado en: {path}\")\n",
    "            return None\n",
    "        \n",
    "        model_state = joblib.load(path)\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=activation, device=self.device).to(self.device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        log_noise_var = model_state['log_noise_var'].to(self.device)\n",
    "        \n",
    "        return (model, log_noise_var, model_state['scaler_dict'], \n",
    "                model_state['scaler_target'], model_state['feature_names'])\n",
    "\n",
    "# ==================== VISUALIZACIÓN Y REPORTE ====================\n",
    "\n",
    "def print_model_metrics(metrics: Dict, title: str = \"Métricas de Evaluación\"):\n",
    "    \"\"\"Mostrar métricas del modelo.\"\"\"\n",
    "    print(f\"\\n📊 {title}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f} µg/m³\")\n",
    "    print(f\"  R²: {metrics['r2']:.3f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.2f} µg/m³\")\n",
    "    print(f\"  Test NLL: {metrics['test_nll']:.3f}\")\n",
    "\n",
    "def print_global_summary(results_by_sensor: Dict):\n",
    "    \"\"\"Mostrar resumen de resultados globales por sensor.\"\"\"\n",
    "    print(f\"\\n🌍 Resumen Modelo Global - Resultados por Sensor\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_rmse = [results_by_sensor[sensor]['metrics']['rmse'] for sensor in results_by_sensor]\n",
    "    all_r2 = [results_by_sensor[sensor]['metrics']['r2'] for sensor in results_by_sensor]\n",
    "    all_mae = [results_by_sensor[sensor]['metrics']['mae'] for sensor in results_by_sensor]\n",
    "    \n",
    "    print(f\"Métricas Promedio:\")\n",
    "    print(f\"  RMSE Promedio: {np.mean(all_rmse):.2f} µg/m³ (±{np.std(all_rmse):.2f})\")\n",
    "    print(f\"  R² Promedio: {np.mean(all_r2):.3f} (±{np.std(all_r2):.3f})\")\n",
    "    print(f\"  MAE Promedio: {np.mean(all_mae):.2f} µg/m³ (±{np.std(all_mae):.2f})\")\n",
    "    \n",
    "    print(f\"\\nDetalle por Sensor:\")\n",
    "    for sensor_id in sorted(results_by_sensor.keys()):\n",
    "        result = results_by_sensor[sensor_id]\n",
    "        metrics = result['metrics']\n",
    "        print(f\"  {sensor_id}: RMSE={metrics['rmse']:.2f}, R²={metrics['r2']:.3f}, MAE={metrics['mae']:.2f}, n={result['n_samples']}\")\n",
    "\n",
    "def save_training_loss_plot(logs: List[Dict], filename: str):\n",
    "    \"\"\"Guardar gráfico de curvas de entrenamiento.\"\"\"\n",
    "    print(f\"📉 Guardando curvas de entrenamiento en {filename}\")\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(log_df['nll'])\n",
    "    ax1.set_title(\"Negative Log-Likelihood\")\n",
    "    ax1.set_xlabel(\"Época\")\n",
    "    ax2.plot(log_df['kl'])\n",
    "    ax2.set_title(\"KL Divergence\")\n",
    "    ax2.set_xlabel(\"Época\")\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ==================== FUNCIONES PRINCIPALES ====================\n",
    "\n",
    "def main_individual(config: Dict):\n",
    "    \"\"\"Función principal para entrenamiento individual.\"\"\"\n",
    "    print(\"🧠 Entrenamiento BNN Individual\")\n",
    "    trainer = BNNUnifiedTrainer()\n",
    "    \n",
    "    # Cargar datos\n",
    "    print(\"\\n📂 Cargando datos...\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty: \n",
    "        return\n",
    "    \n",
    "    print(f\"\\n⚙️ Configuración Individual:\")\n",
    "    print(f\"  Sensor: {config['sensor_id']}\")\n",
    "    print(f\"  Fecha división: {config['split_date']}\")\n",
    "    print(f\"  Outliers: {config['outlier_method']}\")\n",
    "    \n",
    "    # Definir features ANTES de preparar datos\n",
    "    sample_data = trainer.df_master[trainer.df_master['id_no2'] == config['sensor_id']].head(100).copy()\n",
    "    if sample_data.empty:\n",
    "        print(f\"❌ Error: No se encontraron datos para el sensor {config['sensor_id']}\")\n",
    "        return\n",
    "    \n",
    "    all_features = [c for c in sample_data.columns \n",
    "                    if c not in ['fecha', 'id_no2', 'no2_value'] \n",
    "                    and pd.api.types.is_numeric_dtype(sample_data[c])]\n",
    "    \n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    \n",
    "    print(f\"\\n🔧 Usando {len(selected_features)} características\")\n",
    "    print(\"   Características seleccionadas:\", selected_features)\n",
    "    \n",
    "    # Preparar datos\n",
    "    data_prep = trainer.prepare_individual_data(config, selected_features)\n",
    "    if not data_prep:\n",
    "        return\n",
    "    \n",
    "    # Matrices\n",
    "    X_train = data_prep['train_df'][selected_features]\n",
    "    y_train = data_prep['train_df']['no2_value']\n",
    "    X_test = data_prep['test_df'][selected_features]\n",
    "    y_test = data_prep['test_df']['no2_value']\n",
    "    \n",
    "    # Escalado\n",
    "    X_train_scaled, X_test_scaled, scaler_dict = trainer.scale_features(X_train, X_test, selected_features)\n",
    "    y_train_scaled, scaler_target = trainer.scale_target(y_train)\n",
    "    y_test_scaled = pd.Series(\n",
    "        scaler_target.transform(y_test.values.reshape(-1, 1)).flatten(),\n",
    "        index=y_test.index, name=y_test.name\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Datos: Entrenamiento={len(X_train_scaled)}, Test={len(X_test_scaled)}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(X_train_scaled, y_train_scaled, config['train_config'])\n",
    "    \n",
    "    # Evaluar\n",
    "    print(\"\\n🔍 Evaluando modelo...\")\n",
    "    predictions = trainer.predict(model, X_test_scaled, config['K_predict'], log_noise_var)\n",
    "    metrics = trainer.evaluate_model(predictions, y_test, y_test_scaled, scaler_target, log_noise_var, config['sensor_id'])\n",
    "    \n",
    "    # Resultados\n",
    "    print_model_metrics(metrics)\n",
    "    \n",
    "    # Guardar\n",
    "    model_path = f\"../models/bnn_model_{config['sensor_id']}.pkl\"\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, \n",
    "                       selected_features, config)\n",
    "    \n",
    "    save_training_loss_plot(logs, f\"../models/bnn_training_loss_{config['sensor_id']}.png\")\n",
    "    \n",
    "    print(\"\\n✅ Proceso individual completado.\")\n",
    "    return trainer, model, metrics\n",
    "\n",
    "\n",
    "def main_global(config: Dict):\n",
    "    \"\"\"Función principal para entrenamiento global (multi-sensor).\"\"\"\n",
    "    print(\"🌍 Entrenamiento BNN Global Multi-Sensor\")\n",
    "    trainer = BNNUnifiedTrainer()\n",
    "    \n",
    "    # Cargar datos\n",
    "    print(\"\\n📂 Cargando datos...\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty: \n",
    "        return\n",
    "    \n",
    "    print(f\"\\n⚙️ Configuración Global:\")\n",
    "    print(f\"  Sensores entrenamiento: {config['sensors_train']}\")\n",
    "    print(f\"  Sensores test: {config['sensors_test']}\")\n",
    "    print(f\"  Fecha división: {config['split_date']}\")\n",
    "    print(f\"  Outliers: {config['outlier_method']}\")\n",
    "    \n",
    "    # Definir features ANTES de preparar datos\n",
    "    sample_data = trainer.df_master[trainer.df_master['id_no2'].isin(config['sensors_train'])].head(100).copy()\n",
    "    \n",
    "    all_features = [c for c in sample_data.columns \n",
    "                    if c not in ['fecha', 'id_no2', 'no2_value'] \n",
    "                    and pd.api.types.is_numeric_dtype(sample_data[c])]\n",
    "    \n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    \n",
    "    print(f\"\\n🔧 Usando {len(selected_features)} características\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    data_prep = trainer.prepare_global_data(config, selected_features)\n",
    "    if not data_prep:\n",
    "        return\n",
    "    \n",
    "    # Matrices\n",
    "    X_train = data_prep['train_df'][selected_features]\n",
    "    y_train = data_prep['train_df']['no2_value']\n",
    "    X_test = data_prep['test_df'][selected_features]\n",
    "    y_test = data_prep['test_df']['no2_value']\n",
    "    \n",
    "    # Escalado\n",
    "    X_train_scaled, X_test_scaled, scaler_dict = trainer.scale_features(X_train, X_test, selected_features)\n",
    "    y_train_scaled, scaler_target = trainer.scale_target(y_train)\n",
    "    y_test_scaled = pd.Series(\n",
    "        scaler_target.transform(y_test.values.reshape(-1, 1)).flatten(),\n",
    "        index=y_test.index, name=y_test.name\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Datos: Entrenamiento={len(X_train_scaled)}, Test={len(X_test_scaled)}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(X_train_scaled, y_train_scaled, config['train_config'])\n",
    "    \n",
    "    # Evaluar globalmente\n",
    "    print(\"\\n🔍 Evaluando modelo global...\")\n",
    "    predictions = trainer.predict(model, X_test_scaled, config['K_predict'], log_noise_var)\n",
    "    global_metrics = trainer.evaluate_model(predictions, y_test, y_test_scaled, scaler_target, log_noise_var, \"global\")\n",
    "    \n",
    "    # Evaluar por sensor\n",
    "    sensor_results = trainer.evaluate_global_by_sensor(\n",
    "        data_prep, model, log_noise_var, scaler_dict, scaler_target, \n",
    "        selected_features, config['K_predict']\n",
    "    )\n",
    "    \n",
    "    # Resultados\n",
    "    print_model_metrics(global_metrics, \"Métricas Globales\")\n",
    "    print_global_summary(sensor_results)\n",
    "    \n",
    "    # Guardar\n",
    "    model_path = f\"../models/bnn_model_global.pkl\"\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, \n",
    "                       selected_features, config)\n",
    "    \n",
    "    save_training_loss_plot(logs, f\"../models/bnn_training_loss_global.png\")\n",
    "    \n",
    "    print(\"\\n✅ Proceso global completado.\")\n",
    "    return trainer, model, global_metrics, sensor_results\n",
    "\n",
    "\n",
    "# ==================== EJEMPLO DE EJECUCIÓN GLOBAL ====================\n",
    "\n",
    "def ejemplo_global():\n",
    "    \"\"\"Ejemplo de entrenamiento global para múltiples sensores sin usar ID como predictor.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌍 EJEMPLO: ENTRENAMIENTO BNN GLOBAL MULTI-SENSOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = {\n",
    "    'sensors_train': ['28079004', '28079008', '28079011', '28079016', '28079036', '28079038', '28079039','28079040','28079047', '28079048'],\n",
    "    'sensors_test': ['28079050', '28079056', '28079035'],\n",
    "    'split_date': '2024-01-01',\n",
    "    'outlier_method': 'none',\n",
    "\n",
    "    # Conjunto núcleo, sin redundancias: una ventana representativa por familia y dirección del viento en seno/coseno\n",
    "    'features': [\n",
    "        # tráfico (nivel + retardos mínimos informativos)\n",
    "        'intensidad', 'carga',\n",
    "        'intensidad_lag8', 'carga_lag4', 'carga_lag2'\n",
    "\n",
    "        # meteorología base y suavizados representativos\n",
    "        't2m', 't2m_ma6',\n",
    "        'wind_speed_ma24', 'wind_speed_ewm3',\n",
    "        'ssrd', 'ssrd_sum24',\n",
    "        'u10_ewm6',\n",
    "        'tp_sum24',\n",
    "\n",
    "        # dirección del viento en base trigonométrica con una sola ventana\n",
    "        'wind_dir_sin_ma6', 'wind_dir_cos_ma6',\n",
    "\n",
    "        # tiempo cíclico\n",
    "        'hour_sin','hour_cos','dow_sin','dow_cos','month_sin','month_cos'\n",
    "    ],\n",
    "\n",
    "    # Monte Carlo para incertidumbre; la media no debería cambiar por reducir K\n",
    "    'K_predict': 80,\n",
    "\n",
    "    'train_config': {\n",
    "        'learning_rate': 0.003,     # 0.1 es demasiado alto con Adam en este setting\n",
    "        'n_epochs': 120,\n",
    "        'batch_size': 512,\n",
    "        'hidden_dims': [64, 32, 16],    # red más estrecha para tabular con features depurados\n",
    "        'activation': 'ReLU',\n",
    "        'beta': 0.5                  # menor peso efectivo de KL; seguirá con warmup en tu bucle\n",
    "    }\n",
    "}\n",
    "\n",
    "    \n",
    "    return main_global(config)\n",
    "\n",
    "\n",
    "ejemplo_global()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
