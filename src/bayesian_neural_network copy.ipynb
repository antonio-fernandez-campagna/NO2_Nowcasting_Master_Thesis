{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "M√≥dulo para entrenamiento y an√°lisis de Redes Neuronales Bayesianas (BNN).\n",
    "\n",
    "Este m√≥dulo proporciona una interfaz para entrenar y analizar modelos BNN\n",
    "para predecir niveles de NO2, capturando tanto la predicci√≥n como la incertidumbre.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.classes.__path__ = [] # add this line to manually set it to empty.\n",
    "\n",
    "# Constante para estabilidad num√©rica\n",
    "EPS = 1e-6\n",
    "\n",
    "# ==================== COMPONENTES BNN (Basado en bayesian_example.py) ====================\n",
    "\n",
    "class MFVILinear(nn.Module):\n",
    "    \"\"\"Capa lineal Bayesiana usando Inferencia Variacional de Campo Medio (MFVI).\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, prior_weight_std=1.0, prior_bias_std=1.0, init_std=0.05, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.weight_mean = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self.bias_mean = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        self._weight_std_param = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self._bias_std_param = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        \n",
    "        self.reset_parameters(init_std)\n",
    "\n",
    "        prior_mean = 0.0\n",
    "        self.register_buffer('prior_weight_mean', torch.full_like(self.weight_mean, prior_mean))\n",
    "        self.register_buffer('prior_weight_std', torch.full_like(self._weight_std_param, prior_weight_std))\n",
    "        self.register_buffer('prior_bias_mean', torch.full_like(self.bias_mean, prior_mean))\n",
    "        self.register_buffer('prior_bias_std', torch.full_like(self._bias_std_param, prior_bias_std))\n",
    "\n",
    "    def reset_parameters(self, init_std=0.05):\n",
    "        nn.init.kaiming_uniform_(self.weight_mean, a=math.sqrt(5))\n",
    "        bound = self.dim_in ** -0.5 if self.dim_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias_mean, -bound, bound)\n",
    "        _init_std_param = np.log(init_std)\n",
    "        self._weight_std_param.data = torch.full_like(self.weight_mean, _init_std_param)\n",
    "        self._bias_std_param.data = torch.full_like(self.bias_mean, _init_std_param)\n",
    "\n",
    "    @property\n",
    "    def weight_std(self):\n",
    "        return torch.clamp(torch.exp(self._weight_std_param), min=EPS)\n",
    "\n",
    "    @property\n",
    "    def bias_std(self):\n",
    "        return torch.clamp(torch.exp(self._bias_std_param), min=EPS)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        q_weight = dist.Normal(self.weight_mean, self.weight_std)\n",
    "        p_weight = dist.Normal(self.prior_weight_mean, self.prior_weight_std)\n",
    "        kl = dist.kl_divergence(q_weight, p_weight).sum()\n",
    "        \n",
    "        q_bias = dist.Normal(self.bias_mean, self.bias_std)\n",
    "        p_bias = dist.Normal(self.prior_bias_mean, self.prior_bias_std)\n",
    "        kl += dist.kl_divergence(q_bias, p_bias).sum()\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self._normal_sample(self.weight_mean, self.weight_std)\n",
    "        bias = self._normal_sample(self.bias_mean, self.bias_std)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _normal_sample(self, mean, std):\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + std * epsilon\n",
    "\n",
    "def make_mfvi_bnn(layer_sizes, activation='GELU', **layer_kwargs):\n",
    "    nonlinearity = getattr(nn, activation)() if isinstance(activation, str) else activation\n",
    "    net = nn.Sequential()\n",
    "    for i, (dim_in, dim_out) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        net.add_module(f'MFVILinear{i}', MFVILinear(dim_in, dim_out, **layer_kwargs))\n",
    "        if i < len(layer_sizes) - 2:\n",
    "            net.add_module(f'Nonlinarity{i}', nonlinearity)\n",
    "    return net\n",
    "\n",
    "def kl_divergence_model(bnn):\n",
    "    kl = 0.0\n",
    "    for module in bnn.modules():\n",
    "        if hasattr(module, 'kl_divergence'):\n",
    "            kl += module.kl_divergence()\n",
    "    return kl\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    l2_dist = (y - y_pred).pow(2).sum(-1)\n",
    "    return -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "def test_nll(y, y_pred, log_noise_var):\n",
    "    nll_samples = -gauss_loglik(y, y_pred, log_noise_var) # Shape (K, N_test)\n",
    "    nll = -torch.logsumexp(-nll_samples, dim=0) + math.log(nll_samples.shape[0])\n",
    "    return nll.mean()\n",
    "\n",
    "# ==================== CLASE DATASET PYTORCH ====================\n",
    "\n",
    "class BayesianDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "\n",
    "# ==================== CLASE BNN TRAINER ====================\n",
    "\n",
    "class BNNTrainer:\n",
    "    \"\"\"Clase para encapsular el entrenamiento y an√°lisis de BNNs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_master = None\n",
    "        self.model = None\n",
    "        self.scaler_dict = {}\n",
    "        self.scaler_target = None\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_parquet('../data/super_processed/7_4_no2_with_traffic_and_1meteo_and_1trafic_id.parquet')\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar los datos: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def create_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['day_of_week'] = df['fecha'].dt.dayofweek\n",
    "        df['day_of_year'] = df['fecha'].dt.dayofyear\n",
    "        df['month'] = df['fecha'].dt.month\n",
    "        df['hour'] = df['fecha'].dt.hour\n",
    "        df['day'] = df['fecha'].dt.day\n",
    "        df['weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['season'] = df['month'].apply(lambda x: (x%12 + 3)//3) # 1:winter, 2:spring, 3:summer, 4:autumn\n",
    "        \n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        return df\n",
    "\n",
    "    def remove_outliers(self, df: pd.DataFrame, method: str, columns: List[str]) -> pd.DataFrame:\n",
    "        if method == 'none': return df\n",
    "        df_filtered = df.copy()\n",
    "        for col in columns:\n",
    "            if col not in df_filtered.columns: continue\n",
    "            if method == 'iqr':\n",
    "                Q1, Q3 = df_filtered[col].quantile(0.25), df_filtered[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                if IQR > 0:\n",
    "                    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]\n",
    "            elif method == 'zscore':\n",
    "                df_filtered = df_filtered[np.abs(zscore(df_filtered[col], nan_policy='omit')) < 3]\n",
    "        return df_filtered\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, split_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        train = df[df['fecha'] < split_date].copy()\n",
    "        test = df[df['fecha'] >= split_date].copy()\n",
    "        return train, test\n",
    "\n",
    "    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        scaler_dict = {}\n",
    "        X_train_s, X_test_s = X_train.copy(), X_test.copy()\n",
    "        for feature in features:\n",
    "            if feature in X_train.columns and pd.api.types.is_numeric_dtype(X_train[feature]):\n",
    "                scaler = StandardScaler()\n",
    "                X_train_s[feature] = scaler.fit_transform(X_train[[feature]]).flatten()\n",
    "                X_test_s[feature] = scaler.transform(X_test[[feature]]).flatten()\n",
    "                scaler_dict[feature] = scaler\n",
    "        return X_train_s, X_test_s, scaler_dict\n",
    "\n",
    "    def scale_target(self, y_train: pd.Series) -> Tuple[pd.Series, StandardScaler]:\n",
    "        scaler = StandardScaler()\n",
    "        y_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        return pd.Series(y_scaled, index=y_train.index, name=y_train.name), scaler\n",
    "\n",
    "    def train_bnn_model(self, X_train, y_train, train_config):\n",
    "        # Setup\n",
    "        dataset = BayesianDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "        \n",
    "        layer_sizes = [X_train.shape[1]] + train_config['hidden_dims'] + [1]\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=train_config['activation'], device=self.device).to(self.device)\n",
    "        log_noise_var = nn.Parameter(torch.ones(1, device=self.device) * -3.0)\n",
    "        \n",
    "        params = list(model.parameters()) + [log_noise_var]\n",
    "        optimizer = torch.optim.Adam(params, lr=train_config['learning_rate'])\n",
    "        \n",
    "        # Training Loop\n",
    "        logs = []\n",
    "        N_data = len(dataset)\n",
    "        \n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "        for epoch in range(train_config['n_epochs']):\n",
    "            model.train()\n",
    "            epoch_nll, epoch_kl = 0, 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = model(x_batch)\n",
    "                nll = -gauss_loglik(y_batch, y_pred, log_noise_var).mean()\n",
    "                kl = kl_divergence_model(model)\n",
    "                loss = nll + train_config['beta'] * kl / N_data\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_nll += nll.item() * len(x_batch)\n",
    "                epoch_kl += kl.item()\n",
    "            \n",
    "            logs.append({'nll': epoch_nll / N_data, 'kl': epoch_kl / N_data})\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                 print(f\"Epoch {epoch+1}/{train_config['n_epochs']} | NLL: {logs[-1]['nll']:.3f} | KL: {logs[-1]['kl']:.3f}\")\n",
    "        \n",
    "        print(\"Entrenamiento completado.\")\n",
    "        return model, log_noise_var, logs\n",
    "\n",
    "    def predict(self, model, X_test, K, log_noise_var):\n",
    "        model.eval()\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_preds = torch.stack([model(X_test_tensor) for _ in range(K)], dim=0)\n",
    "        \n",
    "        pred_mean = y_preds.mean(0)\n",
    "        epistemic_uncertainty = y_preds.var(0).sqrt()\n",
    "        aleatoric_uncertainty = torch.exp(0.5 * log_noise_var).expand_as(pred_mean)\n",
    "        total_uncertainty = (epistemic_uncertainty**2 + aleatoric_uncertainty**2).sqrt()\n",
    "        \n",
    "        return {\n",
    "            'y_preds_all': y_preds,\n",
    "            'mean': pred_mean,\n",
    "            'epistemic_std': epistemic_uncertainty,\n",
    "            'aleatoric_std': aleatoric_uncertainty,\n",
    "            'total_std': total_uncertainty\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, predictions, y_test, y_test_scaled, scaler_target, log_noise_var):\n",
    "        # Unscale predictions\n",
    "        pred_mean_scaled = predictions['mean'].detach().cpu().numpy()\n",
    "        pred_mean = scaler_target.inverse_transform(pred_mean_scaled).flatten()\n",
    "        \n",
    "        total_std_scaled = predictions['total_std'].detach().cpu().numpy().flatten()\n",
    "        unscaled_std = total_std_scaled * scaler_target.scale_[0]\n",
    "        \n",
    "        epistemic_std = predictions['epistemic_std'].detach().cpu().numpy().flatten()\n",
    "        epistemic_unescaled_std = epistemic_std * scaler_target.scale_[0]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df_preds = pd.DataFrame({\n",
    "            'prediction': pred_mean,\n",
    "            'epistemic_uncertainty': epistemic_unescaled_std\n",
    "        })\n",
    "\n",
    "        # Save to Excel\n",
    "        df_preds.to_csv('bnn_predictions_with_epistemic_uncertainty.csv')\n",
    "\n",
    "\n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred_mean))\n",
    "        r2 = r2_score(y_test, pred_mean)\n",
    "        mae = mean_absolute_error(y_test, pred_mean)\n",
    "        \n",
    "        # Test NLL\n",
    "        y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32).reshape(-1, 1).to(self.device)\n",
    "        nll = test_nll(y_test_tensor, predictions['y_preds_all'], log_noise_var).item()\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse, 'r2': r2, 'mae': mae, 'test_nll': nll,\n",
    "            'y_pred': pred_mean, 'y_pred_std': unscaled_std\n",
    "        }\n",
    "    \n",
    "    def save_model(self, path, model, log_noise_var, scaler_dict, scaler_target, feature_names):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        model_state = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'log_noise_var': log_noise_var,\n",
    "            'scaler_dict': scaler_dict,\n",
    "            'scaler_target': scaler_target,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "        # Guardar en formato pickle\n",
    "        joblib.dump(model_state, path)\n",
    "\n",
    "    def load_model(self, path, layer_sizes, activation):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Modelo no encontrado en: {path}\")\n",
    "            return None\n",
    "        \n",
    "        # Cargar desde formato pickle\n",
    "        model_state = joblib.load(path)\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=activation, device=self.device).to(self.device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        log_noise_var = model_state['log_noise_var'].to(self.device)\n",
    "        \n",
    "        return model, log_noise_var, model_state['scaler_dict'], model_state['scaler_target'], model_state['feature_names']\n",
    "\n",
    "# ==================== FUNCIONES DE VISUALIZACI√ìN Y REPORTE ====================\n",
    "\n",
    "def print_model_metrics(metrics: Dict):\n",
    "    print(\"\\nüìä M√©tricas de Evaluaci√≥n\")\n",
    "    print(f\"  - RMSE: {metrics['rmse']:.2f} ¬µg/m¬≥ (Error Cuadr√°tico Medio)\")\n",
    "    print(f\"  - R¬≤ Score: {metrics['r2']:.3f} (Coeficiente de Determinaci√≥n)\")\n",
    "    print(f\"  - MAE: {metrics['mae']:.2f} ¬µg/m¬≥ (Error Absoluto Medio)\")\n",
    "    print(f\"  - Test NLL: {metrics['test_nll']:.3f} (Log-verosimilitud Negativa en Test, menor es mejor)\")\n",
    "\n",
    "def save_temporal_predictions_plot(test_df: pd.DataFrame, y_pred: np.ndarray, y_pred_std: np.ndarray, filename: str):\n",
    "    print(f\"\\nüìà Guardando gr√°fico de predicciones en {filename}\")\n",
    "    df_plot = test_df[['fecha', 'no2_value']].copy()\n",
    "    df_plot['Predicci√≥n'] = y_pred\n",
    "    df_plot['Incertidumbre_std'] = y_pred_std\n",
    "    df_plot = df_plot.set_index('fecha')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot(df_plot.index, df_plot['no2_value'], label='Valor Real', alpha=0.9)\n",
    "    ax.plot(df_plot.index, df_plot['Predicci√≥n'], label='Predicci√≥n Media (BNN)', linestyle='--')\n",
    "    \n",
    "    ax.fill_between(df_plot.index, \n",
    "                    df_plot['Predicci√≥n'] - 2 * df_plot['Incertidumbre_std'],\n",
    "                    df_plot['Predicci√≥n'] + 2 * df_plot['Incertidumbre_std'],\n",
    "                    color='orange', alpha=0.3, label='Incertidumbre Total (¬±2 std)')\n",
    "\n",
    "    ax.set_title(\"Predicciones del Modelo BNN vs. Datos Reales\")\n",
    "    ax.set_ylabel(\"Concentraci√≥n NO‚ÇÇ (¬µg/m¬≥)\")\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_training_loss_plot(logs: List[Dict], filename: str):\n",
    "    print(f\"üìâ Guardando curvas de entrenamiento en {filename}\")\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(log_df['nll'])\n",
    "    ax1.set_title(\"Negative Log-Likelihood (NLL)\")\n",
    "    ax1.set_xlabel(\"√âpoca\")\n",
    "    ax2.plot(log_df['kl'])\n",
    "    ax2.set_title(\"KL Divergence\")\n",
    "    ax2.set_xlabel(\"√âpoca\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ==================== FUNCI√ìN PRINCIPAL ====================\n",
    "\n",
    "def main(config):\n",
    "    print(\"üß† Entrenamiento de Red Neuronal Bayesiana (BNN)\")\n",
    "    trainer = BNNTrainer()\n",
    "    \n",
    "    print(\"\\nCargando datos...\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty: return\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è Configuraci√≥n del Experimento:\")\n",
    "    print(f\"  - Sensor: {config['sensor_id']}\")\n",
    "    print(f\"  - Fecha de divisi√≥n: {config['split_date']}\")\n",
    "    print(f\"  - Filtrado de Outliers: {config['outlier_method']}\")\n",
    "    \n",
    "    df_sensor = trainer.df_master[trainer.df_master['id_no2'] == config['sensor_id']]\n",
    "    if df_sensor.empty:\n",
    "        print(f\"Error: No se encontraron datos para el sensor {config['sensor_id']}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcesando datos...\")\n",
    "    df_processed = trainer.convert_units(df_sensor.copy())\n",
    "    df_processed = trainer.create_cyclical_features(df_processed)\n",
    "    \n",
    "    train_df, test_df = trainer.split_data(df_processed, pd.to_datetime(config['split_date']))\n",
    "    \n",
    "    train_df = trainer.remove_outliers(train_df, config['outlier_method'], ['no2_value'])\n",
    "    \n",
    "    all_features = [c for c in df_sensor.columns if c not in ['fecha', 'id_no2', 'no2_value'] and pd.api.types.is_numeric_dtype(df_sensor[c])]\n",
    "    temp_df = trainer.create_cyclical_features(df_sensor[['fecha']].copy())\n",
    "    all_features.extend([c for c in temp_df.columns if c not in df_sensor.columns])\n",
    "    \n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    \n",
    "    print(f\"\\nUsando {len(selected_features)} caracteristicas: {selected_features}\")\n",
    "\n",
    "    X_train, X_test = train_df[selected_features], test_df[selected_features]\n",
    "    y_train, y_test = train_df['no2_value'], test_df['no2_value']\n",
    "    \n",
    "    X_train, X_test, scaler_dict = trainer.scale_features(X_train, X_test, selected_features)\n",
    "    y_train_scaled, scaler_target = trainer.scale_target(y_train)\n",
    "    y_test_scaled, _ = trainer.scale_target(y_test) # Scaler is not needed here\n",
    "    \n",
    "    print(f\"Datos de entrenamiento: {len(X_train)}, Datos de prueba: {len(X_test)}\")\n",
    "\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(X_train, y_train_scaled, config['train_config'])\n",
    "    \n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    predictions = trainer.predict(model, X_test, config['K_predict'], log_noise_var)\n",
    "    metrics = trainer.evaluate_model(predictions, y_test, y_test_scaled, scaler_target, log_noise_var)\n",
    "\n",
    "    print(\"\\nüî¨ Resultados del An√°lisis\")\n",
    "    print_model_metrics(metrics)\n",
    "    \n",
    "    # Crear carpeta de resultados si no existe\n",
    "    output_dir = f\"../data/models/\"\n",
    "    \n",
    "    #save_temporal_predictions_plot(test_df, metrics['y_pred'], metrics['y_pred_std'], filename=os.path.join(output_dir, \"predictions.png\"))\n",
    "    #save_training_loss_plot(logs, filename=os.path.join(output_dir, \"training_loss.png\"))\n",
    "    \n",
    "    model_path = os.path.join(output_dir, f\"bnn_model_{config['sensor_id']}.pkl\")\n",
    "    print(f\"\\nüíæ Guardando modelo en {model_path}\")\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, selected_features)\n",
    "    print(\"Proceso completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Entrenamiento de Red Neuronal Bayesiana (BNN)\n",
      "Using device: cpu\n",
      "\n",
      "Cargando datos...\n",
      "\n",
      "‚öôÔ∏è Configuraci√≥n del Experimento:\n",
      "  - Sensor: 28079008\n",
      "  - Fecha de divisi√≥n: 2024-01-01\n",
      "  - Filtrado de Outliers: iqr\n",
      "\n",
      "Procesando datos...\n",
      "\n",
      "Usando 31 caracteristicas: ['longitud_no2', 'latitud_no2', 'distance_m', 'intensidad', 'carga', 'ocupacion', 'vmed', 'latitud_meteo', 'longitud_meteo', 'd2m', 't2m', 'ssr', 'ssrd', 'u10', 'v10', 'sp', 'tp', 'day_of_week', 'day_of_year', 'month', 'year', 'weekend', 'season', 'hour', 'day', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos']\n",
      "Datos de entrenamiento: 48794, Datos de prueba: 7777\n",
      "Iniciando entrenamiento...\n",
      "Entrenamiento completado.\n",
      "\n",
      "Evaluando modelo...\n",
      "\n",
      "üî¨ Resultados del An√°lisis\n",
      "\n",
      "üìä M√©tricas de Evaluaci√≥n\n",
      "  - RMSE: 13.00 ¬µg/m¬≥ (Error Cuadr√°tico Medio)\n",
      "  - R¬≤ Score: 0.539 (Coeficiente de Determinaci√≥n)\n",
      "  - MAE: 9.22 ¬µg/m¬≥ (Error Absoluto Medio)\n",
      "  - Test NLL: 1.383 (Log-verosimilitud Negativa en Test, menor es mejor)\n",
      "\n",
      "üíæ Guardando modelo en ../data/models/bnn_model_28079008.pkl\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Entrenar un modelo BNN para predicci√≥n de NO2.\")\n",
    "parser.add_argument(\"--sensor_id\", type=str, default=\"28079008\", help=\"ID del sensor de NO2 a utilizar.\")\n",
    "parser.add_argument(\"--split_date\", type=str, default=\"2024-01-01\", help=\"Fecha para dividir datos en entrenamiento/prueba (YYYY-MM-DD).\")\n",
    "parser.add_argument(\"--outlier_method\", type=str, default=\"iqr\", choices=['none', 'iqr', 'zscore'], help=\"M√©todo de filtrado de outliers.\")\n",
    "parser.add_argument(\"--hidden_dims\", type=str, default=\"50,50\", help=\"Dimensiones de capas ocultas, separadas por comas.\")\n",
    "parser.add_argument(\"--activation\", type=str, default=\"GELU\", choices=['GELU', 'ReLU', 'LeakyReLU'], help=\"Funci√≥n de activaci√≥n.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Tasa de aprendizaje.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=25, help=\"N√∫mero de √©pocas de entrenamiento.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256, help=\"Tama√±o del batch.\")\n",
    "parser.add_argument(\"--beta\", type=float, default=1.0, help=\"Peso para la divergencia KL (beta).\")\n",
    "parser.add_argument(\"--k_predict\", type=int, default=100, help=\"N√∫mero de muestras Monte Carlo para predicci√≥n.\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "config = {\n",
    "    'sensor_id': args.sensor_id,\n",
    "    'split_date': args.split_date,\n",
    "    'outlier_method': args.outlier_method,\n",
    "    'features': ['all'], # se usan todas por defecto\n",
    "    'K_predict': args.k_predict,\n",
    "    'train_config': {\n",
    "        'learning_rate': args.lr,\n",
    "        'n_epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'hidden_dims': [int(d.strip()) for d in args.hidden_dims.split(',')],\n",
    "        'activation': args.activation,\n",
    "        'beta': args.beta,\n",
    "    }\n",
    "}\n",
    "\n",
    "main(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
