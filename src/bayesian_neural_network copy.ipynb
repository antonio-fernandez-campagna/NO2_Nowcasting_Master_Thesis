{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Módulo para entrenamiento y análisis de Redes Neuronales Bayesianas (BNN).\n",
    "\n",
    "Este módulo proporciona una interfaz para entrenar y analizar modelos BNN\n",
    "para predecir niveles de NO2, capturando tanto la predicción como la incertidumbre.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.classes.__path__ = [] # add this line to manually set it to empty.\n",
    "\n",
    "# Constante para estabilidad numérica\n",
    "EPS = 1e-6\n",
    "\n",
    "# ==================== COMPONENTES BNN (Basado en bayesian_example.py) ====================\n",
    "\n",
    "class MFVILinear(nn.Module):\n",
    "    \"\"\"Capa lineal Bayesiana usando Inferencia Variacional de Campo Medio (MFVI).\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, prior_weight_std=1.0, prior_bias_std=1.0, init_std=0.05, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.weight_mean = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self.bias_mean = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        self._weight_std_param = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self._bias_std_param = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        \n",
    "        self.reset_parameters(init_std)\n",
    "\n",
    "        prior_mean = 0.0\n",
    "        self.register_buffer('prior_weight_mean', torch.full_like(self.weight_mean, prior_mean))\n",
    "        self.register_buffer('prior_weight_std', torch.full_like(self._weight_std_param, prior_weight_std))\n",
    "        self.register_buffer('prior_bias_mean', torch.full_like(self.bias_mean, prior_mean))\n",
    "        self.register_buffer('prior_bias_std', torch.full_like(self._bias_std_param, prior_bias_std))\n",
    "\n",
    "    def reset_parameters(self, init_std=0.05):\n",
    "        nn.init.kaiming_uniform_(self.weight_mean, a=math.sqrt(5))\n",
    "        bound = self.dim_in ** -0.5 if self.dim_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias_mean, -bound, bound)\n",
    "        _init_std_param = np.log(init_std)\n",
    "        self._weight_std_param.data = torch.full_like(self.weight_mean, _init_std_param)\n",
    "        self._bias_std_param.data = torch.full_like(self.bias_mean, _init_std_param)\n",
    "\n",
    "    @property\n",
    "    def weight_std(self):\n",
    "        return torch.clamp(torch.exp(self._weight_std_param), min=EPS)\n",
    "\n",
    "    @property\n",
    "    def bias_std(self):\n",
    "        return torch.clamp(torch.exp(self._bias_std_param), min=EPS)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        q_weight = dist.Normal(self.weight_mean, self.weight_std)\n",
    "        p_weight = dist.Normal(self.prior_weight_mean, self.prior_weight_std)\n",
    "        kl = dist.kl_divergence(q_weight, p_weight).sum()\n",
    "        \n",
    "        q_bias = dist.Normal(self.bias_mean, self.bias_std)\n",
    "        p_bias = dist.Normal(self.prior_bias_mean, self.prior_bias_std)\n",
    "        kl += dist.kl_divergence(q_bias, p_bias).sum()\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self._normal_sample(self.weight_mean, self.weight_std)\n",
    "        bias = self._normal_sample(self.bias_mean, self.bias_std)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _normal_sample(self, mean, std):\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + std * epsilon\n",
    "\n",
    "def make_mfvi_bnn(layer_sizes, activation='GELU', **layer_kwargs):\n",
    "    nonlinearity = getattr(nn, activation)() if isinstance(activation, str) else activation\n",
    "    net = nn.Sequential()\n",
    "    for i, (dim_in, dim_out) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        net.add_module(f'MFVILinear{i}', MFVILinear(dim_in, dim_out, **layer_kwargs))\n",
    "        if i < len(layer_sizes) - 2:\n",
    "            net.add_module(f'Nonlinarity{i}', nonlinearity)\n",
    "    return net\n",
    "\n",
    "def kl_divergence_model(bnn):\n",
    "    kl = 0.0\n",
    "    for module in bnn.modules():\n",
    "        if hasattr(module, 'kl_divergence'):\n",
    "            kl += module.kl_divergence()\n",
    "    return kl\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    l2_dist = (y - y_pred).pow(2).sum(-1)\n",
    "    return -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "def test_nll(y, y_pred, log_noise_var):\n",
    "    nll_samples = -gauss_loglik(y, y_pred, log_noise_var) # Shape (K, N_test)\n",
    "    nll = -torch.logsumexp(-nll_samples, dim=0) + math.log(nll_samples.shape[0])\n",
    "    return nll.mean()\n",
    "\n",
    "# ==================== CLASE DATASET PYTORCH ====================\n",
    "\n",
    "class BayesianDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "\n",
    "# ==================== CLASE BNN TRAINER ====================\n",
    "\n",
    "class BNNTrainer:\n",
    "    \"\"\"Clase para encapsular el entrenamiento y análisis de BNNs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_master = None\n",
    "        self.model = None\n",
    "        self.scaler_dict = {}\n",
    "        self.scaler_target = None\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_parquet('../data/super_processed/7_4_no2_with_traffic_and_1meteo_and_1trafic_id.parquet')\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar los datos: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def create_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df['day_of_week'] = df['fecha'].dt.dayofweek\n",
    "        df['day_of_year'] = df['fecha'].dt.dayofyear\n",
    "        df['month'] = df['fecha'].dt.month\n",
    "        df['hour'] = df['fecha'].dt.hour\n",
    "        df['day'] = df['fecha'].dt.day\n",
    "        df['weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        df['season'] = df['month'].apply(lambda x: (x%12 + 3)//3) # 1:winter, 2:spring, 3:summer, 4:autumn\n",
    "        \n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        return df\n",
    "\n",
    "    def remove_outliers(self, df: pd.DataFrame, method: str, columns: List[str]) -> pd.DataFrame:\n",
    "        if method == 'none': return df\n",
    "        df_filtered = df.copy()\n",
    "        for col in columns:\n",
    "            if col not in df_filtered.columns: continue\n",
    "            if method == 'iqr':\n",
    "                Q1, Q3 = df_filtered[col].quantile(0.25), df_filtered[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                if IQR > 0:\n",
    "                    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]\n",
    "            elif method == 'zscore':\n",
    "                df_filtered = df_filtered[np.abs(zscore(df_filtered[col], nan_policy='omit')) < 3]\n",
    "        return df_filtered\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, split_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        train = df[df['fecha'] < split_date].copy()\n",
    "        test = df[df['fecha'] >= split_date].copy()\n",
    "        return train, test\n",
    "\n",
    "    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        scaler_dict = {}\n",
    "        X_train_s, X_test_s = X_train.copy(), X_test.copy()\n",
    "        for feature in features:\n",
    "            if feature in X_train.columns and pd.api.types.is_numeric_dtype(X_train[feature]):\n",
    "                scaler = StandardScaler()\n",
    "                X_train_s[feature] = scaler.fit_transform(X_train[[feature]]).flatten()\n",
    "                X_test_s[feature] = scaler.transform(X_test[[feature]]).flatten()\n",
    "                scaler_dict[feature] = scaler\n",
    "        return X_train_s, X_test_s, scaler_dict\n",
    "\n",
    "    def scale_target(self, y_train: pd.Series) -> Tuple[pd.Series, StandardScaler]:\n",
    "        scaler = StandardScaler()\n",
    "        y_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        return pd.Series(y_scaled, index=y_train.index, name=y_train.name), scaler\n",
    "\n",
    "    def train_bnn_model(self, X_train, y_train, train_config):\n",
    "        # Setup\n",
    "        dataset = BayesianDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "        \n",
    "        layer_sizes = [X_train.shape[1]] + train_config['hidden_dims'] + [1]\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=train_config['activation'], device=self.device).to(self.device)\n",
    "        log_noise_var = nn.Parameter(torch.ones(1, device=self.device) * -3.0)\n",
    "        \n",
    "        params = list(model.parameters()) + [log_noise_var]\n",
    "        optimizer = torch.optim.Adam(params, lr=train_config['learning_rate'])\n",
    "        \n",
    "        # Training Loop\n",
    "        logs = []\n",
    "        N_data = len(dataset)\n",
    "        \n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "        for epoch in range(train_config['n_epochs']):\n",
    "            model.train()\n",
    "            epoch_nll, epoch_kl = 0, 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = model(x_batch)\n",
    "                nll = -gauss_loglik(y_batch, y_pred, log_noise_var).mean()\n",
    "                kl = kl_divergence_model(model)\n",
    "                loss = nll + train_config['beta'] * kl / N_data\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_nll += nll.item() * len(x_batch)\n",
    "                epoch_kl += kl.item()\n",
    "            \n",
    "            logs.append({'nll': epoch_nll / N_data, 'kl': epoch_kl / N_data})\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                 print(f\"Epoch {epoch+1}/{train_config['n_epochs']} | NLL: {logs[-1]['nll']:.3f} | KL: {logs[-1]['kl']:.3f}\")\n",
    "        \n",
    "        print(\"Entrenamiento completado.\")\n",
    "        return model, log_noise_var, logs\n",
    "\n",
    "    def predict(self, model, X_test, K, log_noise_var):\n",
    "        model.eval()\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_preds = torch.stack([model(X_test_tensor) for _ in range(K)], dim=0)\n",
    "        \n",
    "        pred_mean = y_preds.mean(0)\n",
    "        epistemic_uncertainty = y_preds.var(0).sqrt()\n",
    "        aleatoric_uncertainty = torch.exp(0.5 * log_noise_var).expand_as(pred_mean)\n",
    "        total_uncertainty = (epistemic_uncertainty**2 + aleatoric_uncertainty**2).sqrt()\n",
    "        \n",
    "        return {\n",
    "            'y_preds_all': y_preds,\n",
    "            'mean': pred_mean,\n",
    "            'epistemic_std': epistemic_uncertainty,\n",
    "            'aleatoric_std': aleatoric_uncertainty,\n",
    "            'total_std': total_uncertainty\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, predictions, y_test, y_test_scaled, scaler_target, log_noise_var):\n",
    "        # Unscale predictions\n",
    "        pred_mean_scaled = predictions['mean'].detach().cpu().numpy()\n",
    "        pred_mean = scaler_target.inverse_transform(pred_mean_scaled).flatten()\n",
    "        \n",
    "        total_std_scaled = predictions['total_std'].detach().cpu().numpy().flatten()\n",
    "        unscaled_std = total_std_scaled * scaler_target.scale_[0]\n",
    "        \n",
    "        epistemic_std = predictions['epistemic_std'].detach().cpu().numpy().flatten()\n",
    "        epistemic_unescaled_std = epistemic_std * scaler_target.scale_[0]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df_preds = pd.DataFrame({\n",
    "            'prediction': pred_mean,\n",
    "            'epistemic_uncertainty': epistemic_unescaled_std\n",
    "        })\n",
    "\n",
    "        # Save to Excel\n",
    "        df_preds.to_csv('bnn_predictions_with_epistemic_uncertainty.csv')\n",
    "\n",
    "\n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred_mean))\n",
    "        r2 = r2_score(y_test, pred_mean)\n",
    "        mae = mean_absolute_error(y_test, pred_mean)\n",
    "        \n",
    "        # Test NLL\n",
    "        y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32).reshape(-1, 1).to(self.device)\n",
    "        nll = test_nll(y_test_tensor, predictions['y_preds_all'], log_noise_var).item()\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse, 'r2': r2, 'mae': mae, 'test_nll': nll,\n",
    "            'y_pred': pred_mean, 'y_pred_std': unscaled_std\n",
    "        }\n",
    "    \n",
    "    def save_model(self, path, model, log_noise_var, scaler_dict, scaler_target, feature_names):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        model_state = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'log_noise_var': log_noise_var,\n",
    "            'scaler_dict': scaler_dict,\n",
    "            'scaler_target': scaler_target,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "        # Guardar en formato pickle\n",
    "        joblib.dump(model_state, path)\n",
    "\n",
    "    def load_model(self, path, layer_sizes, activation):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Modelo no encontrado en: {path}\")\n",
    "            return None\n",
    "        \n",
    "        # Cargar desde formato pickle\n",
    "        model_state = joblib.load(path)\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=activation, device=self.device).to(self.device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        log_noise_var = model_state['log_noise_var'].to(self.device)\n",
    "        \n",
    "        return model, log_noise_var, model_state['scaler_dict'], model_state['scaler_target'], model_state['feature_names']\n",
    "\n",
    "# ==================== FUNCIONES DE VISUALIZACIÓN Y REPORTE ====================\n",
    "\n",
    "def print_model_metrics(metrics: Dict):\n",
    "    print(\"\\n📊 Métricas de Evaluación\")\n",
    "    print(f\"  - RMSE: {metrics['rmse']:.2f} µg/m³ (Error Cuadrático Medio)\")\n",
    "    print(f\"  - R² Score: {metrics['r2']:.3f} (Coeficiente de Determinación)\")\n",
    "    print(f\"  - MAE: {metrics['mae']:.2f} µg/m³ (Error Absoluto Medio)\")\n",
    "    print(f\"  - Test NLL: {metrics['test_nll']:.3f} (Log-verosimilitud Negativa en Test, menor es mejor)\")\n",
    "\n",
    "def save_temporal_predictions_plot(test_df: pd.DataFrame, y_pred: np.ndarray, y_pred_std: np.ndarray, filename: str):\n",
    "    print(f\"\\n📈 Guardando gráfico de predicciones en {filename}\")\n",
    "    df_plot = test_df[['fecha', 'no2_value']].copy()\n",
    "    df_plot['Predicción'] = y_pred\n",
    "    df_plot['Incertidumbre_std'] = y_pred_std\n",
    "    df_plot = df_plot.set_index('fecha')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.plot(df_plot.index, df_plot['no2_value'], label='Valor Real', alpha=0.9)\n",
    "    ax.plot(df_plot.index, df_plot['Predicción'], label='Predicción Media (BNN)', linestyle='--')\n",
    "    \n",
    "    ax.fill_between(df_plot.index, \n",
    "                    df_plot['Predicción'] - 2 * df_plot['Incertidumbre_std'],\n",
    "                    df_plot['Predicción'] + 2 * df_plot['Incertidumbre_std'],\n",
    "                    color='orange', alpha=0.3, label='Incertidumbre Total (±2 std)')\n",
    "\n",
    "    ax.set_title(\"Predicciones del Modelo BNN vs. Datos Reales\")\n",
    "    ax.set_ylabel(\"Concentración NO₂ (µg/m³)\")\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_training_loss_plot(logs: List[Dict], filename: str):\n",
    "    print(f\"📉 Guardando curvas de entrenamiento en {filename}\")\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(log_df['nll'])\n",
    "    ax1.set_title(\"Negative Log-Likelihood (NLL)\")\n",
    "    ax1.set_xlabel(\"Época\")\n",
    "    ax2.plot(log_df['kl'])\n",
    "    ax2.set_title(\"KL Divergence\")\n",
    "    ax2.set_xlabel(\"Época\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ==================== FUNCIÓN PRINCIPAL ====================\n",
    "\n",
    "def main(config):\n",
    "    print(\"🧠 Entrenamiento de Red Neuronal Bayesiana (BNN)\")\n",
    "    trainer = BNNTrainer()\n",
    "    \n",
    "    print(\"\\nCargando datos...\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty: return\n",
    "\n",
    "    print(f\"\\n⚙️ Configuración del Experimento:\")\n",
    "    print(f\"  - Sensor: {config['sensor_id']}\")\n",
    "    print(f\"  - Fecha de división: {config['split_date']}\")\n",
    "    print(f\"  - Filtrado de Outliers: {config['outlier_method']}\")\n",
    "    \n",
    "    df_sensor = trainer.df_master[trainer.df_master['id_no2'] == config['sensor_id']]\n",
    "    if df_sensor.empty:\n",
    "        print(f\"Error: No se encontraron datos para el sensor {config['sensor_id']}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcesando datos...\")\n",
    "    df_processed = trainer.convert_units(df_sensor.copy())\n",
    "    df_processed = trainer.create_cyclical_features(df_processed)\n",
    "    \n",
    "    train_df, test_df = trainer.split_data(df_processed, pd.to_datetime(config['split_date']))\n",
    "    \n",
    "    train_df = trainer.remove_outliers(train_df, config['outlier_method'], ['no2_value'])\n",
    "    \n",
    "    all_features = [c for c in df_sensor.columns if c not in ['fecha', 'id_no2', 'no2_value'] and pd.api.types.is_numeric_dtype(df_sensor[c])]\n",
    "    temp_df = trainer.create_cyclical_features(df_sensor[['fecha']].copy())\n",
    "    all_features.extend([c for c in temp_df.columns if c not in df_sensor.columns])\n",
    "    \n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    \n",
    "    print(f\"\\nUsando {len(selected_features)} caracteristicas: {selected_features}\")\n",
    "\n",
    "    X_train, X_test = train_df[selected_features], test_df[selected_features]\n",
    "    y_train, y_test = train_df['no2_value'], test_df['no2_value']\n",
    "    \n",
    "    X_train, X_test, scaler_dict = trainer.scale_features(X_train, X_test, selected_features)\n",
    "    y_train_scaled, scaler_target = trainer.scale_target(y_train)\n",
    "    y_test_scaled, _ = trainer.scale_target(y_test) # Scaler is not needed here\n",
    "    \n",
    "    print(f\"Datos de entrenamiento: {len(X_train)}, Datos de prueba: {len(X_test)}\")\n",
    "\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(X_train, y_train_scaled, config['train_config'])\n",
    "    \n",
    "    print(\"\\nEvaluando modelo...\")\n",
    "    predictions = trainer.predict(model, X_test, config['K_predict'], log_noise_var)\n",
    "    metrics = trainer.evaluate_model(predictions, y_test, y_test_scaled, scaler_target, log_noise_var)\n",
    "\n",
    "    print(\"\\n🔬 Resultados del Análisis\")\n",
    "    print_model_metrics(metrics)\n",
    "    \n",
    "    # Crear carpeta de resultados si no existe\n",
    "    output_dir = f\"../data/models/\"\n",
    "    \n",
    "    #save_temporal_predictions_plot(test_df, metrics['y_pred'], metrics['y_pred_std'], filename=os.path.join(output_dir, \"predictions.png\"))\n",
    "    #save_training_loss_plot(logs, filename=os.path.join(output_dir, \"training_loss.png\"))\n",
    "    \n",
    "    model_path = os.path.join(output_dir, f\"bnn_model_{config['sensor_id']}.pkl\")\n",
    "    print(f\"\\n💾 Guardando modelo en {model_path}\")\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, selected_features)\n",
    "    print(\"Proceso completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Entrenamiento de Red Neuronal Bayesiana (BNN)\n",
      "Using device: cpu\n",
      "\n",
      "Cargando datos...\n",
      "\n",
      "⚙️ Configuración del Experimento:\n",
      "  - Sensor: 28079008\n",
      "  - Fecha de división: 2024-01-01\n",
      "  - Filtrado de Outliers: iqr\n",
      "\n",
      "Procesando datos...\n",
      "\n",
      "Usando 31 caracteristicas: ['longitud_no2', 'latitud_no2', 'distance_m', 'intensidad', 'carga', 'ocupacion', 'vmed', 'latitud_meteo', 'longitud_meteo', 'd2m', 't2m', 'ssr', 'ssrd', 'u10', 'v10', 'sp', 'tp', 'day_of_week', 'day_of_year', 'month', 'year', 'weekend', 'season', 'hour', 'day', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos']\n",
      "Datos de entrenamiento: 48794, Datos de prueba: 7777\n",
      "Iniciando entrenamiento...\n",
      "Entrenamiento completado.\n",
      "\n",
      "Evaluando modelo...\n",
      "\n",
      "🔬 Resultados del Análisis\n",
      "\n",
      "📊 Métricas de Evaluación\n",
      "  - RMSE: 13.00 µg/m³ (Error Cuadrático Medio)\n",
      "  - R² Score: 0.539 (Coeficiente de Determinación)\n",
      "  - MAE: 9.22 µg/m³ (Error Absoluto Medio)\n",
      "  - Test NLL: 1.383 (Log-verosimilitud Negativa en Test, menor es mejor)\n",
      "\n",
      "💾 Guardando modelo en ../data/models/bnn_model_28079008.pkl\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Entrenar un modelo BNN para predicción de NO2.\")\n",
    "parser.add_argument(\"--sensor_id\", type=str, default=\"28079008\", help=\"ID del sensor de NO2 a utilizar.\")\n",
    "parser.add_argument(\"--split_date\", type=str, default=\"2024-01-01\", help=\"Fecha para dividir datos en entrenamiento/prueba (YYYY-MM-DD).\")\n",
    "parser.add_argument(\"--outlier_method\", type=str, default=\"iqr\", choices=['none', 'iqr', 'zscore'], help=\"Método de filtrado de outliers.\")\n",
    "parser.add_argument(\"--hidden_dims\", type=str, default=\"50,50\", help=\"Dimensiones de capas ocultas, separadas por comas.\")\n",
    "parser.add_argument(\"--activation\", type=str, default=\"GELU\", choices=['GELU', 'ReLU', 'LeakyReLU'], help=\"Función de activación.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Tasa de aprendizaje.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=25, help=\"Número de épocas de entrenamiento.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256, help=\"Tamaño del batch.\")\n",
    "parser.add_argument(\"--beta\", type=float, default=1.0, help=\"Peso para la divergencia KL (beta).\")\n",
    "parser.add_argument(\"--k_predict\", type=int, default=100, help=\"Número de muestras Monte Carlo para predicción.\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "config = {\n",
    "    'sensor_id': args.sensor_id,\n",
    "    'split_date': args.split_date,\n",
    "    'outlier_method': args.outlier_method,\n",
    "    'features': ['all'], # se usan todas por defecto\n",
    "    'K_predict': args.k_predict,\n",
    "    'train_config': {\n",
    "        'learning_rate': args.lr,\n",
    "        'n_epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'hidden_dims': [int(d.strip()) for d in args.hidden_dims.split(',')],\n",
    "        'activation': args.activation,\n",
    "        'beta': args.beta,\n",
    "    }\n",
    "}\n",
    "\n",
    "main(config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
