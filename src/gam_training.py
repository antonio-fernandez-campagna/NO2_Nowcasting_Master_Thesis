"""
M√≥dulo refactorizado para entrenamiento y an√°lisis de modelos GAM (Generalized Additive Models).

Este m√≥dulo proporciona una interfaz integrada para entrenar y analizar modelos GAM
para predecir niveles de NO2 bas√°ndose en variables de tr√°fico, meteorol√≥gicas y temporales.
"""

import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
import numpy as np
from datetime import timedelta, datetime
import seaborn as sns
import joblib
import os
from pygam import LinearGAM, s
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')

# Importar configuraciones centralizadas
from src.config import (
    OUTLIER_METHODS, PREPROCESSING_OPTIONS, VARIABLE_CATEGORIES, 
    VARIABLE_METADATA, COLUMNS_FOR_OUTLIERS, TRAFFIC_PLOT_VARIABLES,
    METEO_PLOT_VARIABLES, get_available_traffic_variables, get_available_meteo_variables
)


# ==================== CLASE PRINCIPAL ====================

class GAMTrainer:
    """Clase principal para entrenamiento y an√°lisis de modelos GAM."""
    
    def __init__(self):
        self.df_master = None
        self.model = None
        self.scaler_dict = {}
        self.scaler_target = None
        self._initialize_session_state()
    
    def _initialize_session_state(self):
        """Inicializa el estado de la sesi√≥n."""
        if 'training_data_loaded' not in st.session_state:
            st.session_state.training_data_loaded = False
        if 'model_trained' not in st.session_state:
            st.session_state.model_trained = False
        if 'training_config' not in st.session_state:
            st.session_state.training_config = {}
    
    @st.cache_data(ttl=3600)
    def load_data(_self) -> pd.DataFrame:
        """Carga y preprocesa los datos con cach√©."""
        try:
            # Usar el nuevo dataset con todas las caracter√≠sticas engineered
            df = pd.read_parquet('data/super_processed/7_4_no2_with_traffic_and_1meteo_and_1trafic_id.parquet')
            df['fecha'] = pd.to_datetime(df['fecha'])
            return df
        except Exception as e:
            st.error(f"Error al cargar los datos: {str(e)}")
            return pd.DataFrame()
    
    def create_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Crea variables c√≠clicas para capturar patrones temporales."""
        df = df.copy()

        # Crear variables temporales base
        df['day_of_week'] = df['fecha'].dt.dayofweek
        df['day_of_year'] = df['fecha'].dt.dayofyear
        df['month'] = df['fecha'].dt.month
        df['year'] = df['fecha'].dt.year
        df['weekend'] = df['day_of_week'].isin([5,6]).astype(int)
        df['hour'] = df['fecha'].dt.hour
        df['day'] = df['fecha'].dt.day
        
        # Crear variable estacional num√©rica (0-3: winter, spring, summer, autumn)
        df['season'] = df['month'].apply(
            lambda x: 0 if x in [12,1,2] else 1 if x in [3,4,5] else 2 if x in [6,7,8] else 3
        )
        
        # Variables c√≠clicas temporales b√°sicas
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)
        df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)
        
        # Variables c√≠clicas adicionales m√°s espec√≠ficas
        df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)
        df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)
        df['season_sin'] = np.sin(2 * np.pi * df['season'] / 4)
        df['season_cos'] = np.cos(2 * np.pi * df['season'] / 4)
        
        return df

    
    def remove_outliers(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """Elimina outliers seg√∫n el m√©todo especificado."""
        if method == 'none':
            return df
        
        df_filtered = df.copy()
        
        if method == 'iqr':
            for col in COLUMNS_FOR_OUTLIERS:
                if col in df_filtered.columns:
                    Q1 = df_filtered[col].quantile(0.25)
                    Q3 = df_filtered[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower = Q1 - 1.5 * IQR
                    upper = Q3 + 1.5 * IQR
                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]
        
        elif method == 'zscore':
            from scipy.stats import zscore
            z_scores = df_filtered[COLUMNS_FOR_OUTLIERS].apply(zscore)
            condition = (z_scores.abs() < 3.0).all(axis=1)
            df_filtered = df_filtered[condition]
        
        elif method == 'quantiles':
            for col in COLUMNS_FOR_OUTLIERS:
                if col in df_filtered.columns:
                    lower = df_filtered[col].quantile(0.01)
                    upper = df_filtered[col].quantile(0.99)
                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]
        
        return df_filtered
    
    def split_data(self, df: pd.DataFrame, split_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Divide los datos en conjuntos de entrenamiento y prueba."""
        train_df = df[df['fecha'] < split_date]
        test_df = df[df['fecha'] >= split_date]
        return train_df, test_df
    
    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
        """Escala las caracter√≠sticas usando StandardScaler."""
        scaler_dict = {}
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()
        
        for feature in features:
            if feature in X_train.columns:
                scaler = StandardScaler()
                X_train_scaled[feature] = scaler.fit_transform(X_train[[feature]])
                X_test_scaled[feature] = scaler.transform(X_test[[feature]])
                scaler_dict[feature] = scaler
        
        return X_train_scaled, X_test_scaled, scaler_dict
    
    def scale_target(self, y_train: pd.Series) -> Tuple[np.ndarray, StandardScaler]:
        """Escala la variable objetivo."""
        scaler = StandardScaler()
        y_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()
        return y_scaled, scaler
    
    def train_gam_model(self, X_train: pd.DataFrame, y_train: np.ndarray, feature_names: List[str]) -> LinearGAM:
        """Entrena el modelo GAM."""
        if len(feature_names) == 0:
            raise ValueError("No se han proporcionado caracter√≠sticas para el modelo")
        
        # Crear t√©rminos spline para cada caracter√≠stica
        terms = []
        for i in range(len(feature_names)):
            if any(temp in feature_names[i] for temp in ['sin', 'cos']):
                # Variables c√≠clicas: m√≠nimo 4 splines (> spline_order=3)
                terms.append(s(i, n_splines=5, spline_order=2))
            else:
                # Otras variables: m√°s splines para capturar no-linealidades
                terms.append(s(i, n_splines=8, spline_order=3))
        
        # Verificar que tenemos t√©rminos v√°lidos
        if not terms:
            raise ValueError("No se pudieron crear t√©rminos GAM v√°lidos")
        
        # Crear modelo GAM con regularizaci√≥n
        #try:
        # Construir la f√≥rmula GAM correctamente
        if len(terms) == 1:
            gam_formula = terms[0]
        else:
            gam_formula = terms[0]
            for term in terms[1:]:
                gam_formula = gam_formula + term
        
        gam = LinearGAM(gam_formula, max_iter=1000)
        
        # Entrenar modelo con manejo de errores
        gam.fit(X_train.values, y_train)
        gam.feature_names = feature_names
        
        print("Feature names:", gam.feature_names)
        
        return gam



    
    
    def evaluate_model(self, model: LinearGAM, X_test: pd.DataFrame, y_test: pd.Series, scaler_target: StandardScaler) -> Dict:
        """Eval√∫a el modelo y calcula m√©tricas."""
        import os
        import pandas as pd
        y_pred_scaled = model.predict(X_test.values)
        y_pred = scaler_target.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()

        metrics = {
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'r2': r2_score(y_test, y_pred),
            'mae': mean_absolute_error(y_test, y_pred),
            'y_pred': y_pred,
            'residuals': y_test - y_pred
        }

        # Guardar y_pred en /data/predictions/ con el mismo nombre base que el modelo
        predictions_dir = 'data/predictions'
        os.makedirs(predictions_dir, exist_ok=True)
        # Buscar el nombre del modelo si existe en self, si no, usar un nombre gen√©rico
        model_filename = getattr(self, 'last_model_filename', None)
        if not model_filename:
            # fallback: nombre gen√©rico
            model_filename = 'gam_model_temp.pkl'
        csv_filename = os.path.splitext(os.path.basename(model_filename))[0] + '.csv'
        csv_path = os.path.join(predictions_dir, csv_filename)
        # Guardar y_pred junto con el √≠ndice de X_test
        pred_df = pd.DataFrame({'y_pred': y_pred}, index=X_test.index)
        pred_df.to_csv(csv_path)

        return metrics
    
    def save_model(self, model: LinearGAM, feature_names: List[str], scaler_dict: Dict, 
                   scaler_target: StandardScaler, sensor_id: str, outlier_method: str, 
                   preprocessing: str) -> str:
        """Guarda el modelo entrenado."""
        model_info = {
            'model': model,
            'feature_names': feature_names,
            'scaler_dict': scaler_dict,
            'scaler_target': scaler_target,
            'variable_metadata': VARIABLE_METADATA
        }
        
        # Crear directorio si no existe
        os.makedirs('data/models', exist_ok=True)
        
        filename = f'data/models/gam_model_{sensor_id}_{outlier_method}_{preprocessing}.pkl'
        joblib.dump(model_info, filename)
        # Guardar el nombre del modelo en el atributo para que evaluate_model lo use
        self.last_model_filename = filename
        return filename
    
    def load_model(self, filepath: str) -> Optional[Dict]:
        """Carga un modelo guardado."""
        try:
            return joblib.load(filepath)
        except Exception as e:
            st.error(f"Error al cargar el modelo: {str(e)}")
            return None
    
    def get_partial_effects(self, model: LinearGAM, feature_names: List[str], 
                           values: np.ndarray, feature_name: str = None, 
                           sin_name: str = None, cos_name: str = None) -> List[float]:
        """Calcula efectos parciales para visualizaci√≥n."""
        effects = []
        
        try:
            for val in values:
                XX = np.zeros((1, len(feature_names)))
                
                if sin_name and cos_name:
                    # Variables c√≠clicas - usar el rango correcto para cada tipo
                    if 'hour' in sin_name:
                        period = 24
                    elif 'month' in sin_name:
                        period = 12
                    elif 'day_of_week' in sin_name:
                        period = 7
                    elif 'day_of_year' in sin_name:
                        period = 365.25
                    elif 'season' in sin_name:
                        period = 4
                    elif 'day' in sin_name and 'day_of' not in sin_name:
                        period = 31
                    else:
                        period = len(values)
                    
                    sin_val = np.sin(2 * np.pi * val / period)
                    cos_val = np.cos(2 * np.pi * val / period)
                    
                    if sin_name in feature_names and cos_name in feature_names:
                        XX[0, feature_names.index(sin_name)] = sin_val
                        XX[0, feature_names.index(cos_name)] = cos_val
                        
                        # Calcular efecto combinado
                        try:
                            sin_eff = model.partial_dependence(term=feature_names.index(sin_name), X=XX)
                            cos_eff = model.partial_dependence(term=feature_names.index(cos_name), X=XX)
                            effects.append(float(sin_eff + cos_eff))
                        except:
                            # Fallback: usar predicci√≥n completa
                            pred = model.predict(XX)
                            effects.append(float(pred[0]))
                    else:
                        effects.append(0.0)
                
                elif feature_name and feature_name in feature_names:
                    # Variable simple
                    XX[0, feature_names.index(feature_name)] = val
                    try:
                        eff = model.partial_dependence(term=feature_names.index(feature_name), X=XX)
                        effects.append(float(eff))
                    except:
                        # Fallback: usar predicci√≥n completa
                        pred = model.predict(XX)
                        effects.append(float(pred[0]))
                else:
                    effects.append(0.0)
                    
        except Exception as e:
            st.warning(f"Error calculando efectos parciales: {str(e)}")
            # Retornar efectos neutros
            effects = [0.0] * len(values)
        
        return effects
    
    def validate_gam_config(self, feature_names: List[str], X_train: pd.DataFrame) -> bool:
        """Valida la configuraci√≥n del modelo GAM antes del entrenamiento."""
        try:
            st.write("üîç **Validando configuraci√≥n GAM:**")
            
            # Verificar que la lista de features no est√© vac√≠a
            if not feature_names:
                st.error("‚ùå Lista de features vac√≠a")
                return False
            
            # Verificar que tenemos suficientes datos
            min_samples = max(20, len(feature_names) * 5)
            if len(X_train) < min_samples:
                st.error(f"‚ùå Datos insuficientes: {len(X_train)} < {min_samples} m√≠nimo requerido")
                return False
            
            # Verificar que todas las features existen en X_train
            missing_in_data = [f for f in feature_names if f not in X_train.columns]
            if missing_in_data:
                st.error(f"‚ùå Features no encontradas en datos: {missing_in_data}")
                st.write("Columnas disponibles en X_train:")
                st.write(list(X_train.columns))
                return False
            
            # Verificar caracter√≠sticas de las variables
            cyclical_vars = [f for f in feature_names if any(temp in f for temp in ['sin', 'cos'])]
            regular_vars = [f for f in feature_names if f not in cyclical_vars]
            
            st.write(f"- ‚úÖ Variables c√≠clicas: {len(cyclical_vars)} {cyclical_vars[:3]}{'...' if len(cyclical_vars) > 3 else ''}")
            st.write(f"- ‚úÖ Variables regulares: {len(regular_vars)} {regular_vars[:3]}{'...' if len(regular_vars) > 3 else ''}")
            st.write(f"- ‚úÖ Total features: {len(feature_names)}")
            st.write(f"- ‚úÖ Muestras entrenamiento: {len(X_train)}")
            
            # Verificar varianza de las variables
            low_variance_vars = []
            infinite_vars = []
            for var in feature_names:
                if var in X_train.columns:
                    var_data = X_train[var]
                    
                    # Verificar infinitos
                    if np.isinf(var_data).any():
                        infinite_vars.append(var)
                        continue
                    
                    # Verificar varianza
                    var_val = var_data.var()
                    if var_val < 1e-8:
                        low_variance_vars.append((var, var_val))
            
            if infinite_vars:
                st.error(f"‚ùå Variables con valores infinitos: {infinite_vars}")
                return False
            
            if low_variance_vars:
                st.warning(f"‚ö†Ô∏è Variables con varianza muy baja: {[f'{var}: {val:.2e}' for var, val in low_variance_vars]}")
            
            # Verificar valores NaN
            nan_vars = []
            for var in feature_names:
                if var in X_train.columns:
                    nan_count = X_train[var].isna().sum()
                    if nan_count > 0:
                        nan_vars.append((var, nan_count))
            
            if nan_vars:
                st.error(f"‚ùå Variables con valores NaN: {[(var, count) for var, count in nan_vars]}")
                return False
            
            # Mostrar estad√≠sticas de las primeras features
            st.write("üìä **Estad√≠sticas de muestra:**")
            sample_features = feature_names[:5]  # Mostrar solo las primeras 5
            for var in sample_features:
                if var in X_train.columns:
                    data = X_train[var]
                    st.write(f"  - {var}: min={data.min():.3f}, max={data.max():.3f}, mean={data.mean():.3f}, std={data.std():.3f}")
            
            st.success("‚úÖ Configuraci√≥n GAM v√°lida")
            return True
            
        except Exception as e:
            st.error(f"‚ùå Error validando configuraci√≥n: {str(e)}")
            st.exception(e)
            return False


# ==================== FUNCIONES DE VISUALIZACI√ìN ====================

def show_model_metrics(metrics: Dict):
    """Muestra las m√©tricas del modelo."""
    st.subheader("üìä M√©tricas del Modelo")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "RMSE", 
            f"{metrics['rmse']:.2f} Œºg/m¬≥",
            help="Error cuadr√°tico medio - menor es mejor"
        )
    
    with col2:
        st.metric(
            "R¬≤ Score", 
            f"{metrics['r2']:.3f}",
            help="Coeficiente de determinaci√≥n - m√°s cercano a 1 es mejor"
        )
    
    with col3:
        st.metric(
            "MAE", 
            f"{metrics['mae']:.2f} Œºg/m¬≥",
            help="Error absoluto medio - menor es mejor"
        )


def show_residual_analysis(residuals: np.ndarray, sensor_id: str = None):
    """Muestra an√°lisis de residuos."""
    st.subheader("üìà An√°lisis de Residuos")
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # Histograma de residuos
    sns.histplot(residuals, kde=True, ax=axes[0])
    title_prefix = f"{sensor_id} - " if sensor_id else ""
    axes[0].set_title(f'{title_prefix}Distribuci√≥n de Residuos')
    axes[0].set_xlabel('Residuo (Œºg/m¬≥)')
    axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
    
    # Gr√°fico Q-Q
    sm.qqplot(residuals, line='45', ax=axes[1], fit=True)
    axes[1].set_title(f'{title_prefix}Gr√°fico Q-Q de Residuos')
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)


def show_temporal_effects(trainer: GAMTrainer, model: LinearGAM, feature_names: List[str], sensor_id: str = None):
    """Muestra efectos temporales del modelo."""
    st.subheader("üïê Efectos Temporales")
    
    # Primera fila: Hora, D√≠a de la semana, Mes
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**Efecto de la Hora**")
        hours = np.arange(0, 24)
        if 'hour_sin' in feature_names and 'hour_cos' in feature_names:
            hour_effects = trainer.get_partial_effects(
                model, feature_names, hours, 
                sin_name='hour_sin', cos_name='hour_cos'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(hours, hour_effects, marker='o', linewidth=2, markersize=4)
            title_prefix = f"{sensor_id} - " if sensor_id else ""
            ax.set_title(f'{title_prefix}Efecto de la Hora en NO‚ÇÇ')
            ax.set_xlabel('Hora del d√≠a')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3)
            ax.set_xticks(range(0, 24, 2))
            st.pyplot(fig)
            plt.close(fig)
    
    with col2:
        st.write("**Efecto del D√≠a de la Semana**")
        days = np.arange(0, 7)
        if 'day_of_week_sin' in feature_names and 'day_of_week_cos' in feature_names:
            day_effects = trainer.get_partial_effects(
                model, feature_names, days,
                sin_name='day_of_week_sin', cos_name='day_of_week_cos'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(days, day_effects, marker='o', linewidth=2, markersize=6)
            ax.set_title(f'{title_prefix}Efecto del D√≠a de la Semana')
            ax.set_xlabel('D√≠a de la semana')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3)
            
            day_labels = ['L', 'M', 'X', 'J', 'V', 'S', 'D']
            ax.set_xticks(range(7))
            ax.set_xticklabels(day_labels)
            
            # Resaltar fin de semana
            weekend_color = 'lightcoral' if len(day_effects) > 5 else 'lightblue'
            ax.axvspan(4.5, 6.5, alpha=0.2, color=weekend_color, label='Fin de semana')
            ax.legend()
            st.pyplot(fig)
            plt.close(fig)
    
    with col3:
        st.write("**Efecto del Mes**")
        months = np.arange(1, 13)
        if 'month_sin' in feature_names and 'month_cos' in feature_names:
            month_effects = trainer.get_partial_effects(
                model, feature_names, months,
                sin_name='month_sin', cos_name='month_cos'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(months, month_effects, marker='o', linewidth=2, markersize=4)
            ax.set_title(f'{title_prefix}Efecto del Mes')
            ax.set_xlabel('Mes')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3)
            ax.set_xticks(range(1, 13))
            
            # Etiquetas de meses
            month_labels = ['E', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']
            ax.set_xticklabels(month_labels)
            st.pyplot(fig)
            plt.close(fig)
    
    # Segunda fila: D√≠a del a√±o, Estaciones, Efecto fin de semana
    st.markdown("---")
    col4, col5, col6 = st.columns(3)
    
    with col4:
        st.write("**Efecto del D√≠a del A√±o**")
        if 'day_of_year_sin' in feature_names and 'day_of_year_cos' in feature_names:
            day_year_range = np.arange(1, 366, 10)  # Cada 10 d√≠as para mejor visualizaci√≥n
            day_year_effects = trainer.get_partial_effects(
                model, feature_names, day_year_range,
                sin_name='day_of_year_sin', cos_name='day_of_year_cos'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(day_year_range, day_year_effects, linewidth=2)
            ax.set_title(f'{title_prefix}Efecto del D√≠a del A√±o')
            ax.set_xlabel('D√≠a del a√±o')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3)
            
            # Marcar estaciones
            season_starts = [1, 80, 172, 266]  # Aproximado: invierno, primavera, verano, oto√±o
            season_names = ['Invierno', 'Primavera', 'Verano', 'Oto√±o']
            colors = ['lightblue', 'lightgreen', 'yellow', 'orange']
            
            for i, (start, name, color) in enumerate(zip(season_starts, season_names, colors)):
                end = season_starts[i+1] if i < 3 else 365
                ax.axvspan(start, end, alpha=0.1, color=color, label=name)
            
            ax.legend(loc='upper right', fontsize=8)
            st.pyplot(fig)
            plt.close(fig)
    
    with col5:
        st.write("**Efecto de las Estaciones**")
        if 'season_sin' in feature_names and 'season_cos' in feature_names:
            seasons = np.arange(0, 4)
            season_effects = trainer.get_partial_effects(
                model, feature_names, seasons,
                sin_name='season_sin', cos_name='season_cos'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            bars = ax.bar(seasons, season_effects, 
                         color=['lightblue', 'lightgreen', 'yellow', 'orange'],
                         alpha=0.7)
            ax.set_title(f'{title_prefix}Efecto de las Estaciones')
            ax.set_xlabel('Estaci√≥n')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3, axis='y')
            
            season_labels = ['Invierno', 'Primavera', 'Verano', 'Oto√±o']
            ax.set_xticks(range(4))
            ax.set_xticklabels(season_labels, rotation=45)
            
            # A√±adir valores en las barras
            for bar, effect in zip(bars, season_effects):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{effect:.2f}', ha='center', va='bottom')
            
            st.pyplot(fig)
            plt.close(fig)
    
    with col6:
        st.write("**Efecto Fin de Semana**")
        if 'weekend' in feature_names:
            weekend_values = [0, 1]  # Laborable, Fin de semana
            weekend_effects = trainer.get_partial_effects(
                model, feature_names, weekend_values, feature_name='weekend'
            )
            
            fig, ax = plt.subplots(figsize=(8, 4))
            bars = ax.bar(weekend_values, weekend_effects, 
                         color=['lightblue', 'lightcoral'],
                         alpha=0.7)
            ax.set_title(f'{title_prefix}Efecto Fin de Semana')
            ax.set_xlabel('Tipo de d√≠a')
            ax.set_ylabel('Efecto en NO‚ÇÇ')
            ax.grid(True, alpha=0.3, axis='y')
            
            weekend_labels = ['Laborable', 'Fin de semana']
            ax.set_xticks(range(2))
            ax.set_xticklabels(weekend_labels)
            
            # A√±adir valores en las barras
            for bar, effect in zip(bars, weekend_effects):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{effect:.2f}', ha='center', va='bottom')
            
            st.pyplot(fig)
            plt.close(fig)


def show_variable_effects(trainer: GAMTrainer, model: LinearGAM, feature_names: List[str], sensor_id: str = None):
    """
    Muestra efectos de variables espec√≠ficas.
    
    Ahora autom√°ticamente detecta y plotea todas las variables disponibles
    bas√°ndose en la configuraci√≥n centralizada en src.config.
    
    Args:
        trainer: Instancia del entrenador GAM
        model: Modelo GAM entrenado
        feature_names: Lista de nombres de features del modelo
    """
    st.subheader("üî¨ Efectos de Variables por Categor√≠a")
    
    # Crear tabs para diferentes categor√≠as de variables
    tab1, tab2, tab3 = st.tabs(["üöó Variables de Tr√°fico", "üå§Ô∏è Variables Meteorol√≥gicas", "üìä Comparaci√≥n General"])
    
    with tab1:
        st.write("### Efectos de Variables de Tr√°fico")
        
        # Obtener autom√°ticamente todas las variables de tr√°fico disponibles
        traffic_variables = get_available_traffic_variables(feature_names)
        
        if not traffic_variables:
            st.warning("No se encontraron variables de tr√°fico en el modelo.")
            return
        
        st.write(f"üìä **Variables de tr√°fico detectadas**: {len(traffic_variables)}")
        
        # Mostrar efectos de tr√°fico en cuadr√≠cula din√°mica
        cols = st.columns(2)
        for i, (var, config) in enumerate(traffic_variables.items()):
            if var in feature_names:
                with cols[i % 2]:
                    min_val, max_val = config['range']
                    values = np.linspace(min_val, max_val, 50)
                    effects = trainer.get_partial_effects(
                        model, feature_names, values, feature_name=var
                    )
                    
                    fig, ax = plt.subplots(figsize=(8, 5))
                    ax.plot(values, effects, color=config['color'], linewidth=2.5)
                    ax.fill_between(values, effects, alpha=0.3, color=config['color'])
                    title_prefix = f"{sensor_id} - " if sensor_id else ""
                    ax.set_title(f"{title_prefix}Efecto de {config['title']} en NO‚ÇÇ", fontsize=12, fontweight='bold')
                    ax.set_xlabel(f"{config['title']} ({config['unit']})")
                    ax.set_ylabel('Efecto en NO‚ÇÇ (Œºg/m¬≥)')
                    ax.grid(True, alpha=0.3)
                    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                    
                    # A√±adir anotaciones sobre rangos t√≠picos
                    if var == 'intensidad':
                        ax.axvspan(0, 300, alpha=0.1, color='green', label='Tr√°fico bajo')
                        ax.axvspan(300, 800, alpha=0.1, color='yellow', label='Tr√°fico medio')
                        ax.axvspan(800, 1500, alpha=0.1, color='red', label='Tr√°fico alto')
                        ax.legend(loc='upper right', fontsize=8)
                    elif var in ['carga', 'ocupacion']:
                        ax.axvspan(0, 30, alpha=0.1, color='green', label='Bajo')
                        ax.axvspan(30, 70, alpha=0.1, color='yellow', label='Medio')
                        ax.axvspan(70, 100, alpha=0.1, color='red', label='Alto')
                        ax.legend(loc='upper right', fontsize=8)
                    
                    st.pyplot(fig)
                    plt.close(fig)
                    
                    # Mostrar estad√≠sticas del efecto
                    max_effect = np.max(effects)
                    min_effect = np.min(effects)
                    effect_range = max_effect - min_effect
                    st.write(f"üìä **Rango de efecto**: {effect_range:.2f} Œºg/m¬≥")
                    st.write(f"üî∫ **M√°ximo efecto**: {max_effect:.2f} Œºg/m¬≥")
                    st.write(f"üîª **M√≠nimo efecto**: {min_effect:.2f} Œºg/m¬≥")
    
    with tab2:
        st.write("### Efectos de Variables Meteorol√≥gicas")
        
        # Obtener autom√°ticamente todas las variables meteorol√≥gicas disponibles
        meteo_variables = get_available_meteo_variables(feature_names)
        
        if not meteo_variables:
            st.warning("No se encontraron variables meteorol√≥gicas en el modelo.")
            return
        
        st.write(f"üå§Ô∏è **Variables meteorol√≥gicas detectadas**: {len(meteo_variables)}")
        
        # Mostrar efectos meteorol√≥gicos en cuadr√≠cula din√°mica
        cols = st.columns(2)
        for i, (var, config) in enumerate(meteo_variables.items()):
            if var in feature_names:
                with cols[i % 2]:
                    min_val, max_val = config['range']
                    values = np.linspace(min_val, max_val, 50)
                    effects = trainer.get_partial_effects(
                        model, feature_names, values, feature_name=var
                    )
                    
                    fig, ax = plt.subplots(figsize=(8, 5))
                    ax.plot(values, effects, color=config['color'], linewidth=2.5)
                    ax.fill_between(values, effects, alpha=0.3, color=config['color'])
                    ax.set_title(f"{title_prefix}Efecto de {config['title']} en NO‚ÇÇ", fontsize=12, fontweight='bold')
                    ax.set_xlabel(f"{config['title']} ({config['unit']})")
                    ax.set_ylabel('Efecto en NO‚ÇÇ (Œºg/m¬≥)')
                    ax.grid(True, alpha=0.3)
                    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                    
                    # A√±adir referencias espec√≠ficas
                    if var == 't2m':
                        ax.axvline(x=0, color='blue', linestyle=':', alpha=0.7, label='Congelaci√≥n')
                        ax.axvline(x=25, color='red', linestyle=':', alpha=0.7, label='Calor')
                        ax.legend(fontsize=8)
                    elif var == 'sp':
                        ax.axvline(x=1013.25, color='black', linestyle=':', alpha=0.7, label='Presi√≥n est√°ndar')
                        ax.legend(fontsize=8)
                    elif var == 'tp':
                        ax.axvline(x=0.1, color='blue', linestyle=':', alpha=0.7, label='Lluvia ligera')
                        ax.axvline(x=2.5, color='red', linestyle=':', alpha=0.7, label='Lluvia fuerte')
                        ax.legend(fontsize=8)
                    
                    st.pyplot(fig)
                    plt.close(fig)
                    
                    # Mostrar estad√≠sticas del efecto
                    max_effect = np.max(effects)
                    min_effect = np.min(effects)
                    effect_range = max_effect - min_effect
                    st.write(f"üìä **Rango de efecto**: {effect_range:.2f} Œºg/m¬≥")
                    
                    # Interpretaci√≥n espec√≠fica por variable
                    if var == 't2m':
                        if max_effect > min_effect:
                            st.write("üå°Ô∏è **Interpretaci√≥n**: Mayor temperatura ‚Üí Mayor NO‚ÇÇ")
                        else:
                            st.write("üå°Ô∏è **Interpretaci√≥n**: Mayor temperatura ‚Üí Menor NO‚ÇÇ")
                    elif var == 'tp':
                        if effects[0] > effects[-1]:
                            st.write("üåßÔ∏è **Interpretaci√≥n**: Lluvia reduce NO‚ÇÇ (lavado atmosf√©rico)")
                        else:
                            st.write("üåßÔ∏è **Interpretaci√≥n**: Relaci√≥n compleja con precipitaci√≥n")
    
    with tab3:
        st.write("### Comparaci√≥n de Importancia de Variables")
        
        # Calcular importancia relativa basada en el rango de efectos
        variable_importance = {}
        
        all_variables = {}
        all_variables.update({k: v for k, v in traffic_variables.items() if k in feature_names})
        all_variables.update({k: v for k, v in meteo_variables.items() if k in feature_names})
        
        for var, config in all_variables.items():
            if var in feature_names:
                min_val, max_val = config['range']
                values = np.linspace(min_val, max_val, 30)
                effects = trainer.get_partial_effects(
                    model, feature_names, values, feature_name=var
                )
                effect_range = np.max(effects) - np.min(effects)
                variable_importance[var] = {
                    'range': effect_range,
                    'title': config['title'],
                    'category': 'Tr√°fico' if var in traffic_variables else 'Meteorolog√≠a'
                }
        
        if variable_importance:
            # Ordenar por importancia
            sorted_vars = sorted(variable_importance.items(), key=lambda x: x[1]['range'], reverse=True)
            
            # Gr√°fico de barras de importancia
            fig, ax = plt.subplots(figsize=(12, 8))
            
            vars_names = [item[1]['title'] for item in sorted_vars]
            ranges = [item[1]['range'] for item in sorted_vars]
            categories = [item[1]['category'] for item in sorted_vars]
            
            colors = ['red' if cat == 'Tr√°fico' else 'blue' for cat in categories]
            
            bars = ax.barh(vars_names, ranges, color=colors, alpha=0.7)
            ax.set_xlabel('Rango de Efecto en NO‚ÇÇ (Œºg/m¬≥)')
            title_prefix = f"{sensor_id} - " if sensor_id else ""
            ax.set_title(f'{title_prefix}Importancia Relativa de Variables (basada en rango de efectos)', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='x')
            
            # A√±adir valores en las barras
            for bar, range_val in zip(bars, ranges):
                width = bar.get_width()
                ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{range_val:.2f}', ha='left', va='center', fontweight='bold')
            
            # Leyenda
            from matplotlib.patches import Patch
            legend_elements = [Patch(facecolor='red', alpha=0.7, label='Variables de Tr√°fico'),
                             Patch(facecolor='blue', alpha=0.7, label='Variables Meteorol√≥gicas')]
            ax.legend(handles=legend_elements, loc='lower right')
            
            st.pyplot(fig)
            plt.close(fig)
            
            # Tabla resumen
            st.write("### üìã Resumen de Importancia")
            import pandas as pd
            summary_df = pd.DataFrame([
                {
                    'Variable': item[1]['title'],
                    'Categor√≠a': item[1]['category'],
                    'Rango de Efecto (Œºg/m¬≥)': f"{item[1]['range']:.2f}",
                    'Importancia': 'üî¥ Alta' if item[1]['range'] > np.mean(ranges) else 'üü° Media' if item[1]['range'] > np.mean(ranges)/2 else 'üü¢ Baja'
                }
                for item in sorted_vars
            ])
            st.dataframe(summary_df, use_container_width=True)
            
            # Insights autom√°ticos
            st.write("### üí° Insights Autom√°ticos")
            
            top_traffic = [item for item in sorted_vars if item[1]['category'] == 'Tr√°fico'][:2]
            top_meteo = [item for item in sorted_vars if item[1]['category'] == 'Meteorolog√≠a'][:2]
            
            if top_traffic:
                st.write(f"üöó **Variables de tr√°fico m√°s influyentes**: {', '.join([item[1]['title'] for item in top_traffic])}")
            
            if top_meteo:
                st.write(f"üå§Ô∏è **Variables meteorol√≥gicas m√°s influyentes**: {', '.join([item[1]['title'] for item in top_meteo])}")
            
            avg_traffic_effect = np.mean([item[1]['range'] for item in sorted_vars if item[1]['category'] == 'Tr√°fico'])
            avg_meteo_effect = np.mean([item[1]['range'] for item in sorted_vars if item[1]['category'] == 'Meteorolog√≠a'])
            
            if avg_traffic_effect > avg_meteo_effect:
                st.write("üìä **Conclusi√≥n**: Las variables de tr√°fico tienen mayor impacto promedio en NO‚ÇÇ")
            else:
                st.write("üìä **Conclusi√≥n**: Las variables meteorol√≥gicas tienen mayor impacto promedio en NO‚ÇÇ")


def show_partial_dependence_plots(model: LinearGAM, feature_names: List[str], sensor_id: str = None):
    """Muestra gr√°ficos de dependencia parcial."""
    st.subheader("üìä Dependencias Parciales Detalladas")
    
    cols = 3
    rows = (len(feature_names) + cols - 1) // cols
    
    for row in range(rows):
        columns = st.columns(cols)
        for col in range(cols):
            idx = row * cols + col
            if idx < len(feature_names):
                feature = feature_names[idx]
                with columns[col]:
                    fig, ax = plt.subplots(figsize=(6, 4))
                    
                    try:
                        XX = model.generate_X_grid(term=idx)
                        pdep, confi = model.partial_dependence(term=idx, X=XX, width=0.95)
                        
                        ax.plot(XX[:, idx], pdep, label='Efecto parcial')
                        ax.fill_between(XX[:, idx], confi[:, 0], confi[:, 1], 
                                       alpha=0.2, label='IC 95%')
                        ax.legend()
                    except:
                        XX = model.generate_X_grid(term=idx)
                        pdep = model.partial_dependence(term=idx, X=XX)
                        ax.plot(XX[:, idx], pdep)
                    
                    title_prefix = f"{sensor_id} - " if sensor_id else ""
                    ax.set_title(f'{title_prefix}{feature}')
                    ax.set_ylabel('Efecto parcial')
                    ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)
                    ax.grid(True, alpha=0.3)
                    
                    st.pyplot(fig)
                    plt.close(fig)


def show_info_panel():
    """Muestra panel de informaci√≥n sobre el entrenamiento GAM."""
    with st.expander("‚ÑπÔ∏è Acerca del Entrenamiento GAM", expanded=False):
        st.markdown("""
        **Modelos GAM (Generalized Additive Models)**
        
        Los GAM permiten modelar relaciones no lineales entre variables predictoras y NO‚ÇÇ.
        
        **Caracter√≠sticas:**
        - **Flexibilidad**: Captura relaciones no lineales autom√°ticamente
        - **Interpretabilidad**: Cada variable contribuye de forma aditiva
        - **Robustez**: Maneja diferentes tipos de variables (continuas, c√≠clicas)
        
        **Proceso de entrenamiento:**
        1. **Preprocesamiento**: Creaci√≥n de variables c√≠clicas y conversi√≥n de unidades
        2. **Divisi√≥n temporal**: Separaci√≥n por fechas (entrenamiento/evaluaci√≥n)
        3. **Filtrado de outliers**: ‚ö†Ô∏è **Solo en datos de entrenamiento** (evita data leakage)
        4. **Escalado**: Normalizaci√≥n de variables para mejor convergencia
        5. **Entrenamiento**: Optimizaci√≥n de splines para cada variable
        
        **Buenas pr√°cticas implementadas:**
        - ‚úÖ **Outliers eliminados solo del training set** - Mantiene integridad de evaluaci√≥n
        - ‚úÖ **Divisi√≥n temporal** - Simula predicci√≥n en tiempo real
        - ‚úÖ **Variables c√≠clicas** - Captura patrones temporales complejos
        - ‚úÖ **Escalado por conjunto** - Evita data leakage entre train/test
        
        **Variables utilizadas:**
        - **Temporales**: Hora, d√≠a, mes, estaciones (como funciones c√≠clicas)
        - **Tr√°fico**: Intensidad, carga, ocupaci√≥n, velocidad
        - **Meteorol√≥gicas**: Temperatura, precipitaci√≥n, presi√≥n, viento
        """)
        
        st.markdown("---")
        st.markdown("""
        **üîç ¬øPor qu√© outliers solo en entrenamiento?**
        
        - **Data leakage**: Usar informaci√≥n del test set contaminar√≠a la evaluaci√≥n
        - **Realismo**: En producci√≥n no conocemos los outliers futuros
        - **Robustez**: El modelo debe manejar datos an√≥malos en predicci√≥n
        - **Evaluaci√≥n honesta**: M√©tricas reflejan rendimiento real
        """)


def show_interaction_analysis(trainer: GAMTrainer, model: LinearGAM, feature_names: List[str], X_test: pd.DataFrame, sensor_id: str = None):
    """Muestra an√°lisis de interacciones entre variables."""
    st.subheader("üîÑ An√°lisis de Interacciones entre Variables")
    
    # Seleccionar pares de variables importantes para an√°lisis de interacci√≥n
    key_interactions = [
        ('intensidad', 't2m', 'Tr√°fico vs Temperatura'),
        ('intensidad', 'tp', 'Tr√°fico vs Precipitaci√≥n'),
        ('t2m', 'sp', 'Temperatura vs Presi√≥n'),
        ('carga', 'u10', 'Carga Tr√°fico vs Viento U'),
        ('ocupacion', 'v10', 'Ocupaci√≥n vs Viento V'),
        ('vmed', 'ssrd', 'Velocidad vs Radiaci√≥n Solar')
    ]
    
    available_interactions = [
        (var1, var2, name) for var1, var2, name in key_interactions 
        if var1 in feature_names and var2 in feature_names
    ]
    
    if not available_interactions:
        st.warning("No hay suficientes variables disponibles para an√°lisis de interacciones.")
        return
    
    # Crear tabs para diferentes tipos de interacciones
    tab1, tab2, tab3 = st.tabs([
        "üå°Ô∏è Tr√°fico-Meteorolog√≠a", 
        "üìä Matriz de Correlaciones GAM", 
        "üéØ Escenarios Combinados"
    ])
    
    with tab1:
        st.write("### Interacciones Tr√°fico-Meteorolog√≠a")
        
        # Mostrar heatmaps de interacciones
        for var1, var2, interaction_name in available_interactions[:4]:  # Mostrar m√°ximo 4
            st.write(f"#### {interaction_name}")
            
            # Crear grilla de valores para ambas variables
            if var1 in VARIABLE_METADATA:
                range1 = VARIABLE_METADATA[var1]['typical_range']
            else:
                range1 = (X_test[var1].min(), X_test[var1].max())
            
            if var2 in VARIABLE_METADATA:
                range2 = VARIABLE_METADATA[var2]['typical_range']
            else:
                range2 = (X_test[var2].min(), X_test[var2].max())
            
            # Crear grilla m√°s peque√±a para eficiencia
            var1_vals = np.linspace(range1[0], range1[1], 20)
            var2_vals = np.linspace(range2[0], range2[1], 20)
            
            # Calcular efectos combinados
            interaction_effects = np.zeros((len(var2_vals), len(var1_vals)))
            
            for i, v2 in enumerate(var2_vals):
                for j, v1 in enumerate(var1_vals):
                    # Crear punto de datos con valores medios para otras variables
                    XX = np.zeros((1, len(feature_names)))
                    
                    # Establecer valores medios para todas las variables
                    for k, feat in enumerate(feature_names):
                        if feat == var1:
                            XX[0, k] = v1
                        elif feat == var2:
                            XX[0, k] = v2
                        elif feat in X_test.columns:
                            XX[0, k] = X_test[feat].mean()
                    
                    try:
                        pred = model.predict(XX)
                        interaction_effects[i, j] = pred[0]
                    except:
                        interaction_effects[i, j] = 0
            
            # Crear heatmap
            fig, ax = plt.subplots(figsize=(10, 8))
            im = ax.imshow(interaction_effects, cmap='RdYlBu_r', aspect='auto', 
                          extent=[range1[0], range1[1], range2[0], range2[1]])
            
            ax.set_xlabel(f"{VARIABLE_METADATA.get(var1, {}).get('name', var1)} ({VARIABLE_METADATA.get(var1, {}).get('unit', '')})")
            ax.set_ylabel(f"{VARIABLE_METADATA.get(var2, {}).get('name', var2)} ({VARIABLE_METADATA.get(var2, {}).get('unit', '')})")
            title_prefix = f"{sensor_id} - " if sensor_id else ""
            ax.set_title(f"{title_prefix}Efecto Combinado: {interaction_name}")
            
            # A√±adir colorbar
            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('Predicci√≥n NO‚ÇÇ (Œºg/m¬≥)')
            
            # A√±adir contornos
            contours = ax.contour(var1_vals, var2_vals, interaction_effects, 
                                colors='black', alpha=0.3, linewidths=0.5)
            ax.clabel(contours, inline=True, fontsize=8)
            
            st.pyplot(fig)
            plt.close(fig)
            
            # Interpretaci√≥n autom√°tica
            max_effect = np.max(interaction_effects)
            min_effect = np.min(interaction_effects)
            effect_range = max_effect - min_effect
            
            st.write(f"üìä **Rango de efectos combinados**: {effect_range:.2f} Œºg/m¬≥")
            
            # Encontrar condiciones de m√°ximo y m√≠nimo
            max_idx = np.unravel_index(np.argmax(interaction_effects), interaction_effects.shape)
            min_idx = np.unravel_index(np.argmin(interaction_effects), interaction_effects.shape)
            
            max_v1 = var1_vals[max_idx[1]]
            max_v2 = var2_vals[max_idx[0]]
            min_v1 = var1_vals[min_idx[1]]
            min_v2 = var2_vals[min_idx[0]]
            
            st.write(f"üî∫ **M√°ximo NO‚ÇÇ** ({max_effect:.2f} Œºg/m¬≥): {var1}={max_v1:.1f}, {var2}={max_v2:.1f}")
            st.write(f"üîª **M√≠nimo NO‚ÇÇ** ({min_effect:.2f} Œºg/m¬≥): {var1}={min_v1:.1f}, {var2}={min_v2:.1f}")
    
    with tab2:
        st.write("### Matriz de Correlaciones de Efectos GAM")
        
        # Variables a analizar (evitar variables c√≠clicas y duplicadas)
        analysis_vars = []
        for var in feature_names:
            if (var in VARIABLE_METADATA and 
                not any(temp in var for temp in ['sin', 'cos']) and
                var not in analysis_vars):
                analysis_vars.append(var)
        
        # Limitar a m√°ximo 8 variables para eficiencia
        analysis_vars = analysis_vars[:8]
        
        if len(analysis_vars) >= 2:
            st.write(f"üîç **Analizando correlaciones entre**: {', '.join([VARIABLE_METADATA.get(var, {}).get('name', var) for var in analysis_vars])}")
            
            correlation_matrix = np.zeros((len(analysis_vars), len(analysis_vars)))
            
            # Calcular efectos parciales para cada variable de manera m√°s robusta
            all_effects = {}
            for var in analysis_vars:
                if var in VARIABLE_METADATA:
                    range_var = VARIABLE_METADATA[var]['typical_range']
                    # Usar m√°s puntos para mejor resoluci√≥n
                    values = np.linspace(range_var[0], range_var[1], 50)
                    
                    # Calcular efectos parciales usando el m√©todo m√°s robusto
                    try:
                        # Crear matriz base con valores medios
                        n_points = len(values)
                        XX_base = np.zeros((n_points, len(feature_names)))
                        
                        # Llenar con valores medios de X_test para todas las variables
                        for feat_idx, feat_name in enumerate(feature_names):
                            if feat_name in X_test.columns:
                                XX_base[:, feat_idx] = X_test[feat_name].mean()
                            elif any(temp in feat_name for temp in ['sin', 'cos']):
                                # Para variables c√≠clicas, usar 0 (valor neutro)
                                XX_base[:, feat_idx] = 0
                        
                        # Variar solo la variable de inter√©s
                        var_idx = feature_names.index(var)
                        XX_base[:, var_idx] = values
                        
                        # Obtener predicciones
                        predictions = model.predict(XX_base)
                        
                        # Calcular efectos parciales como diferencia respecto al valor medio
                        mean_prediction = np.mean(predictions)
                        partial_effects = predictions - mean_prediction
                        
                        all_effects[var] = partial_effects
                        
                    except Exception as e:
                        st.warning(f"Error calculando efectos para {var}: {str(e)}")
                        # Usar efectos neutros en caso de error
                        all_effects[var] = np.zeros(len(values))
            
            # Calcular correlaciones entre efectos parciales
            valid_vars = [var for var in analysis_vars if var in all_effects and len(all_effects[var]) > 0]
            
            if len(valid_vars) >= 2:
                correlation_matrix = np.zeros((len(valid_vars), len(valid_vars)))
                
                for i, var1 in enumerate(valid_vars):
                    for j, var2 in enumerate(valid_vars):
                        if i == j:
                            correlation_matrix[i, j] = 1.0
                        else:
                            try:
                                # Verificar que los efectos no sean constantes
                                effects1 = all_effects[var1]
                                effects2 = all_effects[var2]
                                
                                if np.std(effects1) > 1e-10 and np.std(effects2) > 1e-10:
                                    corr = np.corrcoef(effects1, effects2)[0, 1]
                                    correlation_matrix[i, j] = corr if not np.isnan(corr) else 0
                                else:
                                    correlation_matrix[i, j] = 0
                            except:
                                correlation_matrix[i, j] = 0
                
                # Crear heatmap de correlaciones
                fig, ax = plt.subplots(figsize=(10, 8))
                im = ax.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
                
                # Configurar etiquetas
                var_labels = [VARIABLE_METADATA.get(var, {}).get('name', var) for var in valid_vars]
                ax.set_xticks(range(len(valid_vars)))
                ax.set_yticks(range(len(valid_vars)))
                ax.set_xticklabels(var_labels, rotation=45, ha='right')
                ax.set_yticklabels(var_labels)
                
                # A√±adir valores en las celdas
                for i in range(len(valid_vars)):
                    for j in range(len(valid_vars)):
                        color = "white" if abs(correlation_matrix[i, j]) > 0.5 else "black"
                        text = ax.text(j, i, f'{correlation_matrix[i, j]:.2f}',
                                     ha="center", va="center", color=color, fontweight='bold')
                
                ax.set_title(f"{title_prefix}Correlaciones entre Efectos Parciales GAM")
                plt.colorbar(im, ax=ax, label='Correlaci√≥n')
                
                st.pyplot(fig)
                plt.close(fig)
                
                # Interpretaci√≥n de correlaciones con filtros m√°s estrictos
                st.write("### üîç Correlaciones Significativas")
                
                # Mostrar estad√≠sticas de los efectos
                st.write("#### üìä Estad√≠sticas de Efectos Parciales")
                effects_stats = []
                for var in valid_vars:
                    effects = all_effects[var]
                    stats = {
                        'Variable': VARIABLE_METADATA.get(var, {}).get('name', var),
                        'Rango': f"{np.max(effects) - np.min(effects):.3f}",
                        'Std Dev': f"{np.std(effects):.3f}",
                        'Varianza': f"{np.var(effects):.3f}"
                    }
                    effects_stats.append(stats)
                
                import pandas as pd
                stats_df = pd.DataFrame(effects_stats)
                st.dataframe(stats_df, use_container_width=True)
                
                # Filtrar correlaciones significativas (evitar perfectas y diagonales)
                significant_corr_pairs = []
                for i in range(len(valid_vars)):
                    for j in range(i+1, len(valid_vars)):
                        corr_val = correlation_matrix[i, j]
                        # Filtros m√°s estrictos: evitar correlaciones perfectas sospechosas
                        if (0.3 <= abs(corr_val) <= 0.95 and  # Rango razonable
                            np.std(all_effects[valid_vars[i]]) > 1e-6 and  # Efectos no constantes
                            np.std(all_effects[valid_vars[j]]) > 1e-6):
                            significant_corr_pairs.append((valid_vars[i], valid_vars[j], corr_val))
                
                if significant_corr_pairs:
                    st.write("**Correlaciones f√≠sicamente plausibles:**")
                    for var1, var2, corr in sorted(significant_corr_pairs, key=lambda x: abs(x[2]), reverse=True):
                        var1_name = VARIABLE_METADATA.get(var1, {}).get('name', var1)
                        var2_name = VARIABLE_METADATA.get(var2, {}).get('name', var2)
                        
                        # Interpretaci√≥n contextual
                        interpretation = ""
                        if var1 in ['t2m', 'd2m'] and var2 in ['t2m', 'd2m']:
                            interpretation = " (relaci√≥n meteorol√≥gica esperada)"
                        elif var1 in ['intensidad', 'carga', 'ocupacion', 'vmed'] and var2 in ['intensidad', 'carga', 'ocupacion', 'vmed']:
                            interpretation = " (variables de tr√°fico relacionadas)"
                        elif 'ssr' in [var1, var2] and 'ssrd' in [var1, var2]:
                            interpretation = " (radiaciones solares relacionadas)"
                        
                        if corr > 0:
                            st.write(f"üìà **{var1_name}** ‚Üî **{var2_name}**: efectos similares (r={corr:.2f}){interpretation}")
                        else:
                            st.write(f"üìâ **{var1_name}** ‚Üî **{var2_name}**: efectos opuestos (r={corr:.2f}){interpretation}")
                else:
                    st.write("‚úÖ **No se encontraron correlaciones artificiales**. Los efectos parciales son independientes.")
                
                # Advertencias sobre correlaciones sospechosas
                perfect_corr_pairs = []
                for i in range(len(valid_vars)):
                    for j in range(i+1, len(valid_vars)):
                        corr_val = correlation_matrix[i, j]
                        if abs(corr_val) > 0.98:  # Correlaciones casi perfectas
                            perfect_corr_pairs.append((valid_vars[i], valid_vars[j], corr_val))
                
                if perfect_corr_pairs:
                    st.warning("‚ö†Ô∏è **Correlaciones sospechosamente altas detectadas:**")
                    for var1, var2, corr in perfect_corr_pairs:
                        var1_name = VARIABLE_METADATA.get(var1, {}).get('name', var1)
                        var2_name = VARIABLE_METADATA.get(var2, {}).get('name', var2)
                        st.write(f"- {var1_name} vs {var2_name}: r={corr:.3f}")
                    st.write("Esto puede indicar:")
                    st.write("- Multicolinealidad en los datos")
                    st.write("- Efectos parciales muy similares")
                    st.write("- Posible sobreajuste del modelo")
                    
                    # Bot√≥n para diagn√≥stico detallado
                    #if st.button("üî¨ Ejecutar Diagn√≥stico Detallado"):
                    diagnose_perfect_correlations(model, feature_names, X_test)
            
            else:
                st.warning("No hay suficientes variables v√°lidas para an√°lisis de correlaciones.")
        else:
            st.warning("No hay suficientes variables disponibles para an√°lisis de correlaciones.")
    
    with tab3:
        st.write("### Escenarios de Condiciones Combinadas")
        
        # Definir escenarios t√≠picos
        scenarios = {
            "üåÖ Ma√±ana Laborable": {
                "hour_sin": np.sin(2 * np.pi * 8 / 24),  # 8 AM
                "hour_cos": np.cos(2 * np.pi * 8 / 24),
                "weekend": 0,
                "intensidad": 800,  # Tr√°fico alto
                "t2m": 15,  # Temperatura moderada
                "tp": 0,  # Sin lluvia
            },
            "üåÜ Tarde Laborable": {
                "hour_sin": np.sin(2 * np.pi * 18 / 24),  # 6 PM
                "hour_cos": np.cos(2 * np.pi * 18 / 24),
                "weekend": 0,
                "intensidad": 1000,  # Tr√°fico muy alto
                "t2m": 20,  # Temperatura c√°lida
                "tp": 0,  # Sin lluvia
            },
            "üåô Noche Fin de Semana": {
                "hour_sin": np.sin(2 * np.pi * 22 / 24),  # 10 PM
                "hour_cos": np.cos(2 * np.pi * 22 / 24),
                "weekend": 1,
                "intensidad": 200,  # Tr√°fico bajo
                "t2m": 12,  # Temperatura fresca
                "tp": 0,  # Sin lluvia
            },
            "üåßÔ∏è D√≠a Lluvioso": {
                "hour_sin": np.sin(2 * np.pi * 14 / 24),  # 2 PM
                "hour_cos": np.cos(2 * np.pi * 14 / 24),
                "weekend": 0,
                "intensidad": 400,  # Tr√°fico reducido por lluvia
                "t2m": 10,  # Temperatura baja
                "tp": 5,  # Lluvia moderada
            },
            "üî• D√≠a Caluroso Verano": {
                "hour_sin": np.sin(2 * np.pi * 15 / 24),  # 3 PM
                "hour_cos": np.cos(2 * np.pi * 15 / 24),
                "weekend": 0,
                "intensidad": 600,  # Tr√°fico moderado
                "t2m": 35,  # Temperatura muy alta
                "tp": 0,  # Sin lluvia
                "ssrd": 800,  # Radiaci√≥n solar alta
            }
        }
        
        # Calcular predicciones para cada escenario
        scenario_results = {}
        
        for scenario_name, scenario_values in scenarios.items():
            # Crear vector de caracter√≠sticas
            XX = np.zeros((1, len(feature_names)))
            
            # Establecer valores del escenario
            for feat_idx, feat_name in enumerate(feature_names):
                if feat_name in scenario_values:
                    XX[0, feat_idx] = scenario_values[feat_name]
                elif feat_name in X_test.columns:
                    XX[0, feat_idx] = X_test[feat_name].mean()  # Valor medio para otras variables
            
            try:
                prediction = model.predict(XX)[0]
                scenario_results[scenario_name] = prediction
            except:
                scenario_results[scenario_name] = None
        
        # Mostrar resultados
        if scenario_results:
            # Gr√°fico de barras de escenarios
            valid_scenarios = {k: v for k, v in scenario_results.items() if v is not None}
            
            if valid_scenarios:
                fig, ax = plt.subplots(figsize=(12, 6))
                
                scenarios_names = list(valid_scenarios.keys())
                predictions = list(valid_scenarios.values())
                
                colors = ['skyblue', 'orange', 'purple', 'lightblue', 'red'][:len(scenarios_names)]
                bars = ax.bar(scenarios_names, predictions, color=colors, alpha=0.7)
                
                ax.set_ylabel('Predicci√≥n NO‚ÇÇ (Œºg/m¬≥)')
                ax.set_title(f'{title_prefix}Predicciones de NO‚ÇÇ para Diferentes Escenarios')
                ax.grid(True, alpha=0.3, axis='y')
                
                # A√±adir l√≠nea de referencia WHO
                ax.axhline(y=40, color='red', linestyle='--', alpha=0.7, label='L√≠mite WHO (40 Œºg/m¬≥)')
                ax.legend()
                
                # A√±adir valores en las barras
                for bar, pred in zip(bars, predictions):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                           f'{pred:.1f}', ha='center', va='bottom', fontweight='bold')
                
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
                plt.close(fig)
                
                # An√°lisis de escenarios
                st.write("### üìã An√°lisis de Escenarios")
                
                max_scenario = max(valid_scenarios.items(), key=lambda x: x[1])
                min_scenario = min(valid_scenarios.items(), key=lambda x: x[1])
                
                st.write(f"üî∫ **Peor escenario**: {max_scenario[0]} ‚Üí {max_scenario[1]:.1f} Œºg/m¬≥")
                st.write(f"üîª **Mejor escenario**: {min_scenario[0]} ‚Üí {min_scenario[1]:.1f} Œºg/m¬≥")
                
                # Alertas por l√≠mites WHO
                dangerous_scenarios = [name for name, pred in valid_scenarios.items() if pred > 40]
                if dangerous_scenarios:
                    st.warning(f"‚ö†Ô∏è **Escenarios que superan l√≠mite WHO**: {', '.join(dangerous_scenarios)}")
                else:
                    st.success("‚úÖ Todos los escenarios est√°n por debajo del l√≠mite WHO")
                
                # Recomendaciones
                st.write("### üí° Recomendaciones Basadas en Escenarios")
                
                if max_scenario[1] > 40:
                    st.write("üö® **Alerta**: Condiciones de alto riesgo identificadas")
                    st.write("- Considerar restricciones de tr√°fico en horas pico")
                    st.write("- Monitoreo intensivo en d√≠as calurosos")
                
                if 'D√≠a Lluvioso' in valid_scenarios and valid_scenarios['üåßÔ∏è D√≠a Lluvioso'] < np.mean(list(valid_scenarios.values())):
                    st.write("üåßÔ∏è **Beneficio de la lluvia**: La precipitaci√≥n reduce significativamente el NO‚ÇÇ")
                
                range_scenarios = max(valid_scenarios.values()) - min(valid_scenarios.values())
                st.write(f"üìä **Variabilidad por condiciones**: {range_scenarios:.1f} Œºg/m¬≥ de diferencia entre escenarios")


def diagnose_perfect_correlations(model: LinearGAM, feature_names: List[str], X_test: pd.DataFrame):
    """Diagnostica las causas de correlaciones perfectas entre efectos parciales."""
    st.write("### üî¨ Diagn√≥stico de Correlaciones Perfectas")
    
    # Variables a diagnosticar
    analysis_vars = [var for var in feature_names 
                    if var in VARIABLE_METADATA and not any(temp in var for temp in ['sin', 'cos'])][:6]
    
    if len(analysis_vars) < 2:
        st.warning("No hay suficientes variables para diagn√≥stico.")
        return
    
    st.write("#### 1. An√°lisis de Multicolinealidad en Datos Originales")
    
    # Calcular correlaciones en datos originales
    original_corr_matrix = X_test[analysis_vars].corr()
    
    # Mostrar correlaciones altas en datos originales
    high_original_corr = []
    for i, var1 in enumerate(analysis_vars):
        for j, var2 in enumerate(analysis_vars):
            if i < j and abs(original_corr_matrix.iloc[i, j]) > 0.8:
                high_original_corr.append((var1, var2, original_corr_matrix.iloc[i, j]))
    
    if high_original_corr:
        st.warning("‚ö†Ô∏è **Multicolinealidad detectada en datos originales:**")
        for var1, var2, corr in high_original_corr:
            var1_name = VARIABLE_METADATA.get(var1, {}).get('name', var1)
            var2_name = VARIABLE_METADATA.get(var2, {}).get('name', var2)
            st.write(f"- {var1_name} ‚Üî {var2_name}: r={corr:.3f}")
        st.write("**Recomendaci√≥n**: Considerar eliminar una de las variables correlacionadas.")
    else:
        st.success("‚úÖ No hay multicolinealidad significativa en datos originales.")
    
    st.write("#### 2. An√°lisis de Efectos Parciales Individuales")
    
    # Analizar efectos parciales para cada variable
    effects_analysis = {}
    
    for var in analysis_vars[:4]:  # Limitar para eficiencia
        if var in VARIABLE_METADATA:
            range_var = VARIABLE_METADATA[var]['typical_range']
            values = np.linspace(range_var[0], range_var[1], 20)
            
            # Calcular efectos usando diferentes m√©todos
            try:
                # M√©todo 1: Efectos parciales directos del modelo
                XX = np.zeros((len(values), len(feature_names)))
                for feat_idx, feat_name in enumerate(feature_names):
                    if feat_name == var:
                        XX[:, feat_idx] = values
                    elif feat_name in X_test.columns:
                        XX[:, feat_idx] = X_test[feat_name].mean()
                
                predictions = model.predict(XX)
                
                # M√©todo 2: Usar partial_dependence del modelo (si est√° disponible)
                try:
                    var_idx = feature_names.index(var)
                    pd_effects = []
                    for val in values:
                        XX_single = np.zeros((1, len(feature_names)))
                        XX_single[0, var_idx] = val
                        for feat_idx, feat_name in enumerate(feature_names):
                            if feat_idx != var_idx and feat_name in X_test.columns:
                                XX_single[0, feat_idx] = X_test[feat_name].mean()
                        
                        pd_effect = model.partial_dependence(term=var_idx, X=XX_single)
                        pd_effects.append(pd_effect)
                    
                    effects_analysis[var] = {
                        'predictions': predictions,
                        'partial_dependence': np.array(pd_effects),
                        'values': values,
                        'range': np.max(predictions) - np.min(predictions),
                        'std': np.std(predictions),
                        'is_linear': np.corrcoef(values, predictions)[0, 1] > 0.95
                    }
                    
                except Exception as e:
                    effects_analysis[var] = {
                        'predictions': predictions,
                        'partial_dependence': None,
                        'values': values,
                        'range': np.max(predictions) - np.min(predictions),
                        'std': np.std(predictions),
                        'is_linear': np.corrcoef(values, predictions)[0, 1] > 0.95,
                        'error': str(e)
                    }
                    
            except Exception as e:
                st.error(f"Error analizando {var}: {str(e)}")
    
    # Mostrar resultados del an√°lisis
    if effects_analysis:
        st.write("**Caracter√≠sticas de los efectos parciales:**")
        
        analysis_df_data = []
        for var, analysis in effects_analysis.items():
            var_name = VARIABLE_METADATA.get(var, {}).get('name', var)
            analysis_df_data.append({
                'Variable': var_name,
                'Rango de Efecto': f"{analysis['range']:.4f}",
                'Desv. Est√°ndar': f"{analysis['std']:.4f}",
                'Es Lineal': "S√≠" if analysis['is_linear'] else "No",
                'Tiene Error': "S√≠" if 'error' in analysis else "No"
            })
        
        import pandas as pd
        analysis_df = pd.DataFrame(analysis_df_data)
        st.dataframe(analysis_df, use_container_width=True)
        
        # Identificar variables problem√°ticas
        problematic_vars = []
        for var, analysis in effects_analysis.items():
            if analysis['std'] < 1e-8:
                problematic_vars.append(f"{var} (efecto constante)")
            elif analysis['is_linear']:
                problematic_vars.append(f"{var} (efecto perfectamente lineal)")
        
        if problematic_vars:
            st.warning("‚ö†Ô∏è **Variables con efectos problem√°ticos:**")
            for prob_var in problematic_vars:
                st.write(f"- {prob_var}")
            st.write("**Posibles causas:**")
            st.write("- Modelo demasiado simple para la variable")
            st.write("- Datos insuficientes en el rango de la variable")
            st.write("- Variable no tiene efecto real en NO‚ÇÇ")
    
    st.write("#### 3. Recomendaciones")
    
    recommendations = []
    
    if high_original_corr:
        recommendations.append("üîÑ **Reducir multicolinealidad**: Eliminar variables altamente correlacionadas")
    
    if any('error' in analysis for analysis in effects_analysis.values()):
        recommendations.append("‚öôÔ∏è **Revisar configuraci√≥n GAM**: Algunos t√©rminos pueden estar mal especificados")
    
    linear_vars = [var for var, analysis in effects_analysis.items() if analysis.get('is_linear', False)]
    if linear_vars:
        recommendations.append(f"üìà **Variables lineales detectadas**: {', '.join(linear_vars)} - Considerar t√©rminos lineales simples")
    
    constant_vars = [var for var, analysis in effects_analysis.items() if analysis.get('std', 1) < 1e-8]
    if constant_vars:
        recommendations.append(f"üîí **Variables sin efecto**: {', '.join(constant_vars)} - Considerar eliminar del modelo")
    
    if not recommendations:
        recommendations.append("‚úÖ **No se detectaron problemas graves** en el an√°lisis de efectos parciales")
    
    for rec in recommendations:
        st.write(rec)


# ==================== FUNCI√ìN PRINCIPAL ====================

def training_page():
    """Funci√≥n principal del m√≥dulo de entrenamiento."""
    
    # Inicializar trainer
    trainer = GAMTrainer()
    
    # Panel de informaci√≥n
    show_info_panel()
    
    # Cargar datos
    if not st.session_state.training_data_loaded:
        if st.button("Cargar datos para entrenamiento", type="primary"):
            with st.spinner("Cargando datos de entrenamiento..."):
                trainer.df_master = trainer.load_data()
                if not trainer.df_master.empty:
                    st.session_state.training_data_loaded = True
                    st.success("Datos cargados correctamente!")
                    st.rerun()
        return
    
    # Recuperar datos
    trainer.df_master = trainer.load_data()
    
    if trainer.df_master.empty:
        st.error("No se pudieron cargar los datos.")
        return
    
    # Configuraci√≥n del modelo
    st.header("‚öôÔ∏è Configuraci√≥n del Modelo GAM")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Selecci√≥n de sensor
        sensores = sorted(trainer.df_master['id_no2'].unique())
        sensor_seleccionado = st.selectbox(
            "Sensor de NO‚ÇÇ", 
            sensores, 
            index=2 if len(sensores) > 2 else 0
        )
        
        # Filtrar por sensor
        df_sensor = trainer.df_master[trainer.df_master['id_no2'] == sensor_seleccionado]
        
        # Fechas disponibles
        fecha_min = df_sensor["fecha"].min().date()
        fecha_max = df_sensor["fecha"].max().date()
        
        # Fecha de divisi√≥n
        fecha_division = st.date_input(
            "Fecha de divisi√≥n (entrenamiento/prueba)",
            value=pd.to_datetime('2024-01-01').date(),
            min_value=fecha_min,
            max_value=fecha_max,
            help="Los datos anteriores se usan para entrenamiento, posteriores para evaluaci√≥n"
        )
    
    with col2:
        # M√©todo de filtrado de outliers
        outlier_method = st.selectbox(
            "M√©todo de filtrado de outliers",
            options=list(OUTLIER_METHODS.keys()),
            format_func=lambda x: OUTLIER_METHODS[x],
            index=1  # zscore por defecto
        )
        
        # Preprocesamiento
        preprocessing = st.selectbox(
            "Preprocesamiento temporal",
            options=list(PREPROCESSING_OPTIONS.keys()),
            format_func=lambda x: PREPROCESSING_OPTIONS[x],
            index=0  # sin_cos por defecto
        )
    
    # Selecci√≥n de variables
    st.subheader("üîß Selecci√≥n de Variables")
    
    # Crear tabs para categor√≠as
    var_tabs = st.tabs(list(VARIABLE_CATEGORIES.keys()))
    
    selected_features = []
    for i, (category, vars_list) in enumerate(VARIABLE_CATEGORIES.items()):
        with var_tabs[i]:
            # Filtrar variables que existen en los datos
            available_vars = [var for var in vars_list if var in trainer.df_master.columns or 'sin' in var or 'cos' in var]
            
            selected_in_category = st.multiselect(
                f"Variables de {category}",
                available_vars,
                default=available_vars,
                help=f"Selecciona las variables de {category.lower()} para el modelo"
            )
            selected_features.extend(selected_in_category)
    
    if not selected_features:
        st.warning("Selecciona al menos una variable para continuar.")
        return
    
    # Mostrar resumen de configuraci√≥n
    with st.expander("üìã Resumen de Configuraci√≥n"):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(f"**Sensor:** {sensor_seleccionado}")
            st.write(f"**Variables:** {len(selected_features)}")
        with col2:
            st.write(f"**Outliers:** {OUTLIER_METHODS[outlier_method]}")
            st.write(f"**Preprocesamiento:** {PREPROCESSING_OPTIONS[preprocessing]}")
        with col3:
            st.write(f"**Fecha divisi√≥n:** {fecha_division}")
            st.write(f"**Per√≠odo entrenamiento:** {fecha_min} - {fecha_division}")

    
    # Preparar datos
    with st.spinner("Preparando datos..."):
        # Aplicar transformaciones b√°sicas (no outliers a√∫n)
        df_processed = df_sensor.copy()

        st.write("üìä Datos originales:", len(df_sensor))
        
        # Crear variables c√≠clicas si se requiere
        if preprocessing == 'sin_cos':
            df_processed = trainer.create_cyclical_features(df_processed)
                
        # Dividir datos ANTES de eliminar outliers
        fecha_division_dt = pd.to_datetime(fecha_division)
        train_df, test_df = trainer.split_data(df_processed, fecha_division_dt)
        
        st.write("üìÖ Datos entrenamiento (antes outliers):", len(train_df))
        st.write("üìÖ Datos evaluaci√≥n:", len(test_df))
        
        # Eliminar outliers SOLO del conjunto de entrenamiento
        if outlier_method != 'none':
            train_df = trainer.remove_outliers(train_df, outlier_method)
            st.write("üîç Datos entrenamiento (despu√©s outliers):", len(train_df))
            outliers_removed = len(df_processed[df_processed['fecha'] < fecha_division_dt]) - len(train_df)
            st.write(f"‚ùå Outliers eliminados: {outliers_removed}")
        else:
            outliers_removed = 0
        
        if train_df.empty or test_df.empty:
            st.error("No hay suficientes datos para entrenamiento o evaluaci√≥n.")
            return
    
    # Verificar si existe modelo entrenado
    model_filename = f'data/models/gam_model_{sensor_seleccionado}_{outlier_method}_{preprocessing}.pkl'
    model_exists = os.path.exists(model_filename)
    
    # Mostrar informaci√≥n de datos
    st.subheader("üìä Informaci√≥n del Conjunto de Datos")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Muestras entrenamiento", len(train_df))
    with col2:
        st.metric("Muestras evaluaci√≥n", len(test_df))
    with col3:
        st.metric("Variables seleccionadas", len(selected_features))
    with col4:
        st.metric("Outliers eliminados", outliers_removed)
    
    # Botones de acci√≥n
    col1, col2 = st.columns(2)
    
    with col1:
        if model_exists:
            analyze_button = st.button("üîç Analizar Modelo Existente", type="primary")
        else:
            analyze_button = False
            st.info("No existe un modelo entrenado con esta configuraci√≥n")
    
    with col2:
        train_button = st.button("üöÄ Entrenar Nuevo Modelo", type="secondary")
    
    # Ejecutar an√°lisis o entrenamiento
    if analyze_button and model_exists:
        with st.spinner("Cargando y analizando modelo..."):
            model_info = trainer.load_model(model_filename)
            
            if model_info:
                model = model_info['model']
                feature_names = model_info['feature_names']
                scaler_dict = model_info['scaler_dict']
                scaler_target = model_info['scaler_target']
                
                # Preparar datos de prueba
                X_test = test_df[selected_features].copy()
                y_test = test_df['no2_value'].copy()
                
                # Escalar datos de prueba
                for feature in selected_features:
                    if feature in scaler_dict:
                        X_test[feature] = scaler_dict[feature].transform(X_test[[feature]])
                
                # Evaluar modelo
                metrics = trainer.evaluate_model(model, X_test, y_test, scaler_target)
                
                # Mostrar resultados
                show_model_metrics(metrics)
                show_residual_analysis(metrics['residuals'], sensor_id=sensor_seleccionado)
                
                # Tabs para an√°lisis completo del modelo reci√©n entrenado
                tab1, tab2, tab3, tab4 = st.tabs([
                    "üïê Efectos Temporales",
                    "üî¨ Efectos de Variables", 
                    "üìä Dependencias Parciales",
                    "üîÑ An√°lisis de Interacciones"
                ])
                
                with tab1:
                    show_temporal_effects(trainer, model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab2:
                    show_variable_effects(trainer, model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab3:
                    show_partial_dependence_plots(model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab4:
                    show_interaction_analysis(trainer, model, selected_features, X_test, sensor_id=sensor_seleccionado)
    
    elif train_button:
        with st.spinner("Entrenando modelo GAM... Esto puede tardar varios minutos."):
            try:
                # Preparar datos de entrenamiento
                X_train = train_df[selected_features].copy()
                y_train = train_df['no2_value'].copy()
                X_test = test_df[selected_features].copy()
                y_test = test_df['no2_value'].copy()
                
                # Verificar que las features existen en los datos
                missing_features = [f for f in selected_features if f not in train_df.columns]
                if missing_features:
                    st.error(f"‚ùå Features faltantes en los datos: {missing_features}")
                    st.write("Columnas disponibles en train_df:")
                    st.write(list(train_df.columns))
                    return
                
                # Validar configuraci√≥n antes del entrenamiento
                if not trainer.validate_gam_config(selected_features, X_train):
                    st.error("‚ùå Configuraci√≥n inv√°lida. No se puede entrenar el modelo.")
                    return
                
                # Escalar caracter√≠sticas
                X_train_scaled, X_test_scaled, scaler_dict = trainer.scale_features(
                    X_train, X_test, selected_features
                )
                
                # Escalar objetivo
                y_train_scaled, scaler_target = trainer.scale_target(y_train)
                
                # Entrenar modelo
                model = trainer.train_gam_model(X_train_scaled, y_train_scaled, selected_features)
                
                r = model.summary()
                print(r)
                
                # Guardar modelo
                saved_path = trainer.save_model(
                    model, selected_features, scaler_dict, scaler_target,
                    sensor_seleccionado, outlier_method, preprocessing
                )
                
                st.success(f"¬°Modelo entrenado y guardado exitosamente en {saved_path}!")
                
                # Evaluar modelo reci√©n entrenado
                metrics = trainer.evaluate_model(model, X_test_scaled, y_test, scaler_target)
                
                # Mostrar resultados
                show_model_metrics(metrics)
                show_residual_analysis(metrics['residuals'], sensor_id=sensor_seleccionado)
                
                # Tabs para an√°lisis completo del modelo reci√©n entrenado
                tab1, tab2, tab3, tab4 = st.tabs([
                    "üïê Efectos Temporales",
                    "üî¨ Efectos de Variables", 
                    "üìä Dependencias Parciales",
                    "üîÑ An√°lisis de Interacciones"
                ])
                
                with tab1:
                    show_temporal_effects(trainer, model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab2:
                    show_variable_effects(trainer, model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab3:
                    show_partial_dependence_plots(model, selected_features, sensor_id=sensor_seleccionado)
                
                with tab4:
                    show_interaction_analysis(trainer, model, selected_features, X_test, sensor_id=sensor_seleccionado)
            except Exception as e:
                st.error(f"Error durante el entrenamiento: {str(e)}")
                st.exception(e)
    
    # Pie de p√°gina
    st.markdown("---")
    st.markdown(f"""
    <div style="text-align: center; color: #666; font-size: 0.8rem;">
        Entrenamiento GAM para sensor {sensor_seleccionado} | Configuraci√≥n: {OUTLIER_METHODS[outlier_method]} + {PREPROCESSING_OPTIONS[preprocessing]}
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    training_page() 