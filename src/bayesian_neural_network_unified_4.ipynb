{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EJEMPLO: ENTRENAMIENTO BNN GLOBAL MULTI-SENSOR\n",
      "============================================================\n",
      "Entrenamiento BNN Global Multi-Sensor\n",
      "Using device: cpu\n",
      "\n",
      "Cargando datos\n",
      "\n",
      "Configuración Global\n",
      "Sensores entrenamiento: ['28079004', '28079008', '28079011', '28079016', '28079036', '28079038', '28079039', '28079040', '28079047', '28079048']\n",
      "Sensores test: ['28079050', '28079056', '28079035']\n",
      "Fecha división: 2024-01-01\n",
      "Outliers: none\n",
      "\n",
      "Se usan 15 características\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BNNUnifiedTrainer' object has no attribute 'prepare_global_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 758\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m main_global(config)\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 758\u001b[0m     \u001b[43mejemplo_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 754\u001b[0m, in \u001b[0;36mejemplo_global\u001b[0;34m()\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    721\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msensors_train\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079004\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079008\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079011\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079016\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079036\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079038\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079039\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079040\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079047\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079048\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msensors_test\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079050\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079056\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28079035\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    752\u001b[0m }\n\u001b[0;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmain_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 649\u001b[0m, in \u001b[0;36mmain_global\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    646\u001b[0m     selected_features \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_features]\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSe usan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m características\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 649\u001b[0m data_prep \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_global_data\u001b[49m(config, selected_features)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_prep:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BNNUnifiedTrainer' object has no attribute 'prepare_global_data'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Módulo unificado para entrenamiento de Redes Neuronales Bayesianas (BNN) con MFVI,\n",
    "con diagnóstico visual del entrenamiento y validación temporal.\n",
    "\n",
    "Novedades de visualización y diagnóstico:\n",
    "1) Split temporal de validación dentro de main_* para monitorizar RMSE y NLL en validación.\n",
    "2) Registro por época de NLL, KL por muestra, ELBO, tasa de aprendizaje, beta efectiva y sigma_y.\n",
    "3) Gráfica integral con seis paneles: NLL train y val, KL, ELBO, RMSE val, LR y beta, sigma_y.\n",
    "4) Gráficas de dispersión y serie temporal en validación al final del entrenamiento.\n",
    "\n",
    "Nota: No se usa el ID de la estación como variable predictora.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "from scipy.stats import zscore\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.classes.__path__ = []  # estabilidad para entornos sin extensiones\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "# ==================== COMPONENTES BNN (MFVI) ====================\n",
    "\n",
    "class MFVILinear(nn.Module):\n",
    "    \"\"\"Capa lineal Bayesiana usando Inferencia Variacional de Campo Medio (MFVI).\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, prior_weight_std=1.0, prior_bias_std=1.0, init_std=0.05, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.weight_mean = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self.bias_mean = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        self._weight_std_param = nn.Parameter(torch.empty((dim_out, dim_in), **factory_kwargs))\n",
    "        self._bias_std_param = nn.Parameter(torch.empty(dim_out, **factory_kwargs))\n",
    "        \n",
    "        self.reset_parameters(init_std)\n",
    "\n",
    "        prior_mean = 0.0\n",
    "        self.register_buffer('prior_weight_mean', torch.full_like(self.weight_mean, prior_mean))\n",
    "        self.register_buffer('prior_weight_std', torch.full_like(self._weight_std_param, prior_weight_std))\n",
    "        self.register_buffer('prior_bias_mean', torch.full_like(self.bias_mean, prior_mean))\n",
    "        self.register_buffer('prior_bias_std', torch.full_like(self._bias_std_param, prior_bias_std))\n",
    "\n",
    "    def reset_parameters(self, init_std=0.05):\n",
    "        nn.init.kaiming_uniform_(self.weight_mean, a=math.sqrt(5))\n",
    "        bound = self.dim_in ** -0.5 if self.dim_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias_mean, -bound, bound)\n",
    "        _init_std_param = np.log(init_std)\n",
    "        self._weight_std_param.data = torch.full_like(self.weight_mean, _init_std_param)\n",
    "        self._bias_std_param.data = torch.full_like(self.bias_mean, _init_std_param)\n",
    "\n",
    "    @property\n",
    "    def weight_std(self):\n",
    "        return torch.clamp(torch.exp(self._weight_std_param), min=EPS)\n",
    "\n",
    "    @property\n",
    "    def bias_std(self):\n",
    "        return torch.clamp(torch.exp(self._bias_std_param), min=EPS)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        q_weight = dist.Normal(self.weight_mean, self.weight_std)\n",
    "        p_weight = dist.Normal(self.prior_weight_mean, self.prior_weight_std)\n",
    "        kl = dist.kl_divergence(q_weight, p_weight).sum()\n",
    "        \n",
    "        q_bias = dist.Normal(self.bias_mean, self.bias_std)\n",
    "        p_bias = dist.Normal(self.prior_bias_mean, self.prior_bias_std)\n",
    "        kl += dist.kl_divergence(q_bias, p_bias).sum()\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = self._normal_sample(self.weight_mean, self.weight_std)\n",
    "        bias = self._normal_sample(self.bias_mean, self.bias_std)\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _normal_sample(self, mean, std):\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return mean + std * epsilon\n",
    "\n",
    "\n",
    "def make_mfvi_bnn(layer_sizes, activation='GELU', **layer_kwargs):\n",
    "    nonlinearity = getattr(nn, activation)() if isinstance(activation, str) else activation\n",
    "    net = nn.Sequential()\n",
    "    for i, (dim_in, dim_out) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        net.add_module(f'MFVILinear{i}', MFVILinear(dim_in, dim_out, **layer_kwargs))\n",
    "        if i < len(layer_sizes) - 2:\n",
    "            net.add_module(f'Nonlinarity{i}', nonlinearity)\n",
    "    return net\n",
    "\n",
    "\n",
    "def kl_divergence_model(bnn):\n",
    "    kl = 0.0\n",
    "    for module in bnn.modules():\n",
    "        if hasattr(module, 'kl_divergence'):\n",
    "            kl += module.kl_divergence()\n",
    "    return kl\n",
    "\n",
    "\n",
    "def gauss_loglik(y, y_pred, log_noise_var):\n",
    "    l2_dist = (y - y_pred).pow(2).sum(-1)\n",
    "    return -0.5 * (log_noise_var + math.log(2 * math.pi) + l2_dist * torch.exp(-log_noise_var))\n",
    "\n",
    "\n",
    "def test_nll(y, y_pred, log_noise_var):\n",
    "    nll_samples = -gauss_loglik(y, y_pred, log_noise_var)  # (K, N)\n",
    "    nll = -torch.logsumexp(-nll_samples, dim=0) + math.log(nll_samples.shape[0])\n",
    "    return nll.mean()\n",
    "\n",
    "# ==================== DATASET PYTORCH ====================\n",
    "\n",
    "class BayesianDataset(Dataset):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.target[idx]\n",
    "\n",
    "# ==================== ENTRENADOR UNIFICADO ====================\n",
    "\n",
    "class BNNUnifiedTrainer:\n",
    "    \"\"\"\n",
    "    Clase unificada para entrenamiento de BNNs que maneja tanto modelos individuales \n",
    "    como globales con una interfaz consistente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_master = None\n",
    "        self.model = None\n",
    "        self.scaler_dict = {}\n",
    "        self.scaler_target = None\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        if 'cuda' in self.device:\n",
    "            torch.cuda.manual_seed_all(42)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_parquet('../data/super_processed/7_4_no2_with_traffic_and_1meteo_and_1trafic_id.parquet')\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar los datos: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def get_available_sensors(self) -> List[str]:\n",
    "        if self.df_master is None:\n",
    "            self.df_master = self.load_data()\n",
    "        return sorted(self.df_master['id_no2'].unique().tolist())\n",
    "\n",
    "    def remove_outliers(self, df: pd.DataFrame, method: str, columns: List[str] = None) -> pd.DataFrame:\n",
    "        if method == 'none': \n",
    "            return df\n",
    "        if columns is None:\n",
    "            columns = ['no2_value']\n",
    "        df_filtered = df.copy()\n",
    "        for col in columns:\n",
    "            if col not in df_filtered.columns: \n",
    "                continue\n",
    "            if method == 'iqr':\n",
    "                Q1, Q3 = df_filtered[col].quantile(0.25), df_filtered[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                if IQR > 0:\n",
    "                    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "                    df_filtered = df_filtered[(df_filtered[col] >= lower) & (df_filtered[col] <= upper)]\n",
    "            elif method == 'zscore':\n",
    "                df_filtered = df_filtered[np.abs(zscore(df_filtered[col], nan_policy='omit')) < 3]\n",
    "        return df_filtered\n",
    "\n",
    "    def split_data(self, df: pd.DataFrame, split_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        train = df[df['fecha'] < split_date].copy()\n",
    "        test = df[df['fecha'] >= split_date].copy()\n",
    "        return train, test\n",
    "\n",
    "    def scale_features(self, X_train: pd.DataFrame, X_other: pd.DataFrame, features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "        scaler_dict = {}\n",
    "        X_train_s, X_other_s = X_train.copy(), X_other.copy()\n",
    "        for feature in features:\n",
    "            if feature in X_train.columns and pd.api.types.is_numeric_dtype(X_train[feature]):\n",
    "                scaler = StandardScaler()\n",
    "                X_train_s[feature] = scaler.fit_transform(X_train[[feature]]).flatten()\n",
    "                X_other_s[feature] = scaler.transform(X_other[[feature]]).flatten()\n",
    "                scaler_dict[feature] = scaler\n",
    "        return X_train_s, X_other_s, scaler_dict\n",
    "\n",
    "    def apply_feature_scalers(self, X: pd.DataFrame, scaler_dict: Dict, features: List[str]) -> pd.DataFrame:\n",
    "        Xs = X.copy()\n",
    "        for feature in features:\n",
    "            if feature in scaler_dict and feature in X.columns:\n",
    "                Xs[feature] = scaler_dict[feature].transform(X[[feature]]).flatten()\n",
    "        return Xs\n",
    "\n",
    "    def scale_target(self, y_train: pd.Series) -> Tuple[pd.Series, StandardScaler]:\n",
    "        scaler = StandardScaler()\n",
    "        y_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        return pd.Series(y_scaled, index=y_train.index, name=y_train.name), scaler\n",
    "\n",
    "    def train_bnn_model(self, X_train, y_train, train_config,\n",
    "                        X_val=None, y_val=None, y_val_scaled=None, scaler_target=None):\n",
    "        \"\"\"Entrenar el modelo BNN con diagnósticos y validación opcional.\"\"\"\n",
    "        dataset = BayesianDataset(X_train, y_train)\n",
    "        auto_bs = min(train_config.get('batch_size', 512), max(64, len(dataset) // 4))\n",
    "        dataloader = DataLoader(dataset, batch_size=auto_bs, shuffle=True)\n",
    "\n",
    "        layer_sizes = [X_train.shape[1]] + train_config['hidden_dims'] + [1]\n",
    "        model = make_mfvi_bnn(\n",
    "            layer_sizes,\n",
    "            activation=train_config['activation'],\n",
    "            prior_weight_std=1.0,\n",
    "            prior_bias_std=1.0,\n",
    "            init_std=0.05,\n",
    "            device=self.device\n",
    "        ).to(self.device)\n",
    "\n",
    "        log_noise_var = nn.Parameter(torch.ones(1, device=self.device) * -3.0)\n",
    "        params = list(model.parameters()) + [log_noise_var]\n",
    "        optimizer = torch.optim.Adam(params, lr=train_config['learning_rate'])\n",
    "\n",
    "        T = train_config['n_epochs']\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[int(0.6*T), int(0.85*T)], gamma=0.5\n",
    "        )\n",
    "\n",
    "        warmup = max(1, int(0.6 * T))\n",
    "        N_data = len(dataset)\n",
    "\n",
    "        # Tensores de validación si existen\n",
    "        if X_val is not None:\n",
    "            X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(self.device)\n",
    "            y_val_tensor = torch.tensor(y_val_scaled.values, dtype=torch.float32).reshape(-1, 1).to(self.device)\n",
    "\n",
    "        logs = []  # se almacenan dicts con todas las métricas por época\n",
    "\n",
    "        print(\"Inicio del entrenamiento BNN con registro de métricas por época\")\n",
    "        for epoch in range(T):\n",
    "            model.train()\n",
    "            epoch_nll, epoch_kl = 0.0, 0.0\n",
    "\n",
    "            beta_t = train_config.get('beta', 1.0) * min(1.0, (epoch + 1) / warmup)\n",
    "\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(x_batch)\n",
    "                nll = -gauss_loglik(y_batch, y_pred, log_noise_var).mean()\n",
    "                kl = kl_divergence_model(model)\n",
    "\n",
    "                loss = nll + beta_t * kl * (len(x_batch) / N_data)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_nll += nll.item() * len(x_batch)\n",
    "                epoch_kl += kl.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Métricas de validación periódicas\n",
    "            val_rmse = None\n",
    "            val_nll = None\n",
    "            if X_val is not None and (epoch % train_config.get('eval_every', 1) == 0):\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    K_eval = train_config.get('K_eval', 30)\n",
    "                    y_preds_val = torch.stack([model(X_val_tensor) for _ in range(K_eval)], dim=0)\n",
    "                    # NLL en validación en espacio escalado\n",
    "                    val_nll = test_nll(y_val_tensor, y_preds_val, log_noise_var).item()\n",
    "                    # Media y RMSE en espacio original\n",
    "                    pred_mean_val_scaled = y_preds_val.mean(0).detach().cpu().numpy()\n",
    "                    pred_mean_val = scaler_target.inverse_transform(pred_mean_val_scaled).flatten()\n",
    "                    val_rmse = float(np.sqrt(mean_squared_error(y_val.values, pred_mean_val)))\n",
    "\n",
    "            # Registro por época\n",
    "            current_lr = float(optimizer.param_groups[0]['lr'])\n",
    "            sigma_y = float(torch.exp(0.5 * log_noise_var).item())\n",
    "            nll_per_sample = epoch_nll / N_data\n",
    "            kl_per_sample = epoch_kl / N_data\n",
    "            elbo_per_sample = nll_per_sample + beta_t * kl_per_sample\n",
    "\n",
    "            logs.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_nll': nll_per_sample,\n",
    "                'train_kl_per_sample': kl_per_sample,\n",
    "                'train_elbo': elbo_per_sample,\n",
    "                'val_nll': val_nll,\n",
    "                'val_rmse': val_rmse,\n",
    "                'lr': current_lr,\n",
    "                'beta_t': beta_t,\n",
    "                'sigma_y': sigma_y,\n",
    "            })\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                msg = f\"Época {epoch+1}/{T}  NLL:{nll_per_sample:.3f}  KL/N:{kl_per_sample:.3f}  ELBO:{elbo_per_sample:.3f}\"\n",
    "                if val_rmse is not None:\n",
    "                    msg += f\"  Val RMSE:{val_rmse:.2f}  Val NLL:{val_nll:.3f}\"\n",
    "                msg += f\"  LR:{current_lr:.4g}  beta:{beta_t:.3f}  sigma_y:{sigma_y:.3f}\"\n",
    "                print(msg)\n",
    "\n",
    "        print(\"Entrenamiento completado\")\n",
    "        return model, log_noise_var, logs\n",
    "\n",
    "    def predict(self, model, X_test, K, log_noise_var):\n",
    "        model.eval()\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = torch.stack([model(X_test_tensor) for _ in range(K)], dim=0)\n",
    "        pred_mean = y_preds.mean(0)\n",
    "        epistemic_uncertainty = y_preds.var(0).sqrt()\n",
    "        aleatoric_uncertainty = torch.exp(0.5 * log_noise_var).expand_as(pred_mean)\n",
    "        total_uncertainty = (epistemic_uncertainty**2 + aleatoric_uncertainty**2).sqrt()\n",
    "        return {\n",
    "            'y_preds_all': y_preds,\n",
    "            'mean': pred_mean,\n",
    "            'epistemic_std': epistemic_uncertainty,\n",
    "            'aleatoric_std': aleatoric_uncertainty,\n",
    "            'total_std': total_uncertainty\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, predictions, y_test, y_test_scaled, scaler_target, log_noise_var, sensor_id: str = None):\n",
    "        pred_mean_scaled = predictions['mean'].detach().cpu().numpy()\n",
    "        pred_mean = scaler_target.inverse_transform(pred_mean_scaled).flatten()\n",
    "\n",
    "        total_std_scaled = predictions['total_std'].detach().cpu().numpy().flatten()\n",
    "        unscaled_std = total_std_scaled * scaler_target.scale_[0]\n",
    "        \n",
    "        epistemic_std = predictions['epistemic_std'].detach().cpu().numpy().flatten()\n",
    "        epistemic_unscaled_std = epistemic_std * scaler_target.scale_[0]\n",
    "\n",
    "        df_preds = pd.DataFrame({\n",
    "            'prediction': pred_mean,\n",
    "            'epistemic_uncertainty': epistemic_unscaled_std\n",
    "        })\n",
    "        sensor_suffix = f\"_{sensor_id}\" if sensor_id else \"_global\"\n",
    "        filename = f'../predictions/bnn_predictions_with_epistemic_uncertainty{sensor_suffix}.csv'\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        df_preds.to_csv(filename, index=False)\n",
    "        print(f\"Predicciones guardadas en: {filename}\")\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred_mean))\n",
    "        r2 = r2_score(y_test, pred_mean)\n",
    "        mae = mean_absolute_error(y_test, pred_mean)\n",
    "        \n",
    "        y_test_tensor = torch.tensor(y_test_scaled.values, dtype=torch.float32).reshape(-1, 1).to(self.device)\n",
    "        nll = test_nll(y_test_tensor, predictions['y_preds_all'], log_noise_var).item()\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse, 'r2': r2, 'mae': mae, 'test_nll': nll,\n",
    "            'y_pred': pred_mean, 'y_pred_std': unscaled_std,\n",
    "            'predictions_df': df_preds\n",
    "        }\n",
    "\n",
    "    def evaluate_global_by_sensor(self, data_prep: Dict, model, log_noise_var, scaler_dict, \n",
    "                                  scaler_target, selected_features, K_predict: int) -> Dict:\n",
    "        results_by_sensor = {}\n",
    "        test_df = data_prep['test_df']\n",
    "        print(\"Evaluación global por sensor\")\n",
    "\n",
    "        for sensor_id in data_prep['sensors_test']:\n",
    "            print(f\"Evaluando sensor: {sensor_id}\")\n",
    "            sensor_test_df = test_df[test_df['id_no2'] == sensor_id].copy()\n",
    "            if len(sensor_test_df) == 0:\n",
    "                print(f\"No hay datos de test para sensor {sensor_id}\")\n",
    "                continue\n",
    "            \n",
    "            X_test_sensor = sensor_test_df[selected_features].copy()\n",
    "            y_test_sensor = sensor_test_df['no2_value'].copy()\n",
    "            X_test_sensor_scaled = self.apply_feature_scalers(X_test_sensor, scaler_dict, selected_features)\n",
    "\n",
    "            y_test_sensor_scaled = pd.Series(\n",
    "                scaler_target.transform(y_test_sensor.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_test_sensor.index,\n",
    "                name=y_test_sensor.name\n",
    "            )\n",
    "            predictions = self.predict(model, X_test_sensor_scaled, K_predict, log_noise_var)\n",
    "            metrics = self.evaluate_model(predictions, y_test_sensor, y_test_sensor_scaled, \n",
    "                                          scaler_target, log_noise_var, sensor_id)\n",
    "            results_by_sensor[sensor_id] = {\n",
    "                'metrics': metrics,\n",
    "                'test_df': sensor_test_df,\n",
    "                'n_samples': len(sensor_test_df)\n",
    "            }\n",
    "            print(f\"RMSE: {metrics['rmse']:.2f}, R²: {metrics['r2']:.3f}, MAE: {metrics['mae']:.2f}\")\n",
    "        return results_by_sensor\n",
    "    \n",
    "    def save_model(self, path: str, model, log_noise_var, scaler_dict, scaler_target, \n",
    "                   feature_names: List[str], model_config: Dict = None):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        model_state = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'log_noise_var': log_noise_var,\n",
    "            'scaler_dict': scaler_dict,\n",
    "            'scaler_target': scaler_target,\n",
    "            'feature_names': feature_names,\n",
    "            'model_config': model_config or {}\n",
    "        }\n",
    "        joblib.dump(model_state, path)\n",
    "        print(f\"Modelo guardado en: {path}\")\n",
    "\n",
    "    def load_model(self, path: str, layer_sizes: List[int], activation: str = 'GELU'):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Modelo no encontrado en: {path}\")\n",
    "            return None\n",
    "        model_state = joblib.load(path)\n",
    "        model = make_mfvi_bnn(layer_sizes, activation=activation, device=self.device).to(self.device)\n",
    "        model.load_state_dict(model_state['model_state_dict'])\n",
    "        log_noise_var = model_state['log_noise_var'].to(self.device)\n",
    "        return (model, log_noise_var, model_state['scaler_dict'], \n",
    "                model_state['scaler_target'], model_state['feature_names'])\n",
    "\n",
    "# ==================== VISUALIZACIÓN Y REPORTE ====================\n",
    "\n",
    "def print_model_metrics(metrics: Dict, title: str = \"Métricas de Evaluación\"):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.2f} µg/m³\")\n",
    "    print(f\"R²: {metrics['r2']:.3f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.2f} µg/m³\")\n",
    "    print(f\"Test NLL: {metrics['test_nll']:.3f}\")\n",
    "\n",
    "def print_global_summary(results_by_sensor: Dict):\n",
    "    print(\"\\nResumen Modelo Global por Sensor\")\n",
    "    print(\"=\"*60)\n",
    "    all_rmse = [results_by_sensor[s]['metrics']['rmse'] for s in results_by_sensor]\n",
    "    all_r2 = [results_by_sensor[s]['metrics']['r2'] for s in results_by_sensor]\n",
    "    all_mae = [results_by_sensor[s]['metrics']['mae'] for s in results_by_sensor]\n",
    "    print(f\"RMSE promedio: {np.mean(all_rmse):.2f} ± {np.std(all_rmse):.2f} µg/m³\")\n",
    "    print(f\"R² promedio: {np.mean(all_r2):.3f} ± {np.std(all_r2):.3f}\")\n",
    "    print(f\"MAE promedio: {np.mean(all_mae):.2f} ± {np.std(all_mae):.2f} µg/m³\")\n",
    "    for sensor_id in sorted(results_by_sensor.keys()):\n",
    "        m = results_by_sensor[sensor_id]['metrics']\n",
    "        print(f\"{sensor_id}: RMSE={m['rmse']:.2f}, R²={m['r2']:.3f}, MAE={m['mae']:.2f}, n={results_by_sensor[sensor_id]['n_samples']}\")\n",
    "\n",
    "def save_training_loss_plot(logs: List[Dict], filename: str):\n",
    "    \"\"\"Gráfica integral de diagnóstico de entrenamiento y validación.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    log_df = pd.DataFrame(logs)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(log_df['epoch'], log_df['train_nll'], label='NLL entrenamiento')\n",
    "    if 'val_nll' in log_df and log_df['val_nll'].notna().any():\n",
    "        ax.plot(log_df['epoch'], log_df['val_nll'], label='NLL validación')\n",
    "    ax.set_title(\"NLL por época\")\n",
    "    ax.set_xlabel(\"Época\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(log_df['epoch'], log_df['train_kl_per_sample'])\n",
    "    ax.set_title(\"KL por muestra\")\n",
    "    ax.set_xlabel(\"Época\")\n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(log_df['epoch'], log_df['train_elbo'])\n",
    "    ax.set_title(\"ELBO por muestra\")\n",
    "    ax.set_xlabel(\"Época\")\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    if 'val_rmse' in log_df and log_df['val_rmse'].notna().any():\n",
    "        ax.plot(log_df['epoch'], log_df['val_rmse'])\n",
    "        ax.set_title(\"RMSE validación\")\n",
    "        ax.set_xlabel(\"Época\")\n",
    "        ax.set_ylabel(\"µg/m³\")\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(log_df['epoch'], log_df['lr'], label='LR')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(log_df['epoch'], log_df['beta_t'], color='tab:orange', label='beta')\n",
    "    ax.set_title(\"LR y beta\")\n",
    "    ax.set_xlabel(\"Época\")\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(log_df['epoch'], log_df['sigma_y'])\n",
    "    ax.set_title(\"Desviación del ruido σ_y\")\n",
    "    ax.set_xlabel(\"Época\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    print(f\"Curvas de entrenamiento guardadas en {filename}\")\n",
    "\n",
    "def save_validation_plots(y_val: pd.Series, pred_val: np.ndarray, fechas: Optional[pd.Series], out_prefix: str):\n",
    "    \"\"\"Gráficas auxiliares de validación: dispersión y serie temporal.\"\"\"\n",
    "    os.makedirs(os.path.dirname(out_prefix), exist_ok=True)\n",
    "    # Dispersión\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.scatter(y_val.values, pred_val, s=10, alpha=0.5)\n",
    "    minv = min(y_val.min(), pred_val.min())\n",
    "    maxv = max(y_val.max(), pred_val.max())\n",
    "    ax.plot([minv, maxv], [minv, maxv])\n",
    "    ax.set_xlabel(\"Observado (µg/m³)\")\n",
    "    ax.set_ylabel(\"Predicho (µg/m³)\")\n",
    "    ax.set_title(\"Validación: predicho vs observado\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_prefix + \"_scatter.png\")\n",
    "    plt.close(fig)\n",
    "    # Serie temporal si hay fechas\n",
    "    if fechas is not None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.plot(fechas.values, y_val.values, label='observado')\n",
    "        ax.plot(fechas.values, pred_val, label='predicho')\n",
    "        ax.set_title(\"Validación: serie temporal\")\n",
    "        ax.set_xlabel(\"Tiempo\")\n",
    "        ax.set_ylabel(\"µg/m³\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_prefix + \"_series.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "# ==================== FUNCIONES PRINCIPALES ====================\n",
    "\n",
    "def main_individual(config: Dict):\n",
    "    print(\"Entrenamiento BNN Individual\")\n",
    "    trainer = BNNUnifiedTrainer()\n",
    "    print(\"\\nCargando datos\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nConfiguración Individual\")\n",
    "    print(f\"Sensor: {config['sensor_id']}\")\n",
    "    print(f\"Fecha división: {config['split_date']}\")\n",
    "    print(f\"Outliers: {config['outlier_method']}\")\n",
    "\n",
    "    sample_data = trainer.df_master[trainer.df_master['id_no2'] == config['sensor_id']].head(100).copy()\n",
    "    if sample_data.empty:\n",
    "        print(f\"No se encontraron datos para el sensor {config['sensor_id']}\")\n",
    "        return\n",
    "    \n",
    "    all_features = [c for c in sample_data.columns \n",
    "                    if c not in ['fecha', 'id_no2', 'no2_value'] \n",
    "                    and pd.api.types.is_numeric_dtype(sample_data[c])]\n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    \n",
    "    print(f\"\\nSe usan {len(selected_features)} características\")\n",
    "    print(\"Características seleccionadas:\", selected_features)\n",
    "    \n",
    "    data_prep = trainer.prepare_individual_data(config, selected_features)\n",
    "    if not data_prep:\n",
    "        return\n",
    "    \n",
    "    # Split de validación temporal dentro del train\n",
    "    train_df = data_prep['train_df']\n",
    "    val_frac = config.get('val_fraction', 0.1)\n",
    "    split_val_date = train_df['fecha'].quantile(1 - val_frac)\n",
    "    core_train_df = train_df[train_df['fecha'] < split_val_date].copy()\n",
    "    val_df = train_df[train_df['fecha'] >= split_val_date].copy()\n",
    "\n",
    "    X_train = core_train_df[selected_features]\n",
    "    y_train = core_train_df['no2_value']\n",
    "    X_val = val_df[selected_features]\n",
    "    y_val = val_df['no2_value']\n",
    "\n",
    "    X_test = data_prep['test_df'][selected_features]\n",
    "    y_test = data_prep['test_df']['no2_value']\n",
    "    \n",
    "    # Escalado con scalers del core train\n",
    "    X_train_s, X_val_s, scaler_dict = trainer.scale_features(X_train, X_val, selected_features)\n",
    "    X_test_s = trainer.apply_feature_scalers(X_test, scaler_dict, selected_features)\n",
    "    y_train_s, scaler_target = trainer.scale_target(y_train)\n",
    "    y_val_s = pd.Series(scaler_target.transform(y_val.values.reshape(-1, 1)).flatten(), index=y_val.index, name=y_val.name)\n",
    "    y_test_s = pd.Series(scaler_target.transform(y_test.values.reshape(-1, 1)).flatten(), index=y_test.index, name=y_test.name)\n",
    "    \n",
    "    print(f\"Datos: train_core={len(X_train_s)}, val={len(X_val_s)}, test={len(X_test_s)}\")\n",
    "    \n",
    "    # Entrenar con validación\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(\n",
    "        X_train_s, y_train_s, config['train_config'],\n",
    "        X_val=X_val_s, y_val=y_val, y_val_scaled=y_val_s, scaler_target=scaler_target\n",
    "    )\n",
    "    \n",
    "    # Guardar curvas de entrenamiento\n",
    "    save_training_loss_plot(logs, f\"../models/bnn_training_diagnostics_{config['sensor_id']}.png\")\n",
    "\n",
    "    # Evaluación final en test\n",
    "    print(\"\\nEvaluación en test\")\n",
    "    predictions = trainer.predict(model, X_test_s, config['K_predict'], log_noise_var)\n",
    "    metrics = trainer.evaluate_model(predictions, y_test, y_test_s, scaler_target, log_noise_var, config['sensor_id'])\n",
    "    print_model_metrics(metrics)\n",
    "\n",
    "    # Gráficas de validación al final\n",
    "    with torch.no_grad():\n",
    "        y_val_preds = torch.stack([model(torch.tensor(X_val_s.values, dtype=torch.float32).to(trainer.device)) for _ in range(50)], dim=0)\n",
    "        pred_val_mean_scaled = y_val_preds.mean(0).detach().cpu().numpy()\n",
    "        pred_val_mean = scaler_target.inverse_transform(pred_val_mean_scaled).flatten()\n",
    "    save_validation_plots(y_val, pred_val_mean, val_df['fecha'], f\"../models/bnn_val_{config['sensor_id']}\")\n",
    "\n",
    "    model_path = f\"../models/bnn_model_{config['sensor_id']}.pkl\"\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, selected_features, config)\n",
    "\n",
    "    print(\"\\nProceso individual completado\")\n",
    "    return trainer, model, metrics\n",
    "\n",
    "\n",
    "def main_global(config: Dict):\n",
    "    print(\"Entrenamiento BNN Global Multi-Sensor\")\n",
    "    trainer = BNNUnifiedTrainer()\n",
    "    \n",
    "    print(\"\\nCargando datos\")\n",
    "    trainer.df_master = trainer.load_data()\n",
    "    if trainer.df_master.empty:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nConfiguración Global\")\n",
    "    print(f\"Sensores entrenamiento: {config['sensors_train']}\")\n",
    "    print(f\"Sensores test: {config['sensors_test']}\")\n",
    "    print(f\"Fecha división: {config['split_date']}\")\n",
    "    print(f\"Outliers: {config['outlier_method']}\")\n",
    "    \n",
    "    sample_data = trainer.df_master[trainer.df_master['id_no2'].isin(config['sensors_train'])].head(100).copy()\n",
    "    all_features = [c for c in sample_data.columns \n",
    "                    if c not in ['fecha', 'id_no2', 'no2_value'] \n",
    "                    and pd.api.types.is_numeric_dtype(sample_data[c])]\n",
    "    if config['features'] == ['all']:\n",
    "        selected_features = all_features\n",
    "    else:\n",
    "        selected_features = [f for f in config['features'] if f in all_features]\n",
    "    print(f\"\\nSe usan {len(selected_features)} características\")\n",
    "\n",
    "    data_prep = trainer.prepare_global_data(config, selected_features)\n",
    "    if not data_prep:\n",
    "        return\n",
    "    \n",
    "    # Split de validación temporal dentro de train\n",
    "    train_df = data_prep['train_df']\n",
    "    val_frac = config.get('val_fraction', 0.1)\n",
    "    split_val_date = train_df['fecha'].quantile(1 - val_frac)\n",
    "    core_train_df = train_df[train_df['fecha'] < split_val_date].copy()\n",
    "    val_df = train_df[train_df['fecha'] >= split_val_date].copy()\n",
    "\n",
    "    X_train = core_train_df[selected_features]\n",
    "    y_train = core_train_df['no2_value']\n",
    "    X_val = val_df[selected_features]\n",
    "    y_val = val_df['no2_value']\n",
    "\n",
    "    X_test = data_prep['test_df'][selected_features]\n",
    "    y_test = data_prep['test_df']['no2_value']\n",
    "    \n",
    "    # Escalado con scalers del core train\n",
    "    X_train_s, X_val_s, scaler_dict = trainer.scale_features(X_train, X_val, selected_features)\n",
    "    X_test_s = trainer.apply_feature_scalers(X_test, scaler_dict, selected_features)\n",
    "    y_train_s, scaler_target = trainer.scale_target(y_train)\n",
    "    y_val_s = pd.Series(scaler_target.transform(y_val.values.reshape(-1, 1)).flatten(), index=y_val.index, name=y_val.name)\n",
    "    y_test_s = pd.Series(scaler_target.transform(y_test.values.reshape(-1, 1)).flatten(), index=y_test.index, name=y_test.name)\n",
    "    \n",
    "    print(f\"Datos: train_core={len(X_train_s)}, val={len(X_val_s)}, test={len(X_test_s)}\")\n",
    "    \n",
    "    # Entrenar con validación\n",
    "    model, log_noise_var, logs = trainer.train_bnn_model(\n",
    "        X_train_s, y_train_s, config['train_config'],\n",
    "        X_val=X_val_s, y_val=y_val, y_val_scaled=y_val_s, scaler_target=scaler_target\n",
    "    )\n",
    "    \n",
    "    # Guardar curvas de entrenamiento\n",
    "    save_training_loss_plot(logs, f\"../models/bnn_training_diagnostics_global.png\")\n",
    "    \n",
    "    # Evaluación global\n",
    "    print(\"\\nEvaluación global en test\")\n",
    "    predictions = trainer.predict(model, X_test_s, config['K_predict'], log_noise_var)\n",
    "    global_metrics = trainer.evaluate_model(predictions, y_test, y_test_s, scaler_target, log_noise_var, \"global\")\n",
    "    \n",
    "    # Evaluación por sensor\n",
    "    sensor_results = trainer.evaluate_global_by_sensor(\n",
    "        data_prep, model, log_noise_var, scaler_dict, scaler_target, \n",
    "        selected_features, config['K_predict']\n",
    "    )\n",
    "    \n",
    "    print_model_metrics(global_metrics, \"Métricas Globales\")\n",
    "    print_global_summary(sensor_results)\n",
    "\n",
    "    # Gráficas de validación al final\n",
    "    with torch.no_grad():\n",
    "        y_val_preds = torch.stack([model(torch.tensor(X_val_s.values, dtype=torch.float32).to(trainer.device)) for _ in range(50)], dim=0)\n",
    "        pred_val_mean_scaled = y_val_preds.mean(0).detach().cpu().numpy()\n",
    "        pred_val_mean = scaler_target.inverse_transform(pred_val_mean_scaled).flatten()\n",
    "    save_validation_plots(y_val, pred_val_mean, val_df['fecha'], f\"../models/bnn_val_global\")\n",
    "\n",
    "    model_path = f\"../models/bnn_model_global.pkl\"\n",
    "    trainer.save_model(model_path, model, log_noise_var, scaler_dict, scaler_target, selected_features, config)\n",
    "    \n",
    "    print(\"\\nProceso global completado\")\n",
    "    return trainer, model, global_metrics, sensor_results\n",
    "\n",
    "\n",
    "# ==================== EJEMPLO DE EJECUCIÓN GLOBAL ====================\n",
    "\n",
    "def ejemplo_global():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EJEMPLO: ENTRENAMIENTO BNN GLOBAL MULTI-SENSOR\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = {\n",
    "        'sensors_train': ['28079004', '28079008', '28079011', '28079016', '28079036', '28079038', '28079039','28079040','28079047', '28079048'],\n",
    "        'sensors_test': ['28079050', '28079056', '28079035'],\n",
    "        'split_date': '2024-01-01',\n",
    "        'outlier_method': 'none',\n",
    "\n",
    "        # conjunto de variables sin ID de estación\n",
    "        'features': [\n",
    "            'intensidad', 'carga', 'intensidad_lag8', 'carga_lag4', 'carga_lag2',\n",
    "            't2m', 't2m_ma6', 'wind_speed_ma24', 'wind_speed_ewm3',\n",
    "            'ssrd', 'ssrd_sum24', 'u10_ewm6', 'tp_sum24',\n",
    "            'wind_dir_sin_ma6', 'wind_dir_cos_ma6',\n",
    "            'hour_sin','hour_cos','dow_sin','dow_cos','month_sin','month_cos'\n",
    "        ],\n",
    "\n",
    "        'K_predict': 80,\n",
    "\n",
    "        'train_config': {\n",
    "            'learning_rate': 0.003,\n",
    "            'n_epochs': 120,\n",
    "            'batch_size': 512,\n",
    "            'hidden_dims': [64, 32],\n",
    "            'activation': 'ReLU',\n",
    "            'beta': 0.5,\n",
    "            'val_fraction': 0.1,   # solo lectura por train_bnn_model si se pasara ahí\n",
    "            'K_eval': 30,\n",
    "            'eval_every': 1\n",
    "        },\n",
    "\n",
    "        # val_fraction se usa en main_* para el split temporal\n",
    "        'val_fraction': 0.1\n",
    "    }\n",
    "    \n",
    "    return main_global(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ejemplo_global()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
